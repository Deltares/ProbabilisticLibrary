\chapter{Reliability analysis}
\label{chp:ReliabilityAnalysis}


\section{Reliability methods}
\label{sec:ReliabilityMethods}

Equation \eqref{eq:2-4} describes the general formulation of the probability of failure of a single component. While an analytical solution to equation \eqref{eq:2-4} would be ideal, it is typically not possible because the $Z$-function is too complex. Therefore, the probability of failure needs to be estimated with probabilistic computation techniques. The computation techniques available within the \probLib are summarized in this chapter. 

The reason to implement a set of probabilistic computation techniques in the \probLib is that each technique has its (dis)advantages with respect to criteria such as robustness, accuracy and required computation time. The ''best'' technique to be applied therefore depends on the problem under consideration.

\subsection{Numerical Integration}
\label{sec:NumericalIntegration}

Numerical integration solves equation \eqref{eq:2-4} by discretizing the random variables $X_1$\dots $X_n$. Each variable is discretized over a range that is relevant for failure, and subsequently each combination of discretized values of the $X$-variables is used to compute the limit state function. The probabilities of all the combinations that lead to $Z<0$ are summed, which provides the estimate of the overall probability of failure. This summation can be written as follows:

\begin{align}
\hat{P}_{f} =&\sum _{i_{1} =1}^{m_{1} }\sum _{i_{2} =1}^{m_{2} }\dots\sum _{i_{n} =1}^{m_{n} }1_{\left[Z<0\right]}    f_{X} (x_{0,1} +(i_{1} -0.5)\Delta x_{1} ,x_{0,2}\label{eq:2-30} \\ \nonumber
 &+(i_{2} -0.5)\Delta x_{2} ,\dots,x_{0,n} +(i_{n} -0.5)\Delta x_{n} )\Delta x_{1} \Delta x_{2} \dots\Delta x_{n} 
\end{align}
where:

\begin{tabular}{p{0.4in}p{4in}} 
$\hat{P}_{f} $ & is the estimated probability of failure \\
$1_{\left[Z<0\right]} $ & is the indicator function, equal to $1$ for $Z$ $<$ $0$, equal to $0$ for $Z$ $\geq$ $0$ \\
$x_{0,k}$ & is the lower range limit for the $k^{th}$ variable \\
$\Delta$$x_k$ & is the interval width of the $k^{th}$ variable \\
$m_k$ & is the upper bound of $k$ such that $x_{0,k}$+ $m_k$$\Delta$$x_k$ is the upper bound of the $k^{th}$ variable \\
\end{tabular}

In equation \eqref{eq:2-30}, for each variable $X_k$ an equidistant grid with step size $\Delta$$x_k$ is used, but non-equidistant grids can also be used in numerical integration. \Fref{fig:NumIntegr} presents a schematic view of the method, for an example of two random variables $X_1$ and $X_2$. A 2-dimensional grid is defined and the $Z$-function is evaluated at the centre of the grid cells. Red grid points indicate failure $(Z<0)$, green grid points indicate no failure ($Z\geq0$). The total probability of failure (see equation \eqref{eq:2-30}) is estimated as follows: multiply the probability density of the grid cells in the failure domain (red dots) with the size of the grid cells ($\Delta$$x_1$ $\times$ $\Delta$$x_2$) and take the sum of these probabilities.

\begin{figure}[H]
	\label{fig:NumericalIntegration}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures numericalintegration.jpg}
	\caption{Numerical Integration: realizations for two random variables}\label{fig:NumIntegr}
\end{figure}


Like every probabilistic estimation technique, the result of the numerical integration procedure will be an approximation of the actual probability of failure. The errors that are introduced in this method are caused by the following assumptions and approximations:

Each grid cell is assumed to be entirely situated in the failure domain or entirely situated outside the domain of failure domain. In reality, grid cells can be partly in the failure domain as can be seen in \Fref{fig:NumIntegr}.

The probability density is assumed to be constant over the entire grid cell. The domain of potential outcomes of the random variables may not be entirely covered. In the implementation of the procedure, it may be beneficial to transform the $X$-variables to standard normally distributed $U$-variables (see \Aref{Section_2.2.3}). One of the benefits of working in the $U$-space is that the $U$-variables are independent, which simplifies equation \eqref{eq:2-30} as follows:

{\footnotesize{\begin{equation} 
\hat{P}_{f} =\sum _{i_{1} =1}^{m_{1} }\dots \sum _{i_{n} =1}^{m_{n} }1_{\left[Z<0\right]}  \phi \left(u_{0,1} +\left(i_{1} -0.5\right)\Delta u_{1} \right)\cdot \dots\cdot \phi \left(u_{0,n} +\left(i_{n} -0.5\right)\Delta u_{n} \right)\Delta u_{1} ...\Delta u_{n} \label{eq:2-31} 
\end{equation}}}
where:

\begin{tabular}{p{\textwidth-36pt-129mm}p{125mm}}  
	$\hat{P}_{f} $ & is the estimated probability of failure \\ 
	$\phi $ & is the standard normal density function \\ 
	$u_{0,k} $ & is the lower range limit for the $k^{th}$ variable, in the $U$-space \\  
	$\Delta u_{k} $ & is the interval width of the $k^{th}$ variable, in the $U$-space \\  
	$m_{k} $ & is the upper bound of $k$ such that $u_{0,k} +m_{k} \cdot \Delta u_{k}$ is the upper bound of the $k^{th}$ variable\\  
\end{tabular}


\Note{The minimum and maximum values for which the Numerical Integration runs are defined in $U$-space. Numerical Integration fills the remaining space between these values and $-8$ to $8$ using additional cells, ensuring full coverage of the integration domain. Note that $U=8$ corresponds to a probability of approximately $6 \times 10^{-16}$, which is orders of magnitude smaller than the probabilities encountered in typical scenarios.}


\newpage
\subsection{Numerical Bisection}
\label{sec:NumericalBisection}

Numerical Bisection is similar to Numerical Integration, but in this method, the integration cells are generated by bisecting the entire domain.

Whenever an integration cell has the same qualitative result (fail, not fail, or not counting) at all its corner points, we assume that the result remains the same throughout the cell. Consequently, no additional calculations are performed inside the cell or along its edges.

If a cell contains corner points with different qualitative results, it is split into smaller cells, and new corner points are evaluated.

This process continues until the remaining cells -- those with mixed results -- represent a probability lower than an acceptable threshold in reliability. The user can specify this threshold.

\begin{figure}[H]
	\label{fig:NumericalBisection}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures numericalbisection.jpg}
	\caption{Numerical Bisection: realizations for two random variables}
\end{figure}


\subsection{Crude Monte Carlo}
\label{sec:CrudeMonteCarlo}

\subsubsection{Algorithm}

Crude Monte Carlo is the standard Monte Carlo simulation method. Random realizations are generated in proportion to their probability density. The number of realizations that result in failure is counted, as well as the number that do not lead to failure. The probability of failure is then calculated as follows:

\begin{align} 
\label{eq:MonteCarlo}
p_{\text{failure}} = \frac{N_{\text{failure}}}{N}
\end{align}

where:
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$p_{\text{failure}}$ & is the probability of failure \\  
	$N_{\text{failure}}$ & is the number of realizations which were interpreted as failing \\ 
	$N$ & is the total number of realizations
\end{longtable}

\begin{figure}[H]
	\label{fig:MonteCarlo}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures montecarlo.jpg}
	\caption{Crude Monte Carlo: realizations for two random variables}
\end{figure}

\subsubsection{Convergence}
\label{sec:MCConvergence}

The Monte Carlo simulation also leads to a standard deviation of the probability of failure. This standard deviation $\sigma_p$ is based on a confidence level $\alpha$:

\begin{align} 
\label{eq:MonteCarloStandardDeviation}
\sigma_p = z \sqrt{\frac{p \left(1 - p\right)}{N}}
\end{align}

with:

\begin{align} 
\label{eq:MonteCarloStandardConfidence}
z = \Phi^{-1}\left(1 - \frac{\alpha}{2}\right)
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\sigma_p$ & is the standard deviation of the probability of failure \\  
	$p$ & is the probability of failure $p_{\text{failure}}$ \\
	$N$ & is the total number of realizations \\
	$z$ & is the quantile of the standard normal distribution corresponding with $\alpha$.
	$z$ is equal to $1$ in the \probLib \\
	$\Phi$ & is the CDF of the standard normal distribution (see \autoref{eq:StandardNormalCDF}) \\
	$\alpha$ & is the confidence level
\end{longtable}

The standard deviation $\sigma_p$ can be expressed as a variation coefficient $\varepsilon$:

\begin{align} 
\label{eq:MonteCarloVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p} = z \sqrt{\frac{1 - p}{N p}}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)} = z \sqrt{\frac{p}{N \left(1 - p\right)}}
\end{array}	
\right.
\end{align}

The user can set the maximum variation coefficient $\varepsilon_{\text{max}}$. The Monte Carlo simulation will stop when $\varepsilon \leq \varepsilon_{\text{max}}$ (and limited to the minimum and maximum number of realizations). The confidence level $\alpha$ cannot be set by the user. Its value is such that $z$ is $1$.

\begin{figure}[H]
	\label{fig:MonteCarloConvergence}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures montecarloconvergence.jpg}
	\caption{Crude Monte Carlo convergence}
\end{figure}

When the underlying model fails for a certain realization, its results are ignored. The user should decide whether the number of failed realizations is acceptable.

The minimum and maximum values for which the integration will run are defined in the $u$-space. The Monte Carlo analysis will then check whether the remaining area leads to failure or not.

\subsection{Importance Sampling}
\label{sec:ImportanceSampling}

\subsubsection{Algorithm}
\label{sec:ImportanceSamplingAlgorithm}

Importance sampling is a method to increase the efficiency of the Crude Monte Carlo method; that is, to decrease the number of samples and $Z$-function evaluations required to produce a reliable estimate of the failure probability. This is done by replacing the initial probability density of the input variables by a more efficient one, in which ''efficient'' refers to the proportion of the samples which will result in failure. An increasing percentage of samples in the failure domain results in a reduction in the variance of the estimator of the failure probability, hence a smaller number of samples is required for a reliable estimate. 

In Crude Monte Carlo, realizations are drawn proportionally to their probability density $\varphi\left(u_i\right)$, since the \probLib uses the standard normal space. With Importance Sampling, each realization $u$ is mapped to $u_{\text{imp}}$. The \probLib supports the following translation for each variable individually:

\begin{align} 
\label{eq:ImportanceSamplingVariance}
u_{\text{imp}} = \mu_{\text{var}} + \sigma_{\text{var}} \cdot u_{\text{var}}
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\mu_{\text{var}}$ & is the user defined mean value per variable. The combination of all variables is the mean realization of the Importance Sampling algorithm; \\
	$\sigma_{\text{var}}$ & is the user defined standard deviation per variable.
\end{longtable}

A correction is applied in the calculation of failure to compensate for this translation. This is done by giving each realization a weight, which is calculated as follows (the multiplication with $\sigma_{\text{var}}$ is performed to compensate for the dimensionality):

\begin{align} 
\label{eq:ImportanceSamplingVarWeight}
w_{\text{var}} = \frac{\sigma_{\text{var}} \cdot \varphi\left(u_\text{imp}\right)}{\varphi\left(u_{\text{var}}\right)}
\end{align}

and

\begin{align} 
\label{eq:ImportanceSamplingWeight}
W_{\text{realization}} = \prod_{\text{variables}} w_{\text{var}}
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\varphi$ & is the standard normal probability density function, see \autoref{eq:StandardNormalPDF} \\  
	$w_{\text{var}}$ & is the weight factor per variable per realization \\
	$W_{\text{realization}}$ & is the weight factor of the realization \\
\end{longtable}

Corresponding to \autoref{eq:MonteCarlo}, the probability of failure is equal to:

\begin{align} 
\label{eq:ImportanceSampling}
p_{\text{failure}} = \frac{\sum_{\text{failing realizations}} W}{\sum_{\text{all realizations}} W}
\end{align}

To avoid numerical issues and because:

\begin{align} 
\label{eq:ImportanceSamplingLimit}
\lim_{N \to \infty} {\sum_{\text{all realizations}} W} = N
\end{align}

The probability of failure is calculated as follows:

\begin{align} 
\label{eq:ImportanceSamplingCalculated}
p_{\text{failure}} = \frac{\sum_{\text{failing realizations}} W}{N}
\end{align}

\subsubsection{Convergence}

Corresponding to \autoref{eq:MonteCarloStandardDeviation} and \autoref{eq:MonteCarloVariationCoefficient}, the standard deviation and variance coefficient become:

\begin{align} 
\label{eq:ImportanceSamplingStandardDeviation}
\sigma_p = z \sqrt{\frac{p \left(W_{\text{design point}} - p\right)}{N}}
\end{align}

and 

\begin{align} 
\label{eq:ImportanceSamplingVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p} = z \sqrt{\frac{W_{\text{design point}} - p}{N p}}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)} = z \sqrt{\frac{W_{\text{design point}} - \left(1 - p\right)}{N \left(1 - p\right)}}
\end{array}	
\right.
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$z$ & is a value related to the confidence level (see \autoref{eq:MonteCarloStandardConfidence}). In the \probLib, $z$ is $1$ \\
	$W_{\text{design point}}$ & is the weight of the realization of the design point \\  
	$N$ & is the total number of realizations \\  
\end{longtable}

When the underlying model does not succeed for a certain realization, its results will be ignored. The user should decide whether the number of non succeeded realizations is acceptable.

\begin{figure}[H]
	\label{fig:ImportanceSampling}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures ImportanceSampling.jpg}
	\caption{Importance Sampling: realizations for two random variables}
\end{figure}

\subsubsection{Mean realization}
\label{sec:StartPoint}

The mean realization (see {\autoref{eq:ImportanceSamplingVariance}) is essential for the successful behaviour of the Importance Sampling algorithm. The mean realization can be user defined or derived automatically. The following options are available:
	
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	\textbf{None} & The mean realization is user defined. \\
	\textbf{Direction} & Along a user defined direction is searched until the limit state is found, limited by a maximum distance from the origin. This option still requires some knowledge where to find the mean realization.  \\
	\textbf{Sphere} & Over a sphere is searched until the limit state is found. Only at angles $0^\circ$, $90^\circ$, $180^\circ$ and $270^\circ$ is searched. If the limit state is not found, the radius of the sphere is increased. This is a time consuming option. \\  
	\textbf{Sensitivity} & Along the direction of the steepest gradient is searched until the limit state is found, limited by a maximum distance from the origin. The steepest gradient is found by taking the gradient for each variable individually and then combining them. This is the preferred option. \\  
\end{longtable}

\subsection{Adaptive Importance Sampling}
\label{sec:AdaptiveImportanceSampling}

\subsubsection{Algorithm}

Adaptive Importance Sampling is an improvement on the Importance Sampling method. In general, Importance Sampling is sensitive to the user-provided starting point. If the starting point is not chosen carefully, the method may yield unsatisfactory results. In Adaptive Importance Sampling, iterative loops are used to refine the starting point. To achieve this, the K-Means algorithm is applied to identify multiple starting points based on the samples that led to failure in the previous iteration.

The Adaptive Importance Sampling algorithm is displayed in \autoref{fig:ImportanceSamplingLoops}.

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[auto, scale=0.5]
	\node [round] (Start) {Start};
	\node [block, right=of Start] (Initialize) {Initialize starting point};
	\node [block, below=of Initialize] (ImportanceSampling) {Importance Sampling};
	\node [block, below=of ImportanceSampling] (IfConverged) {Converged?};
	\node [block, below=of IfConverged] (IfFinalRound) {Final round?};
	\node [block, left= of IfFinalRound] (IfEnoughFailures) {Enough failures?};
	\node [block, right= of IfFinalRound] (Recalculate) {Recalculate};
	\node [round, below=of Recalculate] (End) {End};
	\node [block, above=of IfEnoughFailures] (Increase) {Increase variance};
	\node [block, left=of Increase] (Move) {Use design point};
	
	\draw [->] (Start) -- (Initialize);
	\draw [->] (Initialize) -- (ImportanceSampling);
	\draw [->] (ImportanceSampling) -- (IfConverged);
	\draw [->] (IfConverged) -| node {yes} (Recalculate);
	\draw [->] (IfConverged) -- node {no} (IfFinalRound) ;
	\draw [->] (IfFinalRound) -- node {yes} (Recalculate) ;
	\draw [->] (Recalculate) -- (End);
	\draw [->] (IfFinalRound) -- node {no} (IfEnoughFailures);
	\draw [->] (IfEnoughFailures) -- node {no} (Increase);
	\draw [->] (IfEnoughFailures) -| node {yes} (Move);
	\draw [->] (Increase) |- (ImportanceSampling);
	\draw [->] (Move) |- (ImportanceSampling);
	
	\end{tikzpicture}
	\caption{Adaptive Importance Sampling}
	\label{fig:ImportanceSamplingLoops}
\end{figure}

These loops are executed until a required fraction of failed realizations $\varepsilon_\text{failed}$ is reached:

\begin{align}
\label{eq:ISFractionFailed}
\text{min}\left(\frac{N_\text{failed}}{N}, 1 - \frac{N_\text{failed}}{N}\right) \geq \varepsilon_\text{failed}
\end{align}

where:
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$N$ & is the total number of failed realizations \\
	$N_\text{failed}$ & is the number of failed realizations
\end{longtable}

The following is performed in each step:

\begin{longtable}{p{40mm}p{\textwidth-24pt-40mm}}  
	\textbf{Initialize starting point} & Find the starting point with one of the starting point algorithms (see \autoref{sec:StartPoint}). \\
	\textbf{Importance Sampling} & The Importance Sampling algorithm (see \autoref{sec:ImportanceSamplingAlgorithm}). The number of samples in each Importance Sampling can be set to a maximum. When the auto break option is used, the algorithm decides when to break a loop and continue with the next loop (see \autoref{sec:AutoBreakLoops}). \\
	\textbf{Converged?} & Checks whether there is convergence (see \autoref{eq:ISFractionFailed}).  \\
	\textbf{Final round?} & Checks whether the last allowed round is reached, in order to prevent endless loops. \\
	\textbf{Recalculate} & Recalculates the last round with a possibly higher number of realizations. \\
	\textbf{Enough failures?} & Checks whether enough failures are found. Depending on this value, the kind of modification of Importance Sampling settings is determined. \\
	\textbf{Increase variance} & Increases the variance (see \autoref{sec:ImportanceSamplingAlgorithm}) for the next loop. \\
	\textbf{Use design point} & Uses the design point found in the last loop as starting point in the next loop. In case no design point was found, the realization closest to the limit state is used. \\
\end{longtable}

The following features are available to reduce calculation time:

\begin{itemize}
\item The maximum number of realizations for non-final loops can be specified separately from the maximum number of realizations in the final loop.
\item Subsequent loops can be skipped if the reliability index exceeds a predefined threshold determined in the first loop.
\item When the design point is carried over to the next loop, it will be rounded to a specified value. Specify $0$ if no rounding is desired.
\item When using scenario tables, previous calculation results can be reused.
\end{itemize}

\subsubsection{Auto break loops}
\label{sec:AutoBreakLoops}

The number of realizations in the loops in Adaptive Importance Sampling can be determined automatically. This algorithm is based on the convergence criterion $\varepsilon_\text{weight}$, which specifies the maximum relative weight of all failing samples. Usually this value is $0.1$.

The loop is broken when the required number of runs with the current starting point ($n_\text{additional}$) is more than the expected number of runs with an improved starting point ($n_\text{expected}$):

\begin{align}
\label{eq:ISAutoBreak}
n_\text{additional} > n_\text{expected} 
\end{align}

with:

\begin{align}
\label{eq:ISAutoBreakAdditional}
n_\text{additional} = n_\text{current} \cdot \left(\frac{W_\text{max}}{W_\text{total} \cdot \varepsilon_\text{weight}} - 1\right)
\end{align}

and

\begin{align}
\label{eq:ISAutoBreakExpected}
n_\text{expected} = \frac{2 \left(\beta_\text{current} + 1\right)}{\varepsilon_\text{weight}}
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$n_\text{current}$ & the number of samples already made in the current loop \\
	$W_\text{max}$ & the maximum weight of a failed sample in the current loop \\
	$W_\text{total}$ & the total weight of the failed samples in the current loop \\
	$\beta_\text{current}$ & the reliability index based on the samples in the current loop \\
	$\varepsilon_\text{weight}$ & convergence criterion,
	which specifies the maximum relative weight of all failing samples, default is $0.1$
\end{longtable}


\subsection{Subset simulation}
\label{sec:SubsetSimulation}

Subset Simulation [\cite{Au2001}] is a staged version of Crude Monte Carlo (see \Autoref{sec:CrudeMonteCarlo}). It consists of multiple iterations, with the first iteration identical to Crude Monte Carlo. The simulation stops once the Crude Monte Carlo convergence criterion is met, i.e., when $\varepsilon \leq \varepsilon_{\text{max}}$ (see \Autoref{eq:MonteCarloVariationCoefficient}).

If the convergence criterion is not met, a new Crude Monte Carlo iteration is performed, using a fraction $k$ of the realizations from the previous run. The realizations closest to failure are selected, and new realizations are generated from them using a Markov Chain or Adaptive Conditional Sampling [\cite{PAPAIOANNOU201589}].

For each old realization, $\tfrac{1}{k}$ new realizations are generated. A new realization is generated in the following way, with $u$-values for each variable var:

\begin{align} 
\label{eq:SubsetSimulationMarkovChain}
u_{\text{var, new}} = \left\{\begin{array}{ll} r_{\text{var}} \geq R_{\text{[0, 1]}}  & u_{\text{var, prop}}\\
r_{\text{var}} < R_{\text{[0, 1]}}  & u_{\text{var, prev}}
\end{array}	
\right.
\end{align}

with:

\begin{align} 
\label{eq:SubsetSimulationRatio}
r_{\text{var}} = \frac{\varphi \left(u_{\text{var, prop}}\right)}{\varphi \left(u_{\text{var, prev}}\right)}
\end{align}

and

\begin{align} 
\label{eq:SubsetSimulationProposed}
u_{\text{var, prop}} = u_{\text{var, prev}} + R_{\text{[-1, 1]}} \cdot \Delta \sigma
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$u_{\text{var, prev}}$ & is the $u$-value per variable of the previous realization \\
	$u_{\text{var, prop}}$ & is a proposed new $u$-value per variable \\
	$u_{\text{var, new}}$ & is the new $u$-value per variable \\
	$R_{\text{[0, 1]}}$ & is a random value between $0$ and $1$ \\  
	$R_{\text{[-1, 1]}}$ & is a random value between $-1$ and $1$ \\  
	$r_{\text{var}}$ & is the ratio of probability density between $u_{\text{var, prop}}$ and $u_{\text{var, prev}}$ \\  
	$\varphi$ & is the probability density function of a standard normal distribution (see \autoref{eq:StandardNormalPDF}) \\  
	$\Delta \sigma$ & is the user given maximum deviation from the previous $u$-value.
	In case the option 'Adaptive Conditional' is used,
	this value is derived automatically and updated during the process
\end{longtable}

The proposed sample $u_{\text{prop}}$ is used if the corresponding $z$-value is less than the $z_{\text{k}}$, which is the highest $z$-value in the subset used to generate new realizations, otherwise the original sample is used.

The probability of failure is calculated as follows:

\begin{align} 
\label{eq:SubsetSimulation}
p = k^{\text{i}} \cdot p_{\text{i, MC}}
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$i$ & is the iteration index, the first being $0$ \\
	$k$ & is the fraction taken from the realizations (recommended value $0.1$) \\  
	$p_{\text{i, MC}}$ & is the probability of failure calculated with Crude Monte Carlo using the realizations of the $i^{\text{th}}$ iteration
\end{longtable}

\newpage
\subsection{Directional Sampling}
\label{sec:DirectionalSampling}

\subsubsection{Algorithm}
Directional Sampling is a type of Monte Carlo method which aims to (strongly) reduce the number of samples in comparison with the Crude Monte Carlo method. In Directional Sampling, realizations are represented as directions, rather than points in the parameter space. Along this direction/vector, the point of failure is identified, and the corresponding distance, $\beta$, to the origin is determined. The remaining probability of failure beyond this point is calculated and added to the total probability of failure as follows:

\begin{align} 
\label{eq:DirectionalSampling}
p_{\text{failure}} = \frac{\sum w_{\text{dir}}}{N_{\text{realizations}}}
\end{align}

with:

\begin{align} 
\label{eq:DirectionalSamplingWeight}
w_{\text{dir}} = \Gamma \left(\frac{N_{\text{variables}}}{2} , \frac{{\beta_{\text{dir}}}^2}{2} \right)
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$w_{\text{dir}}$ & is the weight of a direction or the failure probability in this direction \\
	$\beta_{\text{dir}}$ & is the distance along a direction where the first point of failure is found \\
	$\Gamma$ & is the upper incomplete regularized gamma function
\end{longtable}

\subsubsection{Convergence}

The standard deviation of the failing probability is calculated as follows:

\begin{align} 
\label{eq:DirectionalSamplingStandardDeviation}
\sigma_p = \sqrt{\frac{\sum \left(w_{\text{dir}} - p \right)^2}{N \cdot \left(N - 1\right)}}
\end{align}

The variation coefficient is calculated as follows:

\begin{align} 
\label{eq:DirectionalSamplingVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)}
\end{array}	
\right.
\end{align}

The Directional Sampling simulation stops if the variation coefficient $\varepsilon$ is less than the user given maximum variation coefficient $\varepsilon_{\text{max}}$. Also, the number of minimum and maximum directions should be satisfied.

The minimum and maximum number of iterations refer to an internal procedure to determine the point along the direction where failure occurs.

\begin{figure}[H]
	\label{fig:DirectionalSampling}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures DirectionalSampling.jpg}
	\caption{Directional Sampling: realizations for two random variables}
\end{figure}

\subsection{Latin Hypercube}
\label{sec:LatinHypercube}

The Latin Hypercube algorithm is a type of Monte Carlo sampling method. It uses a fixed number of realizations, $N$. The algorithm divides the $u$-space into $N$ equal sections, starting from $u_\text{min}$ and ending at $u_\text{max}$. It ensures that each section is represented in the realizations, guaranteeing that extreme values are included. However, note that the weights of the realizations may vary. If $u_\text{min}$ and $u_\text{max}$ differ from $-8$ and $8$ -- beyond which calculations are numerically infeasible -- the remaining space is treated as a single cell and included in the failure analysis.

Latin Hypercube uses the same convergence definition as Crude Monte Carlo (see \Autoref{sec:MCConvergence}). However, since a fixed number of realizations is used, convergence is calculated only for informational purposes. The main advantage of Latin Hypercube is its computational efficiency. However, it provides only a rough approximation of the probability of failure.

\subsection{Cobyla}
\label{sec:Cobyla}

The Cobyla algorithm searches for the point in the $u$-space that has the highest probability density and indicates failure. This point is assumed to be representative of the design point.

The Cobyla algorithm is an implementation of Powell's nonlinear derivative-free constrained optimization method, which uses a linear approximation approach \cite{Powell1994}. It is a sequential trust-region algorithm that employs linear approximations to both the objective and constraint functions. These approximations are formed by linear interpolation at $n + 1$ points in the variable space, and the algorithm aims to maintain a well-shaped simplex throughout iterations.

Because the Cobyla algorithm provides a rough estimation of the reliability and design point, it should be used primarily as an initial method in a reliability study.

\newpage
\subsection{FORM}
\label{sec:FORM}

\subsubsection{Algorithm}

FORM stands for First Order Reliability Method. The FORM procedure is not a Monte Carlo simulation. Instead, it directly searches for the design point. This design point is considered representative of the total failure probability, which is calculated as follows:

\begin{align}
p_\text{failure} = 1 - \Phi \left(\beta\right)
\end{align}

The FORM procedure starts from a certain user defined starting point in the parameter space. From there it tries to find a point which is closer to the design point by taking the gradient of the $z$-value in the $u$-space of the parameters. The $z$-value is an indication whether failure occurs and is derived from the failure definition. After a number of steps, the point is close enough to the design point and the calculation stops. 

The FORM analysis searches for the design point: When the design point is found, the reliability index $\beta$ can be evaluated as the distance between the origin and the design point (see \autoref{fig:FORMDesignPoint}). The corresponding probability of failure is:

\begin{align}
p_\text{failure} = 1 - \Phi \left(\beta\right)
\end{align}

where $\Phi$ is the cumulative density function in the standard normal space (see \autoref{eq:StandardNormalCDF}).

The probability of failure found in this way is regarded to be a good approximation of the "real" probability of failure. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1.00\textwidth]{\pathProbLibPictures FORMDesignPoint.png}
	\caption{Schematic representation of FORM}
	\label{fig:FORMDesignPoint}
\end{figure}

To find the design point, the FORM analysis starts at a given starting point, usually the origin, in the standard normal space and iterates to the design point.

In each iteration step, the current point is moved closer to the design point. Using the steepest descend, the next point is found, until a convergence criterion is fulfilled. To get from the current point $u_{\text{i}}$ to the next point $u_{\text{i+1}}$, the predicted point $u_{\text{pred}}$ is determined. 

To calculate $u_{\text{pred}}$, we assume that $z$ is linear with $u$ close to the design point. The gradient is taken between $u_i$ and $u_\text{pred}$: 

\begin{align}
\label{eq:UZLinear}
\lvert \frac{\partial z}{\partial u} \rvert = \frac{z_\text{pred} - z_i}{u_\text{pred} - u_i} = - \frac{z_i}{u_\text{pred} - u_i}
\end{align}

which is equivalent to:

\begin{align}
\label{eq:BetaPredicted}
u_\text{pred} = u_i - \frac{z_i}{\lvert \frac{\partial z}{\partial u} \rvert }
\end{align}

where:
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$i$ & is the iteration index number; \\  
	$\lvert \frac{\partial z}{\partial u} \rvert$ & is the calculated vector length of the derivative to all variables \\ 
	$z_\text{pred}$ & is the limit state value at the design point, which is by definition equal to zero\\  
	$z_i$ & is the calculated limit state value at iteration $i$ \\  
	$u_\text{pred}$ & is the predicted vector of the design point \\  
	$u_i$ & is the iteration vector of $u$ at iteration $i$ \\  
\end{longtable}

The step from the current point to the next point is a step into the direction of $u_{\text{pred}}$. The step is not taken completely, but partially with a relaxation factor $f_{\text{relax}}$, in order to prevent numerical instabilities.

\begin{align}\label{eq:FORM}
u_\text{j, i+1} = - \alpha_\text{j, i} \cdot \lvert  u_\text{i, pred} \rvert \cdot f_\text{relax} + u_\text{j, i} \cdot \left(1 - f_\text{relax}\right)
\end{align}

with the $\alpha$ value per variable:

\begin{align}\label{eq:Alpha}
\alpha_{j} = \frac{\frac{\partial z_\text{j}}{\partial u_\text{j}}}{\lvert \frac{\partial z}{\partial u} \rvert}
\end{align}

where:
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$j$ & is the index of a variable \\  
	$\lvert u \rvert$ & is the vector length of the current point \\  
	$f_\text{relax}$ & is the given relaxation factor. The relaxation factor prevents the algorithm from too big steps, which may cause divergence \\  	
\end{longtable}

and finally leads to the reliability index $\beta$ as:

\begin{align}
\label{eq:FORMBeta}
\beta = \lvert u_i \rvert
\end{align}

The benefit of this method is that it is quick. The disadvantages are that a local design point can be found, which then corresponds to a non representative failure probability. Another disadvantage is that numerical problems may occur, since the $z$-value must be continuous.

One cannot determine whether a design point is a local design point, so the user must use this method with care and after analysis whether this method is suitable for the kind of models he uses. Numerical problems can be detected in the convergence chart. If this occurs, one can decrease the relaxation factor or modify the gradient step size (both directions could improve or decrease results).

\subsubsection{Convergence}

Convergence is reached when the $u_i$, the value of the last iteration $i$, is close enough to the predicted value $u_{\text{pred}}$, so:

\begin{align}
\label{eq:FORMConvergence}
\lvert u_i - u_\text{pred} \rvert<\varepsilon
\end{align}

where:
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$u_i$ & is the point at iteration $i$ \\
	$u_\text{pred}$ & is the predicted final value of $u$, see \autoref{eq:UZLinear} \\
	$\varepsilon$ & is a user given value maximum allowed difference in reliability \\  
\end{longtable}

and, together with \autoref{eq:FORMConvergence}, the convergence criterion:

\begin{align}
\label{eq:FORMConv1}
\frac{\lvert z \rvert}{\lvert \frac{\partial z}{\partial u} \rvert } < \varepsilon
\end{align}

Alternatively to \autoref{eq:UZLinear}, the gradient is taken between $u_0$ (all $u$-values equal to $0$) and $u_\text{pred}$. Then we get for the linear relation:

\begin{align}
\label{eq:UZLinear2}
\lvert \frac{\partial z}{\partial u} \rvert = \frac{z_\text{pred} - z_0}{u_\text{pred} - u_0} = - \frac{z_0}{u_\text{pred}}
\end{align}

The value $z_0$ is calculated with the value for $z$ in the last iteration $z_i$ and the gradient:

\begin{align}
\label{eq:UZLinear3}
z_0 = z_i - \sum_j \frac{\partial z_i}{\partial u_j} \cdot u_j
\end{align}

which leads to:

\begin{align}
\label{eq:BetaPredicted2}
\lvert u_\text{pred}\rvert = \frac{\lvert z_0 \rvert}{\lvert \frac{\partial z}{\partial u} \rvert }
\end{align}

The convergence criterion is defined as the relative difference in length of $u_\text{pred}$ and $u$:

\begin{align}
\label{eq:FORMConv2}
\frac{\lvert u_\text{pred}^2 - u^2 \rvert}{u^2} < \varepsilon
\end{align}

\begin{figure}[H]
	\label{fig:FORM}
	\centering
	\includegraphics[width=1.0\textwidth]{\pathProbLibPictures form.jpg}
	\caption{FORM: example of iteration path to design point}
\end{figure}

\subsubsection{Starting point}

The FORM algorithm starts at a specified starting point. In most cases, beginning at the default location is sufficient.

The available starting point options are the same as those for Importance Sampling (see \Autoref{sec:StartPoint}).

\subsubsection{Loops}

In case calculation options have been specified, which do not lead to convergence, loops can be used to modify the calculation options. If the number of loops is greater than $1$, the relaxation factor $f_\text{relax}$ is modified until convergence is reached. The relaxation factor is modified as follows:

\begin{align}
\label{eq:FORMRelax}
f_\text{relax} = \tfrac{1}{2} \cdot f_\text{relax, prev}
\end{align}

where:

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$f_\text{relax, prev}$ & is the relaxation factor in the previous loop. \\
\end{longtable}

\subsection{FORM then Directional Sampling}
\label{sec:FDIR}

The "FORM then Directional Sampling" method is a reliability approach based on FORM (see \Autoref{sec:FORM}) and Directional Sampling (see \Autoref{sec:DirectionalSampling}).

The method begins with a FORM calculation. If it succeeds (FORM converges), the process is complete, and the FORM result is returned. If FORM fails to converge, a Directional Sampling calculation is performed instead, and its result is used.

The rationale behind this approach is to leverage FORM whenever possible due to its high efficiency while providing a fallback to Directional Sampling in cases where FORM does not converge.

\subsection{Directional Sampling then FORM}
\label{sec:DSFI}

The "Directional Sampling then FORM" method is a reliability approach based on Directional Sampling (see \Autoref{sec:DirectionalSampling}) and FORM (see \Autoref{sec:FORM}).

The method begins with a Directional Sampling reliability calculation, followed by a FORM calculation. It returns the reliability index ($\beta$) from Directional Sampling and the $\alpha$-values from FORM. Since FORM starts from the design point identified by Directional Sampling, it typically requires only a few iterations.

The rationale behind this approach is that Directional Sampling quickly estimates the $\beta$ value but requires many samples to accurately determine all $\alpha$-values. In contrast, FORM efficiently refines the $\alpha$-values when initialized close to the design point.

Note that the starting point of FORM depends on the method for "Contribution per variable" (see \autoref{sec:Contribution per variable}).

\section{Contribution per variable}
\label{sec:Contribution per variable}

For all numerical integration-based methods (Numerical Integration and Numerical Bisection) and Monte Carlo-based methods (Crude Monte Carlo, Importance Sampling, Directional Sampling, and Subset Simulation), the contribution per variable is calculated after determining the reliability index ($\beta$). The appliaed methods are explained in the following sections.

\subsection{Center of gravity}
\label{sec:CenterOfGravity}

The "Center of gravity" method calculates the weighted mean of all failing realizations. This is done as follows:

\begin{align}
\label{eq:CenterOfGravity}
\alpha_{\text{j}} =  f_{\text{normal}} \cdot \frac{\sum_{\text{failing realizations i}} u_{\text{i,j}} \cdot W_{\text{i}}}{\sum_{\text{failing realizations i}} W_{\text{i}}} 
\end{align}

where:
\begin{longtable*}{p{20mm}p{\textwidth-24pt-20mm}}  
	$i$ & indicates a realization in the applied technique \\
	$j$ & indicates a variable \\
	$u$ & is a realization in the applied technique \\
	$W_{\text{i}}$ & is the weight of the realization (for Crude Monte Carlo: $1$) \\
	$f_{\text{normal}}$ & is a normalizing factor so that $\sum \alpha^2_{\text{j}} = 1$
\end{longtable*}

\subsection{Center of angles}
\label{sec:CenterOfAngles}

In the "Center of angles" method, all realizations are defined using spherical coordinates instead of Cartesian coordinates. The weighted mean of the angles in the spherical coordinates is then calculated and converted back into Cartesian coordinates.

\subsection{Nearest to mean}
\label{sec:NearestToMean}

The "Nearest to mean" method selects the realization with the highest probability density from all realizations that are considered failing according to the failure definition.

Although this is the fastest method, it is not recommended for Monte Carlo techniques. The results are affected by the inherent randomness of Monte Carlo methods, which can lead to unreliable outcomes. For instance, unimportant variables may be assigned significant $\alpha$ values due to this randomness.
