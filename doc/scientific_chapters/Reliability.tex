\chapter{Reliability}
\label{sec:Reliability}


\section{Reliability algorithms}
\label{sec:ReliabilityTechniques}

There are a number of ways the reliability index can be calculated. They all have their own advantages and disadvantages.

\subsection{Numerical integration}
\subsubsection{Algorithm}

Numerical Integration is the most time consuming,
but most exact way to calculate the failure probability.
A step size and minimum and maximum values of the input variables are needed for the evaluation.

When the underlying model does not succeed for a certain realization,
its results will be ignored.
The user should decide whether the number of non succeeded realizations is acceptable.

The minimum and maximum value for which the integration will run,
are defined in the u-space.
The numerical integration will fill up the left over space between these values and -8 and 8 by additional cells,
so the whole integration domain will be covered always.
(Note that u=8 corresponds to a probability of $\approx 6*10^{-16}$,
which is orders of magnitude smaller than probabilities of common situations.)

\begin{figure}[H]
	\label{fig:NumericalIntegration}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/numericalintegration.jpg}
	\caption{Numerical integration: realizations for two stochasts}
\end{figure}

%\subsection{Numerical bisection}
% TODO: Numerical bisection only in PTK, not in Streams
%\subsubsection{Algorithm}

%Numerical bisection is similar to numerical integration,
%but now the integration cells are generated by bisection of the complete domain.
%As soon as an integration cell has same qualitative results
%(fail, not fail, not counting) on its corner points,
%we assume that all values inside the cell would deliver the same qualitative result
%and no points will be calculated within the call nor on its sides.
%As long as cells exist with different qualitative results,
%the cell will be split and new corner points will be calculated.

%This process ends when the cells, which don't have a similar result,
%represent a probability lower than an accepted allowed difference in the reliability.
%This value can be supplied by the user.

%\begin{figure}[H]
%	\label{fig:NumericalBisection}
%	\centering
%	\includegraphics[width=1.0\textwidth]{pictures/numericalbisection.jpg}
%	\caption{Numerical bisection: realizations for two stochasts}
%\end{figure}

\subsection{Crude Monte Carlo}
\label{sec:CrudeMonteCarlo}

\subsubsection{Algorithm}

Crude Monte Carlo is the "standard" Monte Carlo simulation. Random realizations will be generated proportional to their probability density. It will be counted how many realizations lead to failure and how many realizations do not lead to failure. The probability of failure is calculated as follows:

\begin{align} 
\label{eq:MonteCarlo}
p_{\text{failure}} = \frac{N_{\text{failure}}}{N}
\end{align}

where
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$p_{\text{failure}}$ & is the probability of failure; \\  
	$N_{\text{failure}}$ & is the number of realizations which were interpreted as failing; \\ 
	$N$ & is the total number of realizations.
\end{longtable}

\subsubsection{Convergence}
\label{sec:MCConvergence}

The Monte Carlo simulation also leads to a standard deviation of the probability of failure. This standard deviation $\sigma_p$ is based on a confidence level $\alpha$. 

\begin{align} 
\label{eq:MonteCarloStandardDeviation}
\sigma_p = z \sqrt{\frac{p \left(1 - p\right)}{N}}
\end{align}

with

\begin{align} 
\label{eq:MonteCarloStandardConfidence}
z = \Phi^{-1}\left(1 - \frac{\alpha}{2}\right)
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\sigma_p$ & is the standard deviation of the probability of failure; \\  
	$p$ & is the probability of failure $p_{\text{failure}}$; \\
	$N$ & is the total number of realizations; \\
	$z$ & is the quantile of the standard normal distribution corresponding with $\alpha$.
	$z$ is 1 in \probLib; \\
	$\Phi$ & is the CDF of the standard normal distribution (see \autoref{eq:StandardNormalCDF}); \\
	$\alpha$ & is the confidence level.
\end{longtable}

The standard deviation $\sigma_p$ can be expressed as a variation coefficient $\varepsilon$ as follows:

\begin{align} 
\label{eq:MonteCarloVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p} = z \sqrt{\frac{1 - p}{N p}}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)} = z \sqrt{\frac{p}{N \left(1 - p\right)}}
\end{array}	
\right.
\end{align}

The user can set the maximum variation coefficient $\varepsilon_{\text{max}}$. The Monte Carlo simulation will stop when $\varepsilon \leq \varepsilon_{\text{max}}$ (and limited to the minimum and maximum number of realizations). The confidence level $\alpha$ can not be set by the user. Its value is such that $z$ is 1.

\begin{figure}[H]
	\label{fig:MonteCarloConvergence}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/montecarloconvergence.jpg}
	\caption{Crude Monte Carlo convergence}
\end{figure}

When the underlying model does not succeed for a certain realization, its results will be ignored.
The user should decide whether the number of non succeeded realizations is acceptable.

The minimum and maximum value for which the integration will run, are defined in the u-space.
The Monte Carlo analysis will check whether the left over area fails or not. 

\begin{figure}[H]
	\label{fig:MonteCarlo}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/montecarlo.jpg}
	\caption{Crude Monte Carlo: realizations for two stochasts}
\end{figure}

\subsection{Importance sampling}
\label{sec:ImportanceSampling}

\subsubsection{Algorithm}
\label{sec:ImportanceSamplingAlgorithm}

Importance sampling is a variation on Crude Monte Carlo (see \autoref{sec:CrudeMonteCarlo}). The difference is that realization are selected in a smarter way, preferably in the area near the limit state.

Usually Monte Carlo realizations are drawn proportional to their probability density $\varphi\left(u_i\right)$,
since \probLib uses the standard normal space.
With importance sampling each realization $u$ is translated to $u_{\text{imp}}$. 
\probLib supports the following translation for each variable separately:

\begin{align} 
\label{eq:ImportanceSamplingVariance}
u_{\text{imp}} = \mu_{\text{var}} + \sigma_{\text{var}} \cdot u_{\text{var}}
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\mu_{\text{var}}$ & The user defined mean value per variable. The combination of all variables is the mean realization of the importance sampling algorithm. \\
	$\sigma_{\text{var}}$ & The user defined standard deviation per variable
\end{longtable}

A correction is applied in the calculation of failure to compensate for this translation. This is done by giving each realization a weight, which is calculated as follows (the multiplication with $\sigma_{\text{var}}$ is performed to compensate for the dimensionality):

\begin{align} 
\label{eq:ImportanceSamplingVarWeight}
w_{\text{var}} = \frac{\sigma_{\text{var}} \cdot \varphi\left(u_\text{imp}\right)}{\varphi\left(u_{\text{var}}\right)}
\end{align}

and 

\begin{align} 
\label{eq:ImportanceSamplingWeight}
W_{\text{realization}} = \prod_{\text{variables}} w_{\text{var}}
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$\varphi$ & is the standard normal probability density function, see \autoref{eq:StandardNormalPDF}; \\  
	$w_{\text{var}}$ & is the weight factor per variable per realization; \\
	$W_{\text{realization}}$ & is the weight factor of the realization; \\
\end{longtable}

Corresponding to \autoref{eq:MonteCarlo}, the probability of failure is equal to:

\begin{align} 
\label{eq:ImportanceSampling}
p_{\text{failure}} = \frac{\sum_{\text{failing realizations}} W}{\sum_{\text{all realizations}} W}
\end{align}

To avoid numerical issues and because 

\begin{align} 
\label{eq:ImportanceSamplingLimit}
\lim_{N \to \infty} {\sum_{\text{all realizations}} W} = N
\end{align}

The probability of failure is calculated as follows:

\begin{align} 
\label{eq:ImportanceSamplingCalculated}
p_{\text{failure}} = \frac{\sum_{\text{failing realizations}} W}{N}
\end{align}

\subsubsection{Convergence}

Corresponding to \autoref{eq:MonteCarloStandardDeviation} and \autoref{eq:MonteCarloVariationCoefficient}, the standard deviation and variance coefficient become:

\begin{align} 
\label{eq:ImportanceSamplingStandardDeviation}
\sigma_p = z \sqrt{\frac{p \left(W_{\text{design point}} - p\right)}{N}}
\end{align}

and 

\begin{align} 
\label{eq:ImportanceSamplingVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p} = z \sqrt{\frac{W_{\text{design point}} - p}{N p}}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)} = z \sqrt{\frac{W_{\text{design point}} - \left(1 - p\right)}{N \left(1 - p\right)}}
\end{array}	
\right.
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$z$ & is a value related to the confidence level (see \autoref{eq:MonteCarloStandardConfidence}). In \probLib, $z$ is 1; \\
	$W_{\text{design point}}$ & is the weight of the realization of the design point; \\  
	$N$ & is the total number of realizations; \\  
\end{longtable}

When the underlying model does not succeed for a certain realization, its results will be ignored. The user should decide whether the number of non succeeded realizations is acceptable.

\begin{figure}[H]
	\label{fig:ImportanceSampling}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/ImportanceSampling.jpg}
	\caption{Importance sampling: realizations for two stochasts}
\end{figure}

\subsubsection{Mean realization}
\label{sec:StartPoint}

The mean realization (see {\autoref{eq:ImportanceSamplingVariance}) is essential for the successful behaviour of the importance sampling algorithm.
The mean realization can be user defined or derived automatically.
The following options are available:
	
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	None & The mean realization is user defined. \\
	Direction & Along a user defined direction is searched until the limit state is found, limited by a maximum distance from the origin. This option still requires some knowledge where to find the mean realization.  \\
	Sphere & Over a sphere is searched until the limit state is found. Only at angles 0$^\circ$, 90$^\circ$, 180$^\circ$ and 270$^\circ$ is searched. If the limit state is not found, the radius of the sphere is increased. This is a time consuming option. \\  
	Sensitivity & Along the direction of the steepest gradient is searched until the limit state is found, limited by a maximum distance from the origin. The steepest gradient is found by taking the gradient for each variable individually and then combining them. This is the preferred option. \\  
\end{longtable}

\subsection{Adaptive importance sampling}
\subsubsection{Algorithm}

Importance sampling is sensitive to the user given start point. If not chosen well, importance sampling might not give satisfactory results. If the start point is difficult to specify, loops can be used. After each loop, the start point is improved.

Multiple start points can be used. Therefore the number of clusters should be set to the desired number of clusters. By using the K-Means algorithm, multiple start points are detected from the samples which lead to failure in the previous loop.

This algorithm is displayed in {\autoref{fig:ImportanceSamplingLoops}}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}[auto, scale=0.6]
	\node [round] (Start) {Start};
	\node [block, right=of Start] (Initialize) {Initialize start point};
	\node [block, below=of Initialize] (ImportanceSampling) {Importance sampling};
	\node [block, below=of ImportanceSampling] (IfConverged) {Converged?};
	\node [block, below=of IfConverged] (IfFinalRound) {Final round?};
	\node [block, left= of IfFinalRound] (IfEnoughFailures) {Enough failures?};
	\node [block, right= of IfFinalRound] (Recalculate) {Recalculate};
	\node [round, below=of Recalculate] (End) {End};
	\node [block, above=of IfEnoughFailures] (Increase) {Increase variance};
	\node [block, left=of Increase] (Move) {Use design point};
	
	\draw [->] (Start) -- (Initialize);
	\draw [->] (Initialize) -- (ImportanceSampling);
	\draw [->] (ImportanceSampling) -- (IfConverged);
	\draw [->] (IfConverged) -| node {yes} (Recalculate);
	\draw [->] (IfConverged) -- node {no} (IfFinalRound) ;
	\draw [->] (IfFinalRound) -- node {yes} (Recalculate) ;
	\draw [->] (Recalculate) -- (End);
	\draw [->] (IfFinalRound) -- node {no} (IfEnoughFailures);
	\draw [->] (IfEnoughFailures) -- node {no} (Increase);
	\draw [->] (IfEnoughFailures) -| node {yes} (Move);
	\draw [->] (Increase) |- (ImportanceSampling);
	\draw [->] (Move) |- (ImportanceSampling);
	
	\end{tikzpicture}
	\caption{Importance sampling loops}
	\label{fig:ImportanceSamplingLoops}
\end{figure}

These loops are executed until a required fraction of failed realizations $\varepsilon_\text{failed}$ is reached:

\begin{align}
\label{eq:ISFractionFailed}
\text{min}\left(\frac{N_\text{failed}}{N}, 1 - \frac{N_\text{failed}}{N}\right) \geq \varepsilon_\text{failed}
\end{align}

where
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$N$ & is the total number of failed realizations; \\
	$N_\text{failed}$ & is the number of failed realizations;
\end{longtable}

The following is performed in each step.

\begin{longtable}{p{40mm}p{\textwidth-24pt-40mm}}  
	Initialize start point & Find the start point with one of the start point algorithms (see \autoref{sec:StartPoint}). \\
	Importance sampling & The importance sampling algorithm (see \autoref{sec:ImportanceSamplingAlgorithm}). The number of samples in each importance sampling can be set to a maximum. When the auto break option is used, the algorithm decides when to break a loop and continue with the next loop (see \autoref{sec:AutoBreakLoops}). \\
	Converged? & Checks whether there is convergence (see \autoref{eq:ISFractionFailed})  \\
	Final round? & Checks whether the last allowed round is reached, in order to prevent endless loops  \\
	Recalculate & Recalculates the last round with a possibly higher number of realizations. \\
	Enough failures? & Checks whether enough failures are found. Depending on this value, the kind of modification of importance sampling settings is determined. \\
	Increase variance & Increases the variance (see \autoref{sec:ImportanceSamplingAlgorithm}) for the next loop. \\
	Use design point & Uses the design point found in the last loop as start point in the next loop. In case no design point was found, the realization closest to the limit state is used. \\
\end{longtable}

The following features are available to reduce calculation time.

\begin{itemize}
	\item The maximum number of realizations for non final loops can be specified, which can be different to the maximum number of realizations of the final loop.
	\item Subsequent loops can be suppressed when the reliability index exceeds a certain value found in the first loop
	\item When the design point is copied to the next loop, it will be rounded to a certain value. When using scenario tables, previous calculation results can be reused. Specify 0 when no rounding is desired.
\end{itemize}

\paragraph{Auto break loops}
\label{sec:AutoBreakLoops}

The number of realizations in the loops in adaptive importance sampling can be determined automatically. This algorithm is based on the convergence criterion $\varepsilon_\text{weight}$, which specifies the maximum relative weight of all failing samples. Usually this value is 0.1.

The loop is broken when the required number of runs with the current start point ($n_\text{additional}$) is more than the expected number of runs with an improved start point ($n_\text{expected}$):

\begin{align}
\label{eq:ISAutoBreak}
n_\text{additional} > n_\text{expected} 
\end{align}

with 

\begin{align}
\label{eq:ISAutoBreakAdditional}
n_\text{additional} = n_\text{current} \cdot \left(\frac{W_\text{max}}{W_\text{total} \cdot \varepsilon_\text{weight}} - 1\right)
\end{align}

and 

\begin{align}
\label{eq:ISAutoBreakExpected}
n_\text{expected} = \frac{2 \left(\beta_\text{current} + 1\right)}{\varepsilon_\text{weight}}
\end{align}

where 

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$n_\text{current}$ & the number of samples already made in the current loop. \\
	$W_\text{max}$ & the maximum weight of a failed sample in the current loop. \\
	$W_\text{total}$ & the total weight of the failed samples in the current loop. \\
	$\beta_\text{current}$ & the reliability index based on the samples in the current loop. \\
	$\varepsilon_\text{weight}$ convergence criterion,
	which specifies the maximum relative weight of all failing samples. Default is 0.1.
\end{longtable}


\subsection{Subset simulation}
\subsubsection{Algorithm}

Subset simulation [\cite{Au2001}] is a staged form of Crude Monte Carlo (see \autoref{sec:CrudeMonteCarlo}). There are several iterations, the first being the same as Crude Monte Carlo. The simulation stops when the Crude Monte Carlo convergence criterion is met: when $\varepsilon \leq \varepsilon_{\text{max}}$  (see \autoref{eq:MonteCarloVariationCoefficient}).

When the criterion is not met, a new Crude Monte Carlo is taken using on the $k$ fraction of the previous Monte Carlo run. The realizations closest to failure are taken and from there new realizations are generated using a Markov Chain. Per old realization $\tfrac{1}{k}$ new realizations are generated. A new realization is generated in the following way with u-values for each variable $var$:

\begin{align} 
\label{eq:SubsetSimulationMarkovChain}
u_{\text{var, new}} = \left\{\begin{array}{ll} r_{\text{var}} \geq R_{\text{[0, 1]}}  & u_{\text{var, prop}}\\
r_{\text{var}} < R_{\text{[0, 1]}}  & u_{\text{var, prev}}
\end{array}	
\right.
\end{align}

with 

\begin{align} 
\label{eq:SubsetSimulationRatio}
r_{\text{var}} = \frac{\varphi \left(u_{\text{var, prop}}\right)}{\varphi \left(u_{\text{var, prev}}\right)}
\end{align}

and

\begin{align} 
\label{eq:SubsetSimulationProposed}
u_{\text{var, prop}} = u_{\text{var, prev}} + R_{\text{[-1, 1]}} \cdot \Delta \sigma
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$u_{\text{var, prev}}$ & is the $u$-value per variable of the previous realization; \\
	$u_{\text{var, prop}}$ & is a proposed new $u$-value per variable; \\
	$u_{\text{var, new}}$ & is the new $u$-value per variable; \\
	$R_{\text{[0, 1]}}$ & is a random value between 0 and 1; \\  
	$R_{\text{[-1, 1]}}$ & is a random value between -1 and 1; \\  
	$r_{\text{var}}$ & is the ratio of probability density between $u_{\text{var, prop}}$ and $u_{\text{var, prev}}$; \\  
	$\varphi$ & is the probability density function of a standard normal distribution (see \autoref{eq:StandardNormalPDF}); \\  
	$\Delta \sigma$ & is the user given maximum deviation from the previous $u$-value.
	In case the option 'Adaptive Conditional' is used,
	this value is derived automatically and updated during the process [\cite{PAPAIOANNOU201589}].
\end{longtable}

The proposed sample $u_{\text{prop}}$ is used if the corresponding z-value is less than the $z_{\text{k}}$, which is the highest z-value in the subset used to generate new realizations, otherwise the original sample is used.

The probability of failure is calculated as follows:

\begin{align} 
\label{eq:SubsetSimulation}
p = k^{\text{i}} \cdot p_{\text{i, MC}}
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$i$ & is the iteration index, the first being 0; \\
	$k$ & is the fraction taken from the realizations (recommended value 0.1); \\  
	$p_{\text{i, MC}}$ & is the probability of failure calculated with Crude Monte Carlo using the realizations of the $i^{\text{th}}$ iteration.
\end{longtable}

\subsection{Directional sampling}
\label{sec:DirectionalSampling}

\subsubsection{Algorithm}

Directional sampling is a kind of Monte Carlo simulation where realizations are directions instead of realizations. A direction is a direction in the parameter space. Along this direction the point of failure is searched and its corresponding distance $\beta$ to the origin is determined. The remaining probability of failure beyond this point is calculated and added to the total probability of failure as follows:

\begin{align} 
\label{eq:DirectionalSampling}
p_{\text{failure}} = \frac{\sum w_{\text{dir}}}{N_{\text{realizations}}}
\end{align}

with 

\begin{align} 
\label{eq:DirectionalSamplingWeight}
w_{\text{dir}} = \Gamma \left(\frac{N_{\text{variables}}}{2} , \frac{{\beta_{\text{dir}}}^2}{2} \right)
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$w_{\text{dir}}$ & is the weight of a direction or the failure probability in this direction; \\
	$\beta_{\text{dir}}$ & is the distance along a direction where the first point of failure is found; \\
	$\Gamma$ & is the upper incomplete regularized gamma function.
\end{longtable}

\subsubsection{Convergence}

The standard deviation of the failing probability is calculated as follows:

\begin{align} 
\label{eq:DirectionalSamplingStandardDeviation}
\sigma_p = \sqrt{\frac{\sum \left(w_{\text{dir}} - p \right)^2}{N \cdot \left(N - 1\right)}}
\end{align}

The variation coefficient is calculated as follows

\begin{align} 
\label{eq:DirectionalSamplingVariationCoefficient}
\varepsilon = \left\{\begin{array}{ll}p < \tfrac{1}{2} & \tfrac{\sigma_p}{p}\\
p \geq\tfrac{1}{2} & \tfrac{\sigma_p}{\left(1-p\right)}
\end{array}	
\right.
\end{align}

The directional sampling simulation stops if the variation coefficient $\varepsilon$ is less than the user given maximum variation coefficient $\varepsilon_{\text{max}}$. Also, the number of minimum and maximum directions should be satisfied.

The minimum and maximum number of iterations refer to an internal procedure to determine the point along the direction where failure occurs.

\begin{figure}[H]
	\label{fig:DirectionalSampling}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/DirectionalSampling.jpg}
	\caption{Directional sampling: realizations for two stochasts}
\end{figure}

%\subsection{Latin hypercube}
% TODO: Latin hypercube only in PTK, not in Streams
%\subsubsection{Algorithm}

%The Latin hypercube algorithm is a kind of Monte Carlo sampling method. 

%Latin hypercube uses a fixed number of realizations $N$.
%The algorithm divides the u-space in $N$ equal sections,
%starting from $u_\text{min}$ and ending at $u_\text{max}$.
%The algorithm ensures that each section is represented in the number of realizations.
%In this way it is ensured that extreme values are used.
%Note that the weights of realizations differ from each other.

%If $u_\text{min}$ and ending at $u_\text{max}$ differ from -8 and 8
%(beyond these values no calculation is possible due to numerical reasons),
%the remainder is regarded as one cell and is used in the failure analysis.

%Latin hypercube uses the same definition for convergence as Crude Monte Carlo
%(see \autoref{sec:MCConvergence}).
%It is only calculated for informative reasons,
%because a fixed number of realizations is used.

%The advantage of Latin hypercube is calculation time.
%But it only gives a rough approximation of the probability of failure.

%\subsection{Cobyla}
% TODO: Cobyla only in PTK, not in Streams
%\subsubsection{Algorithm}

%The Cobyla algorithm searches for the point in the u space
%with the highest probability of density and which indicates failure.
%This point is assumed to be representative for the design point.

%The Cobyla algorithm is an implementation of
%Powell's nonlinear derivative-free constrained optimization
%that uses a linear approximation approach.
%The algorithm is a sequential trust-region algorithm that
%employs linear approximations to the objective and constraint functions,
%where the approximations are formed by linear interpolation at n + 1 points
%in the space of the variables and tries to maintain
%a regular-shaped simplex over iterations.

%The Cobyla algorithm only gives a rough estimation
%of the reliability and design point.
%It should only be used as a first method in a reliability study. 

\subsection{FORM}
\label{sec:FORM}

\subsubsection{Algorithm}

FORM is an acronym for First Order Reliability Method. The FORM procedure is not a Monte Carlo simulation, but searches directly for the design point. The design point is regarded to be representative for the total failure probability according to

\begin{align}
p_\text{failure} = 1 - \Phi \left(\beta\right)
\end{align}

The FORM procedure start from a certain user defined start point in the parameter space. From there it tries to find a point which is closer to the design point by taking the gradient of the Z-value in the u-space of the parameters. The Z-value is an indication whether failure occurs and is derived from the failure definition. After a number of steps, the point is close enough to the design point and the calculation stops. 

The FORM analysis searches for the design point: When the design point is found, the reliability index $\beta$ can be evaluated as the distance between the origin and the design point (see \autoref{fig:FORMDesignPoint}). The corresponding probability of failure is

\begin{align}
p_\text{failure} = 1 - \Phi \left(\beta\right)
\end{align}

where $\Phi$ is the cumulative density function in the standard normal space (see \autoref{eq:StandardNormalCDF}).

The probability of failure found in this way is regarded to be a good approximation of the "real" probability of failure. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1.00\textwidth]{pictures/FORMDesignPoint.png}
	\caption{Schematic representation of FORM}
	\label{fig:FORMDesignPoint}
\end{figure}

To find the design point, the FORM analysis starts at a given start point, usually the origin, in the standard normal space and iterates to the design point.

In each iteration step, the current point is moved closer to the design point. Using the steepest descend, the next point is found, until a convergence criterion is fulfilled. To get from the current point $u_{\text{i}}$ to the next point $u_{\text{i+1}}$, the predicted point $u_{\text{pred}}$ is determined. 

To calculate $u_{\text{pred}}$, we assume that $z$ is linear with $u$ close to the design point. The gradient is taken between $u_i$ and $u_\text{pred}$: 

\begin{align}
\label{eq:UZLinear}
\lvert \frac{\partial z}{\partial u} \rvert = \frac{z_\text{pred} - z_i}{u_\text{pred} - u_i} = - \frac{z_i}{u_\text{pred} - u_i}
\end{align}

which is equivalent to 

\begin{align}
\label{eq:BetaPredicted}
u_\text{pred} = u_i - \frac{z_i}{\lvert \frac{\partial z}{\partial u} \rvert }
\end{align}

where
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$i$ & is the iteration index number; \\  
	$\lvert \frac{\partial z}{\partial u} \rvert$ & is the calculated vector length of the derivative to all variables; \\ 
	$z_\text{pred}$ & is the limit state value at the design point, which is by definition equal to zero; \\  
	$z_i$ & is the calculated limit state value at iteration $i$; \\  
	$u_\text{pred}$ & is the predicted vector of the design point; \\  
	$u_i$ & is the iteration vector of $u$ at iteration $i$; \\  
\end{longtable}

The step from the current point to the next point is a step into the direction of $u_{\text{pred}}$. The step is not taken completely, but partially with a relaxation factor $f_{\text{relax}}$, in order to prevent numerical instabilities.

\begin{align}\label{eq:FORM}
u_\text{j, i+1} = - \alpha_\text{j, i} \cdot \lvert  u_\text{i, pred} \rvert \cdot f_\text{relax} + u_\text{j, i} \cdot \left(1 - f_\text{relax}\right)
\end{align}

with the $\alpha$ value per variable

\begin{align}\label{eq:Alpha}
\alpha_{j} = \frac{\frac{\partial z_\text{j}}{\partial u_\text{j}}}{\lvert \frac{\partial z}{\partial u} \rvert}
\end{align}

where
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$j$ & is the index of a variable; \\  
	$\lvert u \rvert$ & is the vector length of the current point; \\  
	$f_\text{relax}$ & is the given relaxation factor. The relaxation factor prevents the algorithm from too big steps, which may cause divergence; \\  	
\end{longtable}

and finally leads to the reliability index $\beta$ as

\begin{align}
\label{eq:FORMBeta}
\beta = \lvert u_i \rvert
\end{align}

The benefit of this method is that it is quick. The disadvantages are that a local design point can be found, which then corresponds to a non representative failure probability. Another disadvantage is that numerical problems may occur, since the Z-value must be continuous.

One cannot determine whether a design point is a local design point, so the user must use this method with care and after analysis whether this method is suitable for the kind of models he uses. Numerical problems can be detected in the convergence chart. If this occurs, one can decrease the relaxation factor or modify the gradient step size (both directions could improve or decrease results).

\subsubsection{Convergence}

Convergence is reached when the $u_i$, the value of the last iteration $i$, is close enough to the predicted value $u_{\text{pred}}$, so

\begin{align}
\label{eq:FORMConvergence}
\lvert u_i - u_\text{pred} \rvert<\varepsilon
\end{align}

where
\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$u_i$ & is the point at iteration $i$; \\
	$u_\text{pred}$ & is the predicted final value of $u$, see \autoref{eq:UZLinear} ; \\
	$\varepsilon$ & is a user given value maximum allowed difference in reliability; \\  
\end{longtable}

and, together with \autoref{eq:FORMConvergence}, the convergence criterion 

\begin{align}
\label{eq:FORMConv1}
\frac{\lvert z \rvert}{\lvert \frac{\partial z}{\partial u} \rvert } < \varepsilon
\end{align}

Alternatively to \autoref{eq:UZLinear}, the gradient is taken between $u_0$ (all u-values equal to 0) and $u_\text{pred}$. Then we get for the linear relation

\begin{align}
\label{eq:UZLinear2}
\lvert \frac{\partial z}{\partial u} \rvert = \frac{z_\text{pred} - z_0}{u_\text{pred} - u_0} = - \frac{z_0}{u_\text{pred}}
\end{align}

The value $z_0$ is calculated with the value for z in the last iteration $z_i$ and the gradient

\begin{align}
\label{eq:UZLinear3}
z_0 = z_i - \sum_j \frac{\partial z_i}{\partial u_j} \cdot u_j
\end{align}

which leads to

\begin{align}
\label{eq:BetaPredicted2}
\lvert u_\text{pred}\rvert = \frac{\lvert z_0 \rvert}{\lvert \frac{\partial z}{\partial u} \rvert }
\end{align}

The convergence criterion is defined as the relative difference in length of $u_\text{pred}$ and $u$:

\begin{align}
\label{eq:FORMConv2}
\frac{\lvert u_\text{pred}^2 - u^2 \rvert}{u^2} < \varepsilon
\end{align}

\begin{figure}[H]
	\label{fig:FORM}
	\centering
	\includegraphics[width=1.0\textwidth]{pictures/form.jpg}
	\caption{FORM: example of iteration path to design point}
\end{figure}

\subsubsection{Start point}

The FORM algorithm starts at a certain start point. Most of the times starting at the default location is sufficient.

The start point options are the same as the start point options of importance sampling (see \autoref{sec:StartPoint}).

\subsubsection{Loops}

In case calculation options have been specified, which do not lead to convergence, loops can be used to modify the calculation options. If the number of loops is greater than 1, the relaxation factor $f_\text{relax}$ is modified until convergence is reached. The relaxation factor is modified as follows:

\begin{align}
\label{eq:FORMRelax}
f_\text{relax} = \tfrac{1}{2} \cdot f_\text{relax, prev}
\end{align}

where

\begin{longtable}{p{16mm}p{\textwidth-24pt-16mm}}  
	$f_\text{relax, prev}$ & is the relaxation factor in the previous loop. \\
\end{longtable}

\section{Contribution per variable}
\label{sec:Contribution per variable}

% TODO Numerical Bisection not in Streams yet
For all numerical integration based (Numerical Integration) and Monte Carlo based (Crude Monte Carlo, Importance Sampling, Directional Sampling and Subset Simulation) reliability techniques, the contribution per variable is calculated after the calculation of the reliability index. 

In the next subsections various methods will be explained.

\subsection{Center of gravity}
\label{sec:CenterOfGravity}

The center of gravity method takes the weighted mean from all failing realizations. This is calculated as follows:

\begin{align}
\label{eq:CenterOfGravity}
\alpha_{\text{j}} =  f_{\text{normal}} \cdot \frac{\sum_{\text{failing realizations i}} u_{\text{i,j}} \cdot W_{\text{i}}}{\sum_{\text{failing realizations i}} W_{\text{i}}} 
\end{align}

where
\begin{longtable*}{p{20mm}p{\textwidth-24pt-20mm}}  
	$i$ & indicates a realization in the applied technique; \\
	$j$ & indicates a variable; \\
	$u$ & is a realization in the applied technique; \\
	$W_{\text{i}}$ & is the weight of the realization (for Crude Monte Carlo: 1); \\
	$f_{\text{normal}}$ & is a normalizing factor so that $\sum \alpha^2_{\text{j}}$ = 1;
\end{longtable*}

\subsection{Center of angles}
\label{sec:CenterOfAngles}

In this method all realizations are defined with spherical coordinates instead of cartesian coordinates. Then the weighted mean of the angles in the spherical coordinates is taken and converted back to cartesian coordinates.

\subsection{Nearest to mean}
\label{sec:NearestToMean}

This method takes from all realizations, which were regarded as failing according to the failure definition, the realization with the highest probability density.

Although this is the fastest method, this method is not recommended for Monte Carlo techniques. Its results suffer from the randomness in Monte Carlo methods. Unimportant variables may get a significant alpha value for the same reason.
