\chapter{Conversions}\label{chp:Conversions}

\section{Background}
\subsection{Floating point computations}
In the \probLib three types of real (floating point numbers) are defined:
\begin{itemize}
\item {\tt  integer, parameter :: sp = selected\_real\_kind(6, 37) !< Single precision}
\item {\tt  integer, parameter :: dp = selected\_real\_kind(15, 307) !< Double precision}
\item {\tt integer, parameter :: qd = selected\_real\_kind(33, 4931) !< Quadruple precision}
\end{itemize}

The quadruple precision ({\tt kind=qd}) is only applied for the testing. Types {\tt sp} and {\tt dp} correspond to the IEE 754 standard for single and double precision:
\begin{itemize}
\item single precision: 32-bit number with 1 sign bit for positive/negative, 8 bit exponent (in excess-127) and significant precision of 23 bits. This gives a range of $\pm 1.18\cdot10^{-38}$ to $\pm 3.4\cdot10^{38}$.
\item double precision: 64-bit number with 1 sign bit for positive/negative, 11 bit exponent (in excess-1023) and significant precision of 52 bits. This gives a range of $\pm 2.23\cdot10^{-308}$ tot $\pm 1.80\cdot10^{308}$.
\end{itemize}

\Note{the \probLib usually deals with very small probabilities. Therefore all computations are performed in the double precision.}

Small rounding errors are always made when calculating with floating point numbers. Floating point numbers consist of a finite number of bits and so not all real numbers (infinitely many) can have their own unique floating point representation. There are numbers that differ so little from each other that they are equal in floating point notation. The term {\bf machine precision} is a measure of the maximum error made in this rounding and is often defined as the smallest number $\epsilon $ for which it still holds that $1+\epsilon>1$.
\begin{itemize}
\item machine precision for single precision reals $\approx 1.19e^{-07}$ \\
\item machine precision for double precision reals $\approx 2.22e^{-16}$ \\
\end{itemize}
In practice, the first 7 decimal places are significant for single precision numbers, and 16 for double precision. Because of this finite accuracy, calculations that are identical mathematically cannot produce the same calculation result. Sometimes the order in which a calculation is performed can make a big difference in the final answer.
Another possibility for the occurrence of (large) differences is to subtract two floating point numbers that are almost equal to each other. The significant figures are lost in the processing and the result contains less or possibly no significant decimal anymore.

\subsection{The (inverse) cumulative distribution function (CDF)}
The cumulative distribution function (CDF) or a random variable $X$ is a function defined as follows, for $x\in\Re$:
\begin{equation}
  F_X(x) = P(X \le x)
\end{equation}

In case of continuous random variables, the CDF can be written in terms of the probability density function $f_X$:
\begin{equation}
F_X(x) = \int_{-\infty}^{\infty} f_X(t) dt
\end{equation}

The inverse function of the CDF is by definition the function that returns the value of $x$ for which the probability is less than or equal to a given probability. The inverse CDF is also called {\bf percent-point} function or {\bf quantile} function. In terms of $F_X $, the inverse CDF returns $x$ for the given $p$ so that:
\begin{equation}
F_X(x) := P(X \le x) = p
\end{equation}

\subsection{The (inverse) of the standard normal distribution function}
The CDF of the standard normal distribution function, often denoted by $\Phi$, is defined as follows:
\begin{equation}
\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-\frac{1}{2}t^2} dt
\end{equation}

There is a direct relation between $\Phi$ and the error function ($\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt$):
\begin{eqnarray}
  \Phi(x)       &=& \frac{1}{2} \left[ 1 + \text{erf}\left( \frac{x}{\sqrt{2}} \right) \right]\\
  \text{erf}(x) &=& 2 \Phi(x\sqrt{2}) - 1 \label{eq:erf-phi}
\end{eqnarray}

The inverse function of the CDF of the standard normal distribution is called the {\bf probit} function. By definition, the following must hold:
\begin{eqnarray}
  \Phi( \text{probit}(p) ) &=& p \\
  \text{probit}( \Phi(x) ) &=& x
\end{eqnarray}

Using equation~\ref{eq:erf-phi}, the probit function can be written in terms of the inverse of the error function $\text{erf}^{-1}(x)$:
\begin{equation}\label{f:probit_erfinv}
\text{probit}(p) = \sqrt{2} \cdot \text{erf}^{-1}(2p-1)
\end{equation}

There are no closed expressions for the error function and the probit function. Because these functions are often used in (statistical) calculations, there are several numerical approaches available.

\section{Conversion of reliability index to probability (and vice versa)}
Computation techniques applied in the system reliability result in reliability index $\beta$ (\Aref{Section_2.2.4}). For real life applications, it is often required to translate the reliability index to the (non)failure probability and vice versa.

To calculate failure probability $q$ from reliability index $\beta$ the following equation is applied:
\begin{equation}
q = \Phi(-\beta)
\end{equation}

Non-failure probability $p$ on the other hand is derived as follows:
\begin{equation}
p = \Phi(\beta)
\end{equation}

The following equation yields the reliability index $\beta$ given failure probability $q$:
\begin{equation}
\beta = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{q} e^{-\frac{1}{2}t^2} dt
\end{equation}

\Note{the above conversions require a numerical approximation of the probit function. In the \probLib this is done using the methods of \cite{Wichura1988} and \cite{Hall_etall_1968}. Both methods are double precision.}