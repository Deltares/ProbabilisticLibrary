\chapter{Basic techniques and terminology}\label{chp:terminology}

\section{Components in a reliability calculation}
The model for which a reliability calculation is performed, often falls into a system
of multiple components which describes it certain physical phenomenon.
Such a system is called a fault tree.
We first describe how to solve a single component in this fault tree.
The components are combined with methods given in \autoref{chp:CombiningDesignPoints}.

\section{Formulation of the probability of failure of a single component}\label{Section_2.2.2}
The probability of failure for a single component can be written formally as follows:
\begin{equation}
P_{f} = P(Z(X_1, X_2, \dots, X_n)<0) \label{eq:2-1}
\end{equation}
where:

\begin{tabular}{p{0.2in}p{0.2in}p{2.3in}}  
$P_{f} $ & = & failure probability \\  
$Z$ & = & limit state function \\  
$X$ & = & vector of random variables \\  
\end{tabular}

The limit state function, $Z$, defines failure in terms of load and strength variables such that $Z<0$ represents failure. Function $Z$ is often denoted:
\begin{equation}
Z=R-S,\label{eq:2-2}
\end{equation}
where:

\begin{tabular}{p{0.1in}p{0.2in}p{2.1in}}  
$R$ & = & strength, or resistance \\  
$S$ & = & load \\  
\end{tabular}

\Note{in case of a flood defense, the load typically consists of the combination of water levels and waves and in some cases currents. The strength is a combination of dike characteristics that reflect the ability of the dike to resist high loads. A simple example is overflow of a river levee in a river (see \Fref{fig:2.2}). The load in this simple example is the water level in the river, $h_{river}$, and the strength of the levee is captured by its height, $h_{levee}$. The limit state function for this example is simply:}
\begin{equation} 
Z=R-S=h_{levee} -h_{river}\label{eq:2-3} 
\end{equation}

\begin{figure}[H]\centering
\includegraphics*[width=5.05in, height=2.74in, keepaspectratio=false]{\pathProbLibPictures riverlevee_overflow}
\caption{Illustration of a river levee exposed to the threat of overflow.}\label{fig:2.2}
\end{figure}

The limit state function, $Z$, is a function of a number of random variables representing both load and strength variables. The probability of failure can be written as follows:
\begin{equation}
P_{f} =\int \int _{Z<0}...  \int f_{X_{1} ,...,X_{n} } \left(x_{1} ,...,x_{n} \right)dx_{1}  dx_{2} ...dx_{n} =\int _{Z<0}f_{X} \left(x\right)  dx \label{eq:2-4}
\end{equation}
where $f$ is the joint probability distribution of the random variables and $X$ is the vector of variables: $X$ = ($X_1$,\dots $X_n$).
Note that random variables are typically denoted with a capital letter ($X$), while potential realizations of the random variables are denoted with lower case letters ($x$). 

While an analytical solution to equation \eqref{eq:2-4} would be ideal,
it is typically not available because the $Z$-function is too complex.
Therefore, the probability of failure needs to be estimated with probabilistic computation techniques.
Different techniques are available for this purpose.
These techniques will be described in detail in the \Aref{sec:Reliability}.

\section{Conversion between standard normal variables ($U$-domain) and real variables ($X$-domain)}\label{Section_2.2.3}
In practice, it is often advantageous to carry out probabilistic analyses in a standardized space, in which each of the variables are independent.
This independence can help simplify the probabilistic techniques.
The dependence between variables is reintroduced when the standardized variables are transformed back to the real variables.
In \probLib the standardized space that is used is the standard normal space.
This means each of the transformed random variables are normally distributed with mean $0$, and standard deviation $1$.
Each of the random variables in $X$ can be transformed to independent standard normal variables $U = (U_1,\dots U_n)$.
If all the $X$-variables are mutually independent, this transformation can be described as follows:
\begin{equation}
\Phi \left(u_{i} \right)=F_{i} \left(x_{i} \right) \quad \Rightarrow  \quad u_{i} =\Phi ^{-1} \left[F_{i} \left(x_{i} \right)\right]\quad \Rightarrow \quad x_{i} =F_{i}^{-1} \left(\Phi \left(u_{i} \right)\right) \label{eq:2-5} 
\end{equation}
where:

\begin{tabular}{p{0.2in}p{0.2in}p{5.1in}}  
$\Phi $ & = & Standard normal distribution function \\ 
$F_{i} $ & = & Distribution function (cdf) of $X_i$, the $i^{th}$ variable in $X$ \\  
$\Phi ^{-1} $ & = & Inverse standard normal distribution function  \\  
$F_{i}^{-1} $ & = & Inverse distribution function of $X_i$, the $i^{th}$ variable in $X$ \\  
$x_{i} $ & = & Realisation of the $i^{th}$ variable in $X$ \\  
$u_{i} $ & = & Realisation of the $i^{th}$ variable in $U$ corresponding to the $i^{th}$ variable in $X$  \\  
\end{tabular}

In this simple case (mutually independent $X$-variables) the transformation is done such that the non-exceedance probabilities of $x_i$ and $u_i$ are equal:
\begin{equation}
P\left(U_{i} \le u_{i} \right)=P\left(X\le x_{i} \right)  \label{eq:2-6} 
\end{equation}

The concept of this transformation is schematically depicted in \Fref{fig:2.3}. 

\begin{figure}[H]\centering
\includegraphics[width=0.6\columnwidth]{\pathProbLibPictures x_vs_u}
\caption{Schematic view of the transformation between standard normal variable $u_i$ and real world variable $X$ by means of equal probability of (non-)exceedance.}\label{fig:2.3}
\end{figure}

If the $X$-variables are not mutually independent, the transformation becomes more complex. In that case the transformation is done with the following conditional probability functions:
\begin{equation}
\begin{array}{l} {\Phi \left(u_{1} \right)=F_{1} \left(x_{1} \right)} \\ {\Phi \left(u_{2} \right)=F_{2} \left(x_{2} |x_{1} \right)} \\ {\quad \begin{array}{c} {\cdot } \\ {\cdot } \\ {\cdot } \end{array}} \\ {\Phi \left(u_{n} \right)=F_{n} \left(x_{n} |x_{1} ,...,x_{n-1} \right)} \end{array}  \label{eq:2-7} 
\end{equation}

This transformation is referred to as the Rosenblatt transformation (cf. \cite{Rosenblatt1952}). The conditional probabilities need to be derived from: 
\begin{equation}
f_{i} \left(x_{i} |x_{1} ,...,x_{i-1} \right)=\frac{f_{X_{1} ,...,X_{i} } \left(x_{1} ,...,x_{i} \right)}{f_{X_{1} ,...,X_{i-1} } \left(x_{1} ,...,x_{i-1} \right)}   \label{eq:2-8}  
\end{equation}

In which $f_{X_i}$ ($x_1$,\dots $x_i$) is a probability density function that is obtained from:
\begin{equation}
f_{X_{i} } \left(x_{1} ,...,x_{i} \right)=\int _{}... \int _{}f_{X} \left(x_{1} ,...,x_{n} \right)dx_{i+1} ...dx_{n}   \label{eq:2-9}  
\end{equation}

The combination of equations \eqref{eq:2-8} and \eqref{eq:2-9} potentially requires a cumbersome (numerical) integration procedure. 

\Note{in practice, this process is far less complex as correlation is usually limited to pairs of variables.
Generally, the pairs are split into a dependent variable and an independent variable, and the dependent variable is written as a function of the independent variable.
The function consists of a fully deterministic dependent part and a probabilistic independent part:}
\begin{equation}
F_{X_{2} } \left(x_{2} |x_{1} \right)=G\left(x_{1} \right)+F_{2} \left(x_{2}^{*} \right) \label{eq:2-10}  
\end{equation}
where $G$ is a deterministic function, $F_2$ a probability distribution function and $x_2^{*}$ a newly introduced random variable that represents the part of variable $x_2$ that is independent of $x_1$.
This type of correlation models is schematically depicted in \Fref{fig:2.4}.
More details on this type of correlation models are described in \Aref{sec:Correlations}.
For now, it is important to note that with a correlation model as described by the general form of equation \eqref{eq:2-10}, the transformation from and to the $U$-space for variable $X_2$ is done for the independent part: 
\begin{equation}
\Phi \left(u_{2} \right)=F_{2} \left(x_{2}^{*} \right)  \label{eq:2-11)} 
\end{equation}

\begin{figure}[H]\centering
\includegraphics*[width=3.56in, height=2.83in, keepaspectratio=false]{\pathProbLibPictures schematic_correlations}
\caption{Schematic view of correlated variables $x_1$ and $x_2$}\label{fig:2.4}
\end{figure}

\Note{not all random variables can be represented in a meaningful way by $U$-variables.
This is for instance the case for cyclic variables like wind direction. The reason is that transformation \eqref{eq:2-5} only makes sense if large (extreme) $u$-values are associated with large (extreme) $x$-values. Potential outcomes of cyclic variables are not ordered from small to large.
The domain for these variables is $1-360$ degrees, but $360$ degrees is not larger than $1$ degree.
Actually, $1$ degree is almost the same as $360$ degrees. \cite{TechRef} describes how such cyclic variables are treated.}

\section{Reliability index $\beta$}\label{Section_2.2.4}
The reliability index, $\beta$, is a measure for the reliability of a system, i.e. a measure for the probability of failure of the system.
Similar to the probability of failure, $\beta$ is often defined for a given period of time, e.g. a year.
The reliability index is best explained by an example in which the resistance, $R$, and load, $S$, of the system are both described as the sum of independent normally distributed random variables.
The sum of a set of independent normally distributed random variables is also a random variable (see, e.g. \cite{GrimmettStirzaker1983}).
This means in this case, $R$ and $S$ are also normally distributed variables and the same can be stated about the $Z$-function $(Z=R-S)$.
Define $\mu_R$, $\mu_S$ and $\mu_Z$ as the respective mean values of $R$, $S$ and $Z$ and $\sigma_R$, $\sigma_S$ and $\sigma_Z$ as the respective standard deviations of $R$, $S$ and $Z$.
The following relations hold:
\begin{equation} 
\begin{array}{l} {\mu _{Z} =\mu _{R} -\mu _{S} } \\ {\sigma _{Z} =\sqrt{\sigma _{R} ^{2} +\sigma _{S} ^{2} } } \end{array}  \label{eq:2-12}  
\end{equation}

\Fref{fig:2.5} shows an example with $\mu_R=6$, $\mu_S=2$, $\sigma_R = \sigma_S=1$ and consequently $\mu_Z = 4$ and $\sigma_Z=\surd2$.
In reliability analysis, $\mu_Z$ is generally a positive value because otherwise,
failure $(Z<0)$ would occur even during ''average'' conditions.
Since $Z$ is normally distributed, $P[Z<0]$ is equal to:
\begin{equation} 
P\left(Z<0\right)=\Phi \left(-\frac{\mu _{Z} }{\sigma _{Z} } \right)=1-\Phi \left(\frac{\mu _{Z} }{\sigma _{Z} } \right) \label{eq:2-13}  
\end{equation}
where $\Phi$ is the standard normal distribution function.
Based on this equation, the reliability index, $\beta$, is defined as:
\begin{equation}
\beta =\frac{\mu _{Z} }{\sigma _{Z} }  \label{eq:2-14} 
\end{equation}

\begin{figure}[H]\centering
\includegraphics*[width=5.29in, height=3.94in, keepaspectratio=false]{\pathProbLibPictures functions_SRZ}
\caption{Example of normally distributed $S$, $R$ and $Z$ functions with $\mu_R=2$, $\mu_S=6$, $\sigma_R=1$, $\sigma_S=1$, $\mu_Z= 4$ and $\sigma_Z=\surd2$.}\label{fig:2.5}
\end{figure}

For the specific case where the $Z$-function is normally distributed,
the relation between $\beta$ and the probability of failure is:
\begin{equation} 
P\left(Z<0\right)=\Phi \left(-\beta \right)=1-\Phi \left(\beta \right) \label{eq:2-15}  
\end{equation}

Or inversely:
\begin{equation}
\beta =\Phi ^{-1} \left(1-P\left(Z<0\right)\right)  \label{eq:2-16}  
\end{equation}

\begin{figure}[H]\centering
\includegraphics*[width=4.35in, height=3.27in, keepaspectratio=false]{\pathProbLibPictures beta_vs_probability}
\caption{Schematic view of the relation between the reliability index $\beta$ and the probability of failure, $P[Z<0]$. Z is normally distributed with mean $\beta$ and standard deviation 1}\label{fig:2.6}
\end{figure}

This shows why the reliability index $\beta$ is a measure for reliability. Note that if the $Z$-function is not normally distributed, equation \eqref{eq:2-15} does not necessarily hold if $\beta$ is defined as in equation \eqref{eq:2-14}. This is why $\beta$ is a measure for reliability, not an exact representative of the probability of failure. However, often $\beta$ is computed directly from equation \eqref{eq:2-16} in which case it is an exact representative of the probability of failure by definition. \Tref{tab:2.1} shows a range of $\beta$-values and associated probabilities of exceedance.

\begin{longtable}{|p{1.0in}|p{1.0in}|}
\caption{Values of reliability index $\beta$ and associated probability of failure.} \label{tab:2.1} \\ \hline
$\beta $ & $P$ \\ \hline 
$1.0$ & $1.59\cdot 10^{-1}$ \\ \hline 
$1.5$ & $6.68\cdot 10^{-2}$ \\ \hline 
$2.0$ & $2.28\cdot 10^{-2}$ \\ \hline 
$2.5$ & $6.21\cdot 10^{-3}$ \\ \hline 
$3.0$ & $1.35\cdot 10^{-3}$ \\ \hline 
$3.5$ & $2.33\cdot 10^{-4}$ \\ \hline 
$4.0$ & $3.17\cdot 10^{-5}$ \\ \hline 
$4.5$ & $3.40\cdot 10^{-6}$ \\ \hline 
$5.0$ & $2.87\cdot 10^{-7}$ \\ \hline 
\end{longtable}

Small values of $\beta$ indicate large probabilities of failure, large values of $\beta$ indicate small probabilities of failure. This can be easily explained with some examples in which, for the sake of simplicity, $Z$ is assumed to be normally distributed. If $\beta=1$, failure occurs if a random sample of $Z$ is more than $1\cdot\sigma_Z$ lower than the mean. The probability for this to happen is equal to $\Phi(-1)\approx0.16$. On the other hand, if $\beta=4$, failure occurs if a random sample of $Z$ is more than $4 \cdot \sigma_Z$ lower than the mean. The probability for this to happen is equal to $\Phi(-4)\approx3.2 \cdot 10^{-5}$. In other words: larger values of $\beta$ indicate that more extreme events are required for failure to occur, hence a lower probability of failure, hence a larger reliability of the system.

