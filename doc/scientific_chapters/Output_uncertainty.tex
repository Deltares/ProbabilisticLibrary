\chapter{Output uncertainty}
\label{chp:OutputUncertainty}

Output uncertainty is an extension of sensitivity analysis.
Now all input parameters are varied according to their uncertainty definition.
This leads to uncertainty of the output parameters.

This is useful when the user is interested in possible values in the future of a physical property.
For example, due to a load subsidence of the soil surface will occur.
It is interesting how much subsidence will occur in the next ten years.
Another example, due to a side stream a gully will move sidewards.
It is interesting to know the location of the gully in the next year.

\section{Numerical integration}

Numerical integration varies all input parameters step by step and for each step combination,
a realization is generated and calculated.
Realization results are collected in bins, which form a histogram distribution.

\section{Crude Monte Carlo}

Crude Monte Carlo is the 'standard' Monte Carlo simulation.
Random realizations will be generated and realization results are categorized into bins.
When a bin gets more realizations, it has a higher probability.

The Crude Monte Carlo simulation results in a histogram distribution
(see  \autoref{sec:HistogramDistribution}) or,
if all model results are equal, a deterministic distribution
(see \autoref{sec:DeterministicDistribution}).
The distribution is fitted with all model results. 

\section{Importance sampling}

Importance sampling modifies randomly selected realizations
in such a way that more realizations are selected in an area selected by the user.
Therefore each realization is translated to  another realization.
This is done in the same way as Importance Sampling for reliability,
see \autoref{sec:ImportanceSampling}.

Importance sampling is useful when the user is interested in the tail of the distribution.

\section{Directional sampling}

Directional sampling is an accelerated sampling method which identifies the failure domain via sampling through randomly chosen directions. The method performs in a transformed standard space with standard Gaussian variables (mean equal to zero and standard deviation equal to 1).

After transferring the model into the standard space, the value of model $Z_0$ for a sample at the center of space $u_0$ is calculated. 

$n$ points are drawn randomly in the variable space according to an uniform distribution. The line between these points and the origin will serve as the random directions. In the next step, the required reliability index $\beta_{q}$ corresponding with the given quantile value / probability of exceedance is evaluated, and used to find the distance $d_{i,0} = |\beta_{q}|$ from the origin, using \autoref{eq:DirectionalSamplingWeight}, with the assumption of $\beta_{dir} = d_{i,0}$ for all directions. 

Now the value of model $Z_{i,1}$ at each sample point ($d_{i,0}$ distance from the origin on each direction) is evaluated. 

Here the probability of having $Z_{i,1}$ values bigger than the $Z_0$, and the related reliability index $\beta_0$ are calculated to examine the validity of random directions. If the $\beta_{q}$ is bigger than $\beta_0$, every direction with $Z_{i,1} < Z0$ is disregarded in the probability calculations.

The main goal is to find the same value of $Z$ in all directions, in such a way that the given exceedance probability does not change. If the $Z$ values are not same for all the directions, we have to go through the following steps to find it.

Per direction $i$, find a new distance $d_{i,2}$ from the origin for each direction, so that is searched to satisfy the following:

\begin{itemize}
	\item the predicted $Z_{pred}$ is the same for all directions (prediction based on interpolation between $Z_0$ and $max(Z_{i,1})$) 
	\item the combined reliability remains $\beta_q$, based on \autoref{eq:DirectionalSampling}. Note that even when a direction is disregarded in previous steps, still the number of directions $n$ is not changed. 
\end{itemize}

$d_{i,2}$ is calculated based on the interpolation between $Z_{pred}$ and $Z_{i,1}$ values. Now the model response $Z_{i,2}$ at distance $d_{i,2}$ is evaluated. 

The last steps will perform maximum up to $N$ times to reach the same value of $Z_{i,N}$ (with an acceptance error criteria) for all the directions.

It worthy to mention that the directions are disregarded in the analysis, if:

\begin{itemize}
	\item calculated distance for them is much higher than $\beta_q$ or
	\item calculated distance for them is negative
\end{itemize}

\section{FORM}

FORM (First Order Reliability Method) is like FORM in reliability analysis (see \autoref{sec:FORM}).
Starting from the origin in u-space, steps of fixed length are taken along the steepest gradient.
For each step a realization is performed and its result is added to a CDF curve
(see \autoref{sec:CDFCurveDistribution}).

Each subsequent step in the resulting CDF curve is calculated as follows:

\begin{align}
\label{eq:FORMCDF}
\vec{u}_{i+1} = \vec{u}_{i} + L \cdot \nabla z \left(\vec{u_{i}}\right)
\end{align}

The reliability of the added point is

\begin{align}
\label{eq:FORMReliability}
\beta_{i+1} = \mid\mid\vec{u}_{i+1}\mid\mid
\end{align}

where
\begin{longtable*}{p{20mm}p{\textwidth-24pt-20mm}}  
	$\vec{u}$ & is a point in the parameter space defined in u-values; \\  
	$z\left(\vec{u}\right)$ & is the z value calculated at $\vec{u}$; \\  
	$L$ & is the step size; \\  
	$\nabla z\left(\vec{u}\right)$ & is the steepest gradient of $z$ at $\vec u$. \\
\end{longtable*}

\section{FOSM}

FOSM (First Order Second Moment) is a much faster technique than Crude Monte Carlo.It takes the gradient in the origin and then it predicts the further shape of the result distributions. It assumes the results have a normal distribution.

Although very fast, its assumptions that results have a normal distribution is often not true.
But for a quick feeling of the output uncertainty it can be very useful.
