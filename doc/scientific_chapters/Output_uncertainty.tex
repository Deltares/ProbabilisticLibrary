\chapter{Output uncertainty}
\label{chp:OutputUncertainty}

Output uncertainty is an extension of sensitivity analysis.
Now all input parameters are varied according to their uncertainty definition.
This leads to uncertainty of the output parameters.

This is useful when the user is interested in possible values in the future of a physical property.
For example, due to a load subsidence of the soil surface will occur.
It is interesting how much subsidence will occur in the next ten years.
Another example, due to a side stream a gully will move sidewards.
It is interesting to know the location of the gully in the next year.

\section{Numerical integration}

Numerical integration varies all input parameters step by step and for each step combination,
a realization is generated and calculated.
Realization results are collected in bins, which form a histogram distribution.

\section{Crude Monte Carlo}

Crude Monte Carlo is the 'standard' Monte Carlo simulation.
Random realizations will be generated and realization results are categorized into bins.
When a bin gets more realizations, it has a higher probability.

The Crude Monte Carlo simulation results in a histogram distribution
(see  \autoref{sec:HistogramDistribution}) or,
if all model results are equal, a deterministic distribution
(see \autoref{sec:DeterministicDistribution}).
The distribution is fitted with all model results. 

\section{Importance sampling}

Importance sampling modifies randomly selected realizations
in such a way that more realizations are selected in an area selected by the user.
Therefore each realization is translated to  another realization.
This is done in the same way as Importance Sampling for reliability,
see \autoref{sec:ImportanceSampling}.

Importance sampling is useful when the user is interested in the tail of the distribution.

\section{Directional sampling}

Directional sampling is an accelerated sampling method which identifies the failure domain via sampling through randomly chosen directions. The method performs in a transformed standard space with standard Gaussian variables (mean equal to zero and standard deviation equal to 1).

After transferring the model into the standard space, the value of model $Z_0$ for a sample at the center of space $u_0$ is calculated. 

$n$ points are drawn randomly in the variable space according to an uniform distribution. The line between these points and the origin will serve as the random directions. In the next step, the required of reliability index $\beta_{q}$ corresponding with the given quantile value / probability of exceedance is evaluated, and used to find the distance $d_{i,0} = |\beta_{q}|$ from the origin to obtain $n$ sample points in the sample space.

Now the value of model $Z_{i,1}$ at each sample point ($d_{i,0}$ distance from the origin on each direction) is evaluated. Subsequently, the related $\beta$ value to each samples (i.e. directions) is calculated. If the $\beta_{q}$ is bigger than $\beta_0$, every direction with $Z_{i,1} < Z0$ is disregarded in the future probability calculations (i.e., they are proceeding in undesired direction), otherwise it is valid to be considered. Note that the number of samples $n$ is independent of their validity status. To calculate the probability of failure, equation \autoref{eq:DirectionalSamplingWeight} shall be used, in this stage, the assumption of $\beta_{dir} = d_{i,0}$ for all directions is applied. See \ref{fig:DS-first} as the first step for generating the directions and samples are shown for the model $Z=X1_X2$, with $X1$ and $X2=N(0,1)$.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{pictures/DS-first.png}
	\caption{Generation of first samples in random directions}
	\label{fig:DS-first}
\end{figure}

The main goal is to find the same value of $Z$ in all directions, with a given tolerance, as the model response on given exceedance probability. If the $Z$ values are not same for all the directions, we have to go through the following steps to find it.

1- Per direction $i$, find a new distance $d_{i,2}$ from the origin for each direction, so that is searched to satisfy the following:

\begin{itemize}
	\item the predicted model value $Z_{pred}$ is the same for all directions (prediction based on interpolation between $Z_0$ and $max(Z_{i,1})$) 
	\item the combined reliability, based on equation \autoref{eq:DirectionalSampling}, remains equal to $\beta_q$. Note that even when a direction is disregarded in previous steps, still the number of directions $n$, is not changed. 
\end{itemize}

2- $d_{i,2}$ is calculated based on the interpolation between $Z_{pred}$ and $Z_{i,1}$ values. Now the model response $Z_{i,2}$ at distance $d_{i,2}$ is evaluated. 

The final steps will be repeated to reach $Z_{i}$, either up to a maximum of $ð‘$ iterations or until the convergence criteria are met. The final $Z_{i}$ then used as the output of the model at the given quantile value or probability of exceedance. The graph \ref{fig:DS-last} shows the output for 90\% quantile, where $Z=1,3826$ is evaluated. The reader may notice how even the limited number of valid samples shape the borders of the failure area for the given exceedence quantile as a linear line for the simple model of $Z=X1 + X2$. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{pictures/DS-last.png}
	\caption{The second/final step of iterations for the valid directions}
	\label{fig:DS-last}
\end{figure}

For the error criteria, the distribution of differences between the distances to the center of the U-space in all valid directions in the last and current iterations is taken into consideration. These differences form a distribution, and its 95\% quantile is used as an indication of convergence. This quantile is then compared with a user-defined value."

%It worthy to mention that the directions are disregarded in the analysis, if:
%
%\begin{itemize}
%	\item calculated distance for them is much higher than $/beta_q$ or
%	\item calculated distance for them is negative
%\end{itemize}

\section{FORM}

FORM (First Order Reliability Method) is like FORM in reliability analysis (see \autoref{sec:FORM}).
Starting from the origin in u-space, steps of fixed length are taken along the steepest gradient.
For each step a realization is performed and its result is added to a CDF curve
(see \autoref{sec:CDFCurveDistribution}).

Each subsequent step in the resulting CDF curve is calculated as follows:

\begin{align}
\label{eq:FORMCDF}
\vec{u}_{i+1} = \vec{u}_{i} + L \cdot \nabla z \left(\vec{u_{i}}\right)
\end{align}

The reliability of the added point is

\begin{align}
\label{eq:FORMReliability}
\beta_{i+1} = \mid\mid\vec{u}_{i+1}\mid\mid
\end{align}

where
\begin{longtable*}{p{20mm}p{\textwidth-24pt-20mm}}  
	$\vec{u}$ & is a point in the parameter space defined in u-values; \\  
	$z\left(\vec{u}\right)$ & is the z value calculated at $\vec{u}$; \\  
	$L$ & is the step size; \\  
	$\nabla z\left(\vec{u}\right)$ & is the steepest gradient of $z$ at $\vec u$. \\
\end{longtable*}

\section{FOSM}

FOSM (First Order Second Moment) is a much faster technique than Crude Monte Carlo.It takes the gradient in the origin and then it predicts the further shape of the result distributions. It assumes the results have a normal distribution.

Although very fast, its assumptions that results have a normal distribution is often not true.
But for a quick feeling of the output uncertainty it can be very useful.
