\subsection{Linearisation of $Z$-functions}\label{Section_2.2.5}
$Z$-functions in e.g. flood risk analysis generally describe a combination of hydrodynamics and geotechnical processes. Due to the complexity of the $Z$-function it is sometimes practical to use linear approximations. The linearisation can result in significant reductions of the computation time. This is for instance the case with the probabilistic computation method FORM, (see \Aref{sec:methodform}) which is generally much faster than other probabilistic computation techniques such as Monte Carlo (see \Aref{sec:crudemontecarlo}). 

Another advantage of the linearisation is that it enables (semi-)analytical approaches to complex system analysis, that otherwise would not have been possible. Such an approach is for instance used in the `Hohenbichler method' (see \Aref{Section_2.4.2}) which is applied to compute the total probability of failure of a system of components.

The disadvantage of the linearisation is of course the fact that it is an approximation of the $Z$-function, which means an error is likely to be introduced in the estimate of the failure probability. As long as this error is small compared to other modeling errors and uncertainties this poses no real problem, but this needs to be verified as much as possible.

The linearisation of the $Z$-function is generally applied in the $U$-space, in which the $U$-variables are independent standard normally distributed random variables. In other words: the function $Z(u)$ is linearized, where $U$ is the vector of standard normally distributed variables $U_1$, \dots  $U_n$. The linear approximation of the $Z$-function has the following form:
\begin{equation}
Z_L =B+A_{{\rm 1}} U_{{\rm 1}} +{\rm  }\ldots + A_{{\rm n}} U_{{\rm n}} \label{eq:2-17}  
\end{equation}

The linearisation is done by taking the tangent of the $Z$-function in a selected location $U=u_d$. This means the $A$-values are chosen as follows: 
\begin{equation}
A_{i} =\frac{\partial {Z}}{\partial u_{{\rm i}} } \left(u_{d} \right)\quad ;i=1...n  \label{eq:2-18}  
\end{equation}

This linearisation process is depicted in \Fref{fig:2.7} and \Fref{fig:2.8}.

\begin{figure}[H]\centering
\includegraphics*[width=5.42in, height=4.06in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image15}
\caption{Example of function $Z(U_1, U_2)$}\label{fig:2.7}
\end{figure}

\begin{figure}[H]\centering
\includegraphics*[width=5.25in, height=3.94in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image16}
\caption{Linearisation of the $Z$-function (dark red plane) of the $Z$-function of \Fref{fig:2.7} in a selected location (white dot). }\label{fig:2.8}
\end{figure}

Clearly, the linearised $Z$-function is different from the actual $Z$-function. This means an error will be introduced in the estimation of the probability of failure, $P(Z<0)$. To reduce this error as much as possible, the linearisation is generally done in the design point, $u_d$. This is the location on the hyperplane $Z=0$ with the highest probability density. The method FORM, as described in \Aref{sec:methodform} is based on this principle.

Generally, the main objective is to compute the probability of failure, i.e.\ $P(Z<0)$ $\approx$ $P(Z_L<0)$. In that case, the right hand side of equation \eqref{eq:2-17} can be multiplied or divided by a constant. If this constant is taken to be the norm of vector $A$ = ($A_1$, \dots  ,$A_n$) the linear $Z$-function has the following form: 
\begin{equation}
Z_L =\beta +\alpha _{1} U_{1} +...+\alpha _{n} U_{n} =\beta +\sum _{i=1}^{n}\alpha _{i} U_{i} \label{eq:2-19}
\end{equation}

In which:
\begin{equation} 
\beta =\frac{B}{\left\| A\right\| } ;\quad \alpha _{i} =\frac{A_{i} }{\left\| A\right\| } ,i=1...n;\quad \left\| A\right\| =\sqrt{\sum _{i=1}^{n}A_{i} ^{2}  } \label{eq:2-20}  
\end{equation}

The norm of vector $\alpha$=($\alpha_1$, \dots  ,$\alpha_n$) is then equal to 1:
\begin{equation} 
\sqrt{\sum _{i=1}^{n}\alpha _{i}^{2}  } =1 \label{eq:2-21}  
\end{equation}

This means the linearised $Z$-function has been normalised. Since the $U$-variables are independent standard normally distributed values, this means: 
\begin{equation}
\sum _{i=1}^{n}\alpha _{i} U_{i}  \sim N(0,1) \label{eq:2-22}  
\end{equation}

In other words: the sum of the product of $\alpha$-values and $U$-variables, $\Sigma$$\alpha_i U_i$, is standard normally distributed. This means that in order to compute $P(Z_L<0)$, $\Sigma$$\alpha_i U_i$ can be replaced by a single standard normally distributed variable $U^{*}$:
\begin{equation}
Z_{L} =\beta +U^{*} \label{eq:2-23}  
\end{equation}

Note that, since the density function of $U^{*}$ is symmetric around $U^{*}=0$, $Z_L$ can also be described as follows: 
\begin{equation}
Z_{L} =\beta -U^{*} \label{eq:2-24}  
\end{equation}

In equation \eqref{eq:2-23}, failure occurs if $Z_L$$<$0, i.e.\ if $U^{*}$$<$-$\beta$. The probability that this occurs is equal to $\Phi$(-$\beta$), where $\Phi$ is the standard normal distribution function and $\beta$ is the reliability index which was introduced in \Aref{Section_2.2.4}. While $\beta$ is an indicator for the probability of failure, the $\alpha$-values are indicators for the relative importance of the associated random variables, as will be shown below. From equation \eqref{eq:2-19} it can be seen that:
\begin{equation}
P\left(Z_{L} <0\right)=P\left(\beta +\sum _{i=1}^{n}\alpha _{i} U_{i}  <0\right) \label{eq:2-25}  
\end{equation}

If we increase the mean of variable $U_i$ ($\overline{u}_i$) with a small value $\varepsilon_i$ this will have an effect on the probability of failure. The magnitude of this effect is an indicator of the relative importance of variable $U_i$. For this purpose, define the random variable $U_i$' as follows: 
\begin{equation}
U_{i}^{'} =U_{i} +\varepsilon _{i}  \label{eq:2-26}  
\end{equation}

Since $U_i$ is standard normally distributed, $U_i$' is normally distributed with mean $\varepsilon_i$ and standard deviation $1$. Subsequently, $U_i$ in equation \eqref{eq:2-19} is replaced by $U_i$', resulting in a new $Z$-function $Z_L$':
\begin{equation}
\begin{array}{l} {Z_{L}^{'} =\beta +\alpha _{1} U_{1} +...+\alpha _{i} U_{i}^{'} +...+\alpha _{n} U_{n} } \\ {\quad  =\beta +\alpha _{1} U_{1} +...+\alpha _{i} \left(U_{i} +\varepsilon _{i} \right)+...+\alpha _{n} U_{n} } \\ {\quad  =\left(\beta +\alpha _{i} \varepsilon _{i} \right)+\alpha _{1} U_{1} +...+\alpha _{i} U_{i} +...+\alpha _{n} U_{n} } \end{array} \label{eq:2-27}  
\end{equation}

So the perturbation of the mean of variable $U_i$ results in a new $Z$-function with reliability index $\beta$' instead of $\beta$, with:
\begin{equation}
\beta ^{'} =\beta +\alpha _{i} \varepsilon _{i}  \label{eq:2-28}  
\end{equation}

This means:
\begin{equation} 
\frac{\partial \beta }{\partial \bar{u}_{i} } =\frac{\partial \beta }{\partial \varepsilon _{i} } =\frac{\beta '-\beta }{\varepsilon _{i} } =\frac{\left(\beta +\alpha _{i} \varepsilon _{i} \right)-\beta }{\varepsilon _{i} } =\frac{\alpha _{i} \varepsilon _{i} }{\varepsilon _{i} } =\alpha _{i}  \label{eq:2-29}  
\end{equation}

In other words: $\alpha_i$ is a measure of the sensitivity of reliability index $\beta$ to changes in the mean value of variable $U_i$. This also means $\alpha_i$ is a measure of the sensitivity of the probability of failure to changes in the mean value of variable $U_i$. This information is used in the Hohenbichler method for combining probabilities of components in a system (see \Aref{Section_2.4.2}).

\Note{as stated before, linearized $Z$-functions are the basis for various computation techniques that are explained in the following sections. A full understanding of this linearisation process and the meaning of $\alpha$-values and $\beta$ is therefore essential for further reading of this document.}

\section{Failure probability for a single component}\label{Section_2.3}
In this section, aspects of computing the failure probability for a single component are addressed.

\subsection{Introduction}\label{Section_2.3.1}
Equation \eqref{eq:2-4} describes the general formulation of the probability of failure of a single component. While an analytical solution to equation \eqref{eq:2-4} would be ideal, it is typically not possible because the $Z$-function is too complex. Therefore, the probability of failure needs to be estimated with probabilistic computation techniques. The computation techniques available within the \probLib are summarized in \Tref{tab:2.2}. Each of these techniques will be described in detail in the current section. The reason to implement a set of probabilistic computation techniques in the \probLib is that each technique has its (dis)advantages with respect to criteria such as robustness, accuracy and required computation time. The ''best'' technique to be applied therefore depends on the problem under consideration. \Aref{Section_2.3.8} discusses the (dis)advantages of each of these techniques. First, the techniques are explained individually.


\pagebreak
\begin{longtable}{|p{\textwidth-24pt-70mm}|p{70mm}|}
\caption{Computation techniques available in the \probLib for the computation of failure probability of a cross section of a longitudinal segment.}\label{tab:2.2}\\ \hline 
\textbf{Method} & \textbf{Variant} \\ \hline 
Numerical integration & -- \\ \hline 
Monte Carlo & Crude\newline Importance sampling\newline Directional sampling \\ \hline 
FORM (First order reliability method) & -- \\ \hline 
\end{longtable}


\subsection{Numerical Integration\label{sec:numericalintegration}}%\label{Section_2.3.2}
Numerical integration solves equation \eqref{eq:2-4} by discretizing the random variables $X_1$\dots $X_n$. Each variable is discretized over a range that is relevant for failure, and subsequently each combination of discretized values of the $X$-variables is used to compute the limit state function. The probabilities of all the combinations that lead to $Z<0$ are summed, which provides the estimate of the overall probability of failure. This summation can be written as follows:

\begin{align}
\hat{P}_{f} =&\sum _{i_{1} =1}^{m_{1} }\sum _{i_{2} =1}^{m_{2} }\dots\sum _{i_{n} =1}^{m_{n} }1_{\left[Z<0\right]}    f_{X} (x_{0,1} +(i_{1} -0.5)\Delta x_{1} ,x_{0,2}\label{eq:2-30} \\ \nonumber
 &+(i_{2} -0.5)\Delta x_{2} ,\dots,x_{0,n} +(i_{n} -0.5)\Delta x_{n} )\Delta x_{1} \Delta x_{2} \dots\Delta x_{n} 
\end{align}
where:

\begin{tabular}{p{0.4in}p{0.2in}p{4in}} 
$\hat{P}_{f} $ & = & Estimated probability of failure \\
$1_{\left[Z<0\right]} $ & = & Indicator function, equal to $1$ for $Z$ $<$ $0$, equal to $0$ for $Z$ $\geq$ $0$ \\
$x_{0,k}$ & = & Lower range limit for the $k^{th}$ variable \\
$\Delta$$x_k$ & = & Interval width of the $k^{th}$ variable \\
$m_k$ & = & Upper bound of $k$ such that $x_{0,k}$+ $m_k$$\Delta$$x_k$ is the upper bound of the $k^{th}$ variable  \\
\end{tabular}

In equation \eqref{eq:2-30}, for each variable $X_k$ an equidistant grid with step size $\Delta$$x_k$ is used, but non-equidistant grids can also be used in numerical integration. \Fref{fig:2.9} presents a schematic view of the method, for an example of two random variables $X_1$ and $X_2$. A 2-dimensional grid is defined and the $Z$-function is evaluated at the centre of the grid cells. Red grid points indicate failure $(Z<0)$, green grid points indicate no failure ($Z\geq0$). The total probability of failure (see equation \eqref{eq:2-30}) is estimated as follows: multiply the probability density of the grid cells in the failure domain (red dots) with the size of the grid cells ($\Delta$$x_1$ $\times$ $\Delta$$x_2$) and take the sum of these probabilities.

\begin{figure}[H]\centering
\includegraphics*[width=4.75in, height=3.56in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image17}
\caption{Schematic view of the method of numerical integration for an example of two random variables. A 2-dimensional grid is defined and the $Z$-function is evaluated at the centre of the grid cells. Red grid points indicate failure $(Z<0)$, green grid point indicate no failure ($Z\geq0$).}\label{fig:2.9}
\end{figure}

Like every probabilistic estimation technique, the result of the numerical integration procedure will be an approximation of the actual probability of failure. The errors that are introduced in this method are caused by the following assumptions and approximations:

Each grid cell is assumed to be entirely situated in the failure domain or entirely situated outside the domain of failure domain. In reality, grid cells can be partly in the failure domain as can be seen in \Fref{fig:2.9}.

The probability density is assumed to be constant over the entire grid cell. The domain of potential outcomes of the random variables may not be entirely covered. In the implementation of the procedure, it may be beneficial to transform the $X$-variables to standard normally distributed $U$-variables (see \Aref{Section_2.2.3}). One of the benefits of working in the $U$-space is that the $U$-variables are independent, which simplifies equation \eqref{eq:2-30} as follows:

{\footnotesize{\begin{equation} 
\hat{P}_{f} =\sum _{i_{1} =1}^{m_{1} }\dots \sum _{i_{n} =1}^{m_{n} }1_{\left[Z<0\right]}  \phi \left(u_{0,1} +\left(i_{1} -0.5\right)\Delta u_{1} \right)\cdot \dots\cdot \phi \left(u_{0,n} +\left(i_{n} -0.5\right)\Delta u_{n} \right)\Delta u_{1} ...\Delta u_{n} \label{eq:2-31} 
\end{equation}}}
where:

\begin{tabular}{p{\textwidth-36pt-129mm}p{4mm}p{125mm}}  
$\hat{P}_{f} $ & = & Estimated probability of failure \\ 
$\phi $ & = & standard normal density function \\ 
$u_{0,k} $ & = & Lower range limit for the $k^{th}$ variable, in the $U$-space \\  
$\Delta u_{k} $ & = & Interval width of the $k^{th}$ variable, in the $U$-space \\  
$m_{k} $ & = & Upper bound of $k$ such that $u_{0,k} +m_{k} \cdot \Delta u_{k}$ is the upper bound of the $k^{th}$ variable \\  
\end{tabular}


\subsection{Crude Monte Carlo\label{sec:crudemontecarlo}}%\label{Section_2.3.3}
Crude Monte Carlo sampling refers to the repeated sampling of the variables from the multivariate probability distribution function $f_X (x)$ (or, if the variables are mutually independent, sampling from the respective distribution functions $f_{X_i}$($x_1$),\dots , $f_{X_n}$($x_n$)). A single sample $x_i$ refers to a vector of length $n$, where $n$ is the number of random variables. For each sample $x_i$, the resulting value of limit state function $Z(x_i)$ is computed. The probability of failure is estimated as the ratio of samples for which $Z(x_i)$ $<$ $0$, $N_f$, to the total number of samples, $N$:
\begin{equation}
\hat{P}_{f} =\frac{N_{f} }{N} =\frac{\sum _{i=1}^{N}{\rm I}\left(Z\left(x_{i} \right)\right) }{N}  \label{eq:2-32}
\end{equation}
where $I$ is the indicator function, which is equal to unity when $Z<0$, equal to zero when $Z\geq0$. \Fref{fig:2.10} shows a schematic view of the procedure for an example with two random variables $X_1$ and $X_2$. Each dot represents a sampled pair ($x_1$, $x_2$). Red grid points indicate failure $(Z<0)$, green grid point indicate no failure ($Z\geq0$). The estimated probability of failure is equal to the number of the red dots divided by the total number of dots.

\begin{figure}[H]\centering
\includegraphics*[width=4.07in, height=3.05in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image18}
\caption{Schematic view of Monte Carlo sampling for an example with two random variables. Each dot represents a sampled pair ($x_1$, $x_2$). Red dots indicate failure $(Z<0)$, green dots indicate no failure ($Z\geq0$).}\label{fig:2.10}
\end{figure}

The required number of samples, $N$, to provide a reliable estimate of the probability of failure depends on the actual failure probability $P_f$ and on the acceptable error in the estimate of $P_f$. Additionally, it depends on the acceptable probability that the real error is within the accepted range. This is because even though taking a large number of samples will most likely result in small errors (law of large numbers), it can never be fully guaranteed due to the random character of the Monte Carlo sampling. However, it is possible to take $N$ large enough to guarantee with for example 95\% or 99\% certainty that the error in the estimate is within the acceptable range. This probability, $P_k$, can be expressed as:
\begin{equation}
P_{k} =\Phi \left(k\right)-\Phi \left(-k\right)\quad ;k>0. \label{eq:2-33} 
\end{equation}
where $\Phi$ represents the standard normal distribution function, and $k$ represents a sort of reliability index that the error is within the accepted range. The relation between $k$ and $P_k$ is schematically depicted in \Fref{fig:2.11}.

\begin{figure}[H]\centering
\includegraphics*[width=4.02in, height=3.02in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image19}
\caption{Relation between $k$ and $P_k$ according to equation \eqref{eq:2-34}. Function $\phi$ is the standard normal density function}\label{fig:2.11}
\end{figure}

For example, a probability of 95\% corresponds with a $k$-value of $1.96$, because 95\% of samples from a standard normal distribution function have a value between $-1.96$ and $1.96$. In formula, $k$ is defined as:
\begin{equation} 
k=\Phi ^{-1} \left(\frac{1+P_{k} }{2} \right) \label{eq:2-34}  
\end{equation}
where $P_k$ is the desired probability that the actual error is within the defined acceptable range. The required number of samples $N$ can be estimated with the following formula (Melchers, 2002): 
\begin{equation}
N=\frac{k^{2} }{\varepsilon ^{2} } \left(\frac{1-P_{f} }{P_{f} } \right)\label{eq:2-35} 
\end{equation}
where $\varepsilon$ is the acceptable relative error in the estimate of $P_f$:
\begin{equation}
\varepsilon =\left(\frac{\left|\hat{P}_{f} -P_{f} \right|}{P_{f} } \right) \label{eq:2-36}  
\end{equation}
where $\hat{P}_{f} $ is the Monte Carlo estimator of failure probability $P_f$.

Note that the required number of samples depends on the failure probability, which is not known in advance. Therefore, an estimate of the order of magnitude of the failure probability must be assumed, which can subsequently be revised during the Monte Carlo sampling procedure.

\Tref{tab:2.3} shows the required number of samples for combinations of $\varepsilon$ and $P_f$. The value of $k$ in this example is taken equal to $1.96$. The numbers from this table show that, taking into account that each sample involves an evaluation of the $Z$-function, crude Monte Carlo is a rather inefficient method (i.e.\ a large number of $Z$-function evaluations is required) especially for estimating small failure probabilities. 

\begin{longtable}{|p{0.6in}|p{0.4in}|p{0.4in}|p{0.4in}|}
\caption{Required number of samples with crude Monte Carlo for combinations of the acceptable relative error $\varepsilon$ and actual probability of failure $P_f$. The value of $k$ in equation \eqref{eq:2-34} is taken equal to $1.96$.} \label{tab:2.3} \\ \hline 
~ & \multicolumn{3}{|p{1.5in}|}{$\varepsilon$} \\ \hline 
~$P_f$ & $0.10$ & $0.05$ & $0.01$ \\ \hline 
$10^{-2}$ & $4\cdot10^{4}$ & $2\cdot10^{5}$ & $2\cdot10^{7}$ \\ \hline 
$10^{-3}$ & $4\cdot10^{5}$ & $2\cdot10^{6}$ & $2\cdot10^{8}$ \\ \hline 
$10^{-4}$ & $4\cdot10^{6}$ & $2\cdot10^{7}$ & $2\cdot10^{9}$ \\ \hline
$10^{-5}$ & $4\cdot10^{7}$ & $2\cdot10^{8}$ & $2\cdot10^{10}$ \\ \hline
\end{longtable}

\subsection{Monte Carlo Importance Sampling}\label{Section_2.3.4}
In this section, aspects of importance sampling as an addition computational component to Monte Carlo type methods are addressed.

\subsubsection{General description}\label{Section_2.3.4.1}
Importance sampling is a method to increase the efficiency of the crude Monte Carlo method; that is, to decrease the number of samples and $Z$-function evaluations required to produce a reliable estimate of the failure probability. This is done by replacing the initial probability density, $f_X$, of the input variables by a more efficient one, $h_X$, in which ''efficient'' refers to the proportion of the samples which will result in failure. An increasing percentage of samples in the failure domain results in a reduction in the variance of the estimator of the failure probability, hence a smaller number of samples is required for a reliable estimate. 

There are a number of ways in which importance sampling can be applied; two of these are described in this section. The first increases the variance of the density function, resulting in a higher likelihood that failure events are sampled. The second essentially shifts the density function towards the failure domain so that, again, the likelihood of a failure sample increases. These two methods are illustrated in \Fref{fig:2.12}; the left-hand side illustrates the concept of a shifting of the density function, and the right-hand side illustrates the concept of increased variance. 

\begin{figure}[H]\centering
\includegraphics*[width=2.30in, height=3.07in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image20} \includegraphics*[width=3.06in, height=3.01in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image21} 
\caption{Concept of importance sampling; left shows the concept of density shifting, right shows the concept of increased variance. The limit state function (LSF) is illustrated. Note that the LSF's are linear in this figure, but this is by no means a requirement for the applicability of importance sampling.}\label{fig:2.12}
\end{figure}

Because the sampling hasn't taken place from the initial distribution, the typical estimator of the failure probability (see equation \eqref{eq:2-32}) needs to be corrected for this fact. This is done via the following formula:
\begin{equation}
\hat{P}_{f} =\frac{\sum _{i=1}^{N}{\rm I}\left(Z\left(x_{i} \right)\right)\frac{f_{X} \left(x_{i} \right)}{h_{X} \left(x_{i} \right)}  }{N}  \label{eq:2-37} 
\end{equation}
where $\hat{P}_f$ is the estimated probability of failure, $I$ is the indicator function (equal to unity when $Z<0$, equal to zero when $Z\geq0$), $N$ is the total number of samples taken, $f_X$ is the density function of $x$ and $h_X$ is the importance sampling density function. 

Equation \eqref{eq:2-37} can be explained by comparing it with equation \eqref{eq:2-32}, which describes the crude Monte Carlo method. In both equations, the indicator $I$ is equal to one if the sampled vector $x_i$ is in the failure domain and equal to $0$ if $x_i$ is outside the failure domain. In the crude Monte Carlo method, each sampled failure event scores a point and the more points scored, the higher the estimated probability of failure. In the importance sampling method it needs to be taken into account that the sampling of vector $x_i$ was influenced by the fact that the density function was changed: $h_X($x$)$ was applied instead of the real density function $f_X($x$)$. This means the probability of sampling $x_i$ was increased by a factor $c = h_X(x_i)/f_X(x_i)$. This manipulation in the density function needs to be compensated for in the ''scoring''. Therefore, a sampled event $x_i$ in the failure domain does not score a full point, but ''only'' $1/c = f_X(x_i)/h_X(x_i)$.

So, the difference between equation \eqref{eq:2-37} and \eqref{eq:2-32} is the correction term $f_X/h_X$. This correction is necessary to make the estimate of $P_f$ unbiased (provided h is well chosen) and accordingly that the error in the estimate can be made as small as desired by taking a sufficiently large number of samples, N. For importance sampling there is no simple generic error estimate like equation \eqref{eq:2-35} for Crude Monte Carlo sampling, because the error estimate depends on the choice of $h_X($x$)$. The efficiency of importance sampling therefore also depends strongly on the choice of $h_X($x$)$. Prior knowledge of the problem under consideration is therefore necessary to be able to define an efficient importance sampling method. Without such knowledge, there is even the potential danger that the important area for the limit state function (LSF) will be missed. 

\subsubsection{Implementation of the method of increased variance}\label{Section_2.3.4.2}
The implementation of this general formula in the \probLib for the case of increased variance will now be described. Equations \eqref{eq:2-38} through \eqref{eq:2-44} show how the formula programmed in the \probLib can be derived from the general expression in equation \eqref{eq:2-37}. A random sampling of standard normal variables is first done; let us refer to these variables as $U_1$, where $U_1$ is the vector containing all the variables. Subsequently, each $u_1$-value is multiplied by a constant factor, $a$, and increased with a shift, $b$, to obtain a sample $u = b+a \cdot u_1$ (in the \probLib, $a$ and $b$ can differ per variable). Thus, the initial distribution of each $u_1$-value is standard normal. The new set of variables, $U$, then has a normal distribution with a mean value equal to $b$ and a standard deviation equal to $a$. The general form of the normal distribution is given below for reference. 
\begin{equation}
f\left(x\right)=\frac{1}{\sqrt{2\pi \sigma ^{2} } } \exp \left[-\frac{\left(x-\mu \right)^{2} }{2\sigma ^{2} } \right] \label{eq:2-38} 
\end{equation}

The ratio $f(u)/h(u)$ is then derived as follows, shown for the case of one variable:
\begin{equation}
f\left(u\right)=\frac{1}{\sqrt{2\pi \left(1\right)^{2} } } \exp \left[-\frac{\left(u-0\right)^{2} }{2\left(1\right)^{2} } \right]=\frac{1}{\sqrt{2\pi } } \exp \left[-\frac{u^{2} }{2} \right] \label{eq:2-39} 
\end{equation}
\begin{equation}
h\left(u\right)=\frac{1}{\sqrt{2\pi a ^{2} } } \exp \left[-\frac{\left(u-b \right)^{2} }{2a ^{2} } \right]  \label{eq:2-40} 
\end{equation}
\begin{equation}
\frac{f\left(u\right)}{h\left(u\right)} =\frac{\frac{1}{\sqrt{2\pi } } \exp \left[-\frac{u^{2} }{2} \right]}{\frac{1}{\sqrt{2\pi a^{2} } } \exp \left[-\frac{(u-b)^{2} }{2a^{2} } \right]} =a\frac{\exp \left[-\frac{u^{2} }{2} \right]}{\exp \left[-\frac{(u-b)^{2} }{2a^{2} } \right]}  \label{eq:2-41} 
\end{equation}

Writing $u$ in terms of $u_1$, the equation becomes:
\begin{equation}
\frac{f\left(u\right)}{h\left(u\right)} =a\frac{\exp \left[-\frac{u^{2} }{2} \right]}{\exp \left[-\frac{\left(b+au_{1}-b \right)^{2} }{2a^{2} } \right]} =a\frac{\exp \left[-\frac{u^{2} }{2} \right]}{\exp \left[-\frac{u_{1} ^{2} }{2} \right]} \label{eq:2-43} 
\end{equation}

Note that the one-variable example can easily be expanded to more variables, using the property that the $u$-values are independent, and hence their probability densities can be multiplied for the multi-variate case. The multi-variate form of equation \eqref{eq:2-43} is:
\begin{equation}
\frac{f\left(u\right)}{h\left(u\right)} =\prod_{k} a_{k} \frac{\exp \left[-\frac{1}{2} \sum _{k}u_{k} ^{2}  \right]}{\exp \left[-\frac{1}{2} \sum _{k}u_{1k} ^{2}  \right]}  \label{eq:2-44} 
\end{equation}
where $k$ runs over the total number of random variables. Equation \eqref{eq:2-44} is the form in which the ratio is programmed in the \probLib.

\subsection{Adaptive Importance Sampling}
Adaptive Importance Sampling is an improvement of the Importance Sampling method described in \Sref{Section_2.3.4}. In general, Importance Sampling is sensitive to the user given start point. If not chosen well, the method might not give satisfactory results. In Adaptive Importance Sampling loops are used to improve the start point.

The algorithm of the Adaptive Importance Sampling is presented in \Fref{AdaptiveImportanceSampling}.

\begin{figure}[H]\centering
\includegraphics[width=\textwidth]{funcdesign_chapters/figsystemreliability/AdaptiveImportanceSampling.png}
\caption{The algorithm of Adaptive Importance Sampling.}\label{AdaptiveImportanceSampling}
\end{figure}

The individual steps are:
\begin{itemize}
\item \textbf{Initialize start point:} here one of the FORM start methods is used, the method can be selected by the user.
\item \textbf{Importance Sampling:} in this step the Importance Sampling method from \Sref{Section_2.3.4} is applied.
\item \textbf{Converged?:} the convergence is reached when $\min\{N_{failed}/N,1-N_{failed}/N\}> \epsilon_{failed}$ where $N_{failed}$ is the number of failed realizations and $N$ is the total number of realizations. The value of $\epsilon_{failed}$ is specified by the user.
\item \textbf{Final round?:} this step checks whether the last allowed round is reached, this is done to prevent endless loops. The maximal number of loops is specified by the user. Usually \\ 5 rounds are enough.
\item \textbf{Recalculate:} in this step the Importance Sampling from \Sref{Section_2.3.4} is applied with a larger number of realizations. Here the last design point is used as the start point.
\item \textbf{Enough failures?:} this step checks whether enough failures are found, this is the case when $N_{failed}>N_{enough}$. The value of $N_{enough}$ is specified by the user.
\item \textbf{Increase variance:} when there are not enough failures (hence $N_{failed}\leq N_{enough}$), then the variance coefficient is increased with a certain value. The value is specified by the user. With this value, the Importance Sampling calculation is repeated.
\item \textbf{Use design point:} when there are enough failures (hence $N_{failed}> N_{enough}$), then the calculated design point is used as input for the next Importance Sampling calculation.
\end{itemize}

\subsection{Monte Carlo Directional Sampling\label{sec:directionalsampling}}%\label{Section_2.3.5}
Directional Sampling (see, e.g. \cite{Bjerager1988}), also referred to as Directional Simulation is a type of Monte Carlo method which aims to (strongly) reduce the number of samples in comparison with the Crude Monte Carlo method.

Directional sampling is depicted in Figure \ref{fig:2.13}. The first step in the method is to sample a direction in the standard-Normal space. This is done by sampling ${\mathbf{u}} = \left( {{u_1},{u_2},...,{u_n}} \right)$. For the $i^{th}$ sample, ${{\mathbf{u}}_i} = \left( {{u_{{1_i}}},{u_{{2_i}}},...,{u_{{n_i}}}} \right)$, the directional unit vector ${\boldsymbol{\theta}_i}$ is obtained by normalizing $\bf u_i$ as described in Eq. \ref{eq:theta}. The next step in the method is to determine, for the given direction $\theta_i$, the value (or length) $\lambda_i$ such that the limit state function evaluated at $\lambda_i \cdot \theta_i$ equals zero. The procedure to determine the length $\lambda$ is described in the following subsection. The conditional failure probability for the $i^{th}$ sample, $P_{i}$,  is then calculated using the Chi-square ($\chi^2$) distribution (see Eq. \ref{eq:DS_Pfcond}). The $\chi_n^2$ distribution with $n$ degrees of freedom is the distribution function of the sum of the squares of $n$ independent standard normal random variables. Because we work in the standard normal space, the probability that $\left\| {{{\mathbf{u}}_i}} \right\| \leq \lambda_{i}$ is described by this distribution function. The final step is to estimate the failure probability as the mean of the conditional probabilities over all $N$ sampled directions (Equation \ref{eq:Pfhat}). 

\begin{equation}
\label{eq:theta}
{\bf{\theta_i }} = \frac{{\bf{u_i}}}{{\left\| {\bf{u_i}} \right\|}} = \left( {{{\bar u}_1},{{\bar u}_2},...,{{\bar u}_n}} \right)
\end{equation}

\begin{equation}
\label{eq:DS_Pfcond}
P_{i} = 1-\chi _{n}^{2} \left(\lambda _{i}^{2} \right)
\end{equation}

\begin{equation}
\label{eq:Pfhat}
{\hat P_f} = \frac{1}{N}\sum\limits_{i = 1}^N {{P_{i}}} 
\end{equation}


\begin{figure}[H]\centering
\includegraphics[scale=1]{funcdesign_chapters/figsystemreliability/directionalSampling_basicConcept}
\caption{Schematic view of the directional sampling method in the standard normal space. The distances $\lambda_i$ to the limit state ($Z=0$)  are shown for unit-length directional vectors $\theta_i$ for three directional samples ($i = 1,2,3$). }\label{fig:2.13}
\end{figure}

Similar to other Monte Carlo methods, the outcome of the estimated probability of failure is a random variable and the error in the estimate can be made as small as possible by taking a sufficient number of samples. For directional sampling, the standard deviation, $\sigma$, of the estimated probability of failure can be quantified as follows (see, e.g. \cite{Grooteman2011},\cite{Melchers2002} page 84):
\begin{equation}
\sigma _{\hat{p}_{f} } =\sqrt{\frac{1}{N\left(N-1\right)} \sum _{i=1}^{N}\left(P_{i} -\hat{P}_{f} \right)^{2}  }  \label{eq:covdirs}  
\end{equation}

From this equation, relative errors and confidence intervals can be estimated. Note that the error in the estimated failure probability is a random variable that approaches a normal distribution function, as $N$ increases. This follows from the central limit theorem (see, e.g. \cite{GrimmettStirzaker1983}). The error in the estimated probability of failure will decrease with increasing number of sampled directions. Equation \eqref{eq:covdirs} can be used to determine the number of sampled directions required for an acceptably reliable estimate of the failure probability. 


\subsubsection{Determining the distance to the limit state}
There are two search procedures in the \probLib to find $\lambda_i$, the distance  to the limit state $Z=0$ in the $i^{th}$ sampled direction. The first method is illustrated in Figure \ref{fig:DS_Z}. Starting with $\lambda_i = 0$, the value of $\lambda_i$ is incrementally increased by a fixed amount (the default is $\Delta \lambda =1$). For the $k^{th}$ iteration, the limit state function is evaluated at ${\mathbf{U}} = k\Delta\lambda \cdot \boldsymbol{\theta}_i$. This continues until the limit state function becomes negative. For example, in Figure \ref{fig:DS_Z}, this occurs at $k=6$. The algorithm then interpolates between $(k-1)\Delta\lambda$ and $k\Delta\lambda$ until it converges to $Z=0$ with a convergence criteria on $\lambda_i$ such that consecutive estimates lie within some small distance of each other (e.g. 0.001). 


\begin{figure}[H]
\centering 
\includegraphics[scale=1]{funcdesign_chapters/figsystemreliability/directionalSampling_currentMethodZ}
%\includegraphics[scale = 1]{figs/u1prime_distribution_revised}
  	\caption{Method 1 to determine $\lambda_i$, the distance to the limit state $Z=0$ for the $i^{th}$ sampled direction.}
  	\label{fig:DS_Z}
\end{figure}

The second method is illustrated in Figure \ref{fig:DS_Z_new}. It is a gradient-based method that requires fewer limit state function evaluations (LSFEs) than Method 1 (described above). The algorithm first considers two values for $\lambda_i$: $\lambda^0_i=0$, and $\lambda^1_i = \lambda_{step}$, where the default value is $\lambda_{step}=3$. The superscript indicates the iteration in the algorithm. The limit state function is evaluated at both values, $Z^0=Z(\lambda^0_i \cdot \boldsymbol{\theta}_i)$ and $Z^1 =Z(\lambda^1_i \cdot \boldsymbol{\theta}_i)$. Based on the two points $[\lambda^0_i,Z^0]$ and $[\lambda^1_i,Z^1]$, linear extrapolation is used to estimate the value $\lambda^2_i$ for which $Z=0$. This extrapolation is indicated in Figure \ref{fig:DS_Z_new} with the line `Gradient, step 1'. The limit state function is then evaluated again $Z^2 = Z(\lambda^2_i \cdot \boldsymbol{\theta}_i)$, and based on the points $[\lambda^1_i,Z^1]$ and $[\lambda^2_i,Z^2]$, linear extrapolation is again used to determine $\lambda^3_i$, the value for which $Z=0$ (in Figure \ref{fig:DS_Z_new} this is the line `Gradient, step 2'). Note that for each iteration, the extrapolation is constrained so that if $\lambda^k_i - \lambda^{k-1}_i>\lambda_{step}$, then $\lambda^k_i$ is set equal to $\lambda^{k-1}_i + \lambda_{step}$. The iterative process continues until either:
\begin{itemize}
\item
$Z(\lambda^k_i \cdot \boldsymbol{\theta}_i)$ becomes negative, at which point interpolation between $[\lambda^{k-1}_i,Z^{k-1}]$ and $[\lambda^{k}_i,Z^{k}]$ takes place in the same manner as Method 1,
\item
$\lambda^{k-1}_i$ and $\lambda^k_i$ differ by less than a small value $\epsilon$ (default is $\epsilon=0.001$), which indicates one-sided convergence to $Z=0$, or
\item
$\lambda^k_i > \lambda_{max}$ (default is $\lambda_{max}=20$), at which point $\lambda_i$ is set to $\lambda_{max}$, which is equivalent to a failure probability of zero.
\end{itemize}


\begin{figure}[H]
\centering 
\includegraphics[scale=1]{funcdesign_chapters/figsystemreliability/directionalSampling_newMethodOnly.pdf}
%\includegraphics[scale = 1]{figs/u1prime_distribution_revised}
  	\caption{Method 2 (gradient-based) to determine $\lambda_i$, the distance to the limit state $Z=0$ for the $i^{th}$ sampled direction.}
  	\label{fig:DS_Z_new}
\end{figure}

The situation can arise where the gradient at the first iteration (the one derived using $[\lambda^0_i,Z^0]$ and $[\lambda^1_i,Z^1]$) is positive. This means that for increasing distance in the failure space, the limit state function is getting further away from $Z=0$. If this occurs, the algorithm evaluates the limit state function at ${\mathbf{U}} = \lambda_{max} \cdot \boldsymbol{\theta}_i$. If the value of the limit state function is positive, $\lambda_i$ is set equal to $\lambda_{max}$. If the value is negative, we set $\lambda^2_i=\lambda^1_i+\lambda_{step}$, and the iterative process continues to find $\lambda_i$.

\subsection{First-order reliability method (FORM)\label{sec:methodform}}%\label{Section_2.3.6}
The term first-order refers to the linearisation of the limit state function, as previously described in \Aref{Section_2.2.5}. This linearisation takes place at a location referred to as the design point. A ''location'' in this case refers to a specific realization $x_1$, \dots ,$x_n$ of the $X$-variables, or $u_1$, \dots , $u_n$ of the $U$-variables. The design point is the location along the limit state  $(Z~=~0)$, where the probability density is maximal. This location is not known in advance and is determined via an iterative procedure, which will be explained in this section.

The FORM procedure is generally executed in the standard normal space ($U$-variables). The standard normally distributed variables have by definition a mean value of zero and a standard deviation of $1$, and are mutually independent. The advantage of working in the standard normal space is that in this space the design point has a clear interpretation. Namely, in the standard normal space, the design point is the location along the limit state $(Z~=~0)$ which is closest to the origin (see \Fref{fig:2.15} for an illustration). This can be easily explained by the fact that for standard normally distributed variables the density is highest for $u=0$ and decreases with increasing value of \textbar $u$\textbar . So in the u space the density decreases with increasing distance from the origin. Therefore, the design point is the point along the limit state that is closest to the origin.

\begin{figure}[H]\centering
\includegraphics*[width=4.59in, height=3.26in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image24}
\caption{Illustration of the design point in the $U$-space}\label{fig:2.15}
\end{figure}

The distance from the origin to the design point is equal to the reliability index $\beta$ that was introduced in \Aref{Section_2.2.4}. This means if the location of the design point is known, the reliability index, $\beta$, is known and hence the estimated probability of failure can be derived from equation \eqref{eq:2-15}. This is how the probability of failure is estimated in the FORM method. Note that this is an estimate, and not the precise probability of failure. This is because the limit state function  $(Z~=~0)$, was linearized to provide the estimate. The error in the estimate therefore depends on the extent in which the real limit state function is non-linear. This is illustrated in \Fref{fig:2.16} below, where the solid line indicates the true limit state and the dashed line represents the FORM approximation. The shaded area represents the true failure domain, the area to the upper right of the dashed line is the assumed failure domain.

The reason why FORM in general provides good estimates of the failure probability is the fact that the linearisation is done in the design point, which means in the vicinity of the design point the linear $Z$-function is a good approximation of the real $Z$-function. The design point is the location on the limit state with the highest probability density. This means the failure events with the highest probability of occurrence will generally be in the vicinity of the design point. So, the linearised $Z$-function is generally a good approximation for the areas that matter the most in terms of probability of failure.

\begin{figure}[H]\centering
\includegraphics*[width=3.88in, height=2.78in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image25}
\caption{Illustration of the FORM approximation}\label{fig:2.16}
\end{figure}

As demonstrated in \Aref{Section_2.2.5}, the linearized limit state function $Z_L$ is essentially a hyper-plane and is described mathematically as follows:
\begin{equation}
Z_{L} =\beta +\alpha _{1} U_{1} +...+\alpha _{n} U_{n} =\beta +\sum _{i=1}^{n}\alpha _{i} U_{i} \label{eq:2-47}  
\end{equation}

Note that the $\alpha$-values have been normalized, as described in \Aref{Section_2.2.5}. This means the sum of the squares of the $\alpha$-values is equal to $1$. In the remainder of this document, $\alpha$-values are always assumed to be normalized in case they are used as coefficients in an equation with $U$-variables (unless mentioned otherwise). Given that the design point is the point along the $Z=0$ line that lies closest to the origin, the coordinates of the design point can be determined using geometry as follows:
\begin{equation}
u_{d,i} =-\alpha _{i} \beta \quad ;i=1...n \label{eq:2-48}  
\end{equation}
where $u_{d,i}$ is the value of the $i^{th}$ random variable in the design point. As demonstrated in \Aref{Section_2.2.4}, the probability of failure can be estimated directly from the value of $\beta$:
\begin{equation} 
P\left(Z<0\right)=\Phi \left(-\beta \right)=1-\Phi \left(\beta \right) \label{eq:2-49}  
\end{equation}
where $\Phi$ is the standard normal distribution function. In other words, once the design point is known, the $Z$-function can be linearised as described in \Aref{Section_2.2.5}, and subsequently the probability of failure can be estimated from equation \eqref{eq:2-49}. The challenge in the FORM procedure is therefore not to compute the failure probability but to locate the design point. 

The procedure to locate the design point is described below. The procedure and formulas will be presented in general form. Furthermore, the procedure will be clarified in a number of figures for the following example $Z$-function: 
\begin{equation*}{\rm Z }={\rm 5} - U_{1}^{0.8}  U_{2} ^{{\rm 1}.{\rm 2}} \end{equation*}
where $U_1$ and $U_2$ are standard normal random variables. \Fref{fig:2.17} shows the contourlines of this $Z$-function.
Generally, these contour lines are not known in advance, otherwise the search for the design point would be straightforward.
Therefore, an iterative search procedure is required.
The procedure starts at a ''user defined'' starting location in the $U$-space and jumps to a selected location in each following iteration step.
In other words: in each iteration step the location in the $U$-space is determined that will serve as the starting point for the next iteration step.
The procedure ends when the design point is found.
Each iteration step consists of the following five sub-steps:

The five steps in a FORM iteration.
\begin{enumerate}
\item Linearisation of the $Z$-function in $u^t$, where $u^t$ is the starting location of iteration $t$;

\item Normalisation of the linearised $Z$-function in $u^t$;

\item Estimation of the location of the design point, based on the $Z$-function of step 2;

\item Selection of location $u^{t+1}$, which will serve as the starting location of iteration $t+1$;

\item Verification if the iteration procedure has converged.
\end{enumerate}

These five steps are described in more detail below. Note that location $u^t$ refers to a vector of $u$-values: $u^t = (u_1^t,\dots ,u_n^t)$.

\textbf{[1]} The starting location of each iteration, $t$, is determined in the previous iteration, $t-1$.
The starting location in the first iteration step can either be selected ''arbitrarily'' or by more advanced methods in which the $U$-space is partially explored in advance of the FORM procedure.
In the current example the starting location in the first iteration step is chosen to be $u_i=1$; $i=1...n$ (red dot in \Fref{fig:2.17}).
%See section \ref{sec:formstartmethods} for the start methods available. 

In each iteration, first the $Z$-function is determined for the selected location at the beginning of the iteration, i.e.\ $Z(u_1,\dots ,u_n)$ is quantified. Subsequently, the $Z$-function is linearised in the current location. For this purpose, the partial derivatives of $Z$ to the individual $U$-variables are quantified. Generally, the $Z$-function is too complex to have an analytical expression of the partial derivatives, which means a numerical estimation technique is required. For this purpose, the $Z$-function is evaluated for small perturbations ($\Delta u$) of the $u$-values as shown in \Fref{fig:2.18}. The partial derivates can then be estimated as follows:
\begin{equation}
\frac{\partial Z}{\partial u_{i} } \left(u_{1} ,...,u_{n} \right)\approx \frac{Z\left(u_{1} ,...,u_{i} +\Delta u_{i} ,...,u_{n} \right)-Z\left(u_{1} ,...,u_{n} \right)}{\Delta u_{i} } \quad ;i=1...n \label{eq:2-50} 
\end{equation}

Note that equation \eqref{eq:2-50} describes a one-sided discretisation method. The \probLib actually uses a two-sided method, in which also a negative perturbation is applied on $u_i$:
\begin{equation}
\frac{\partial Z}{\partial u_{i} } \left(u_{1} ,...,u_{n} \right)\approx \frac{Z\left(u_{1} ,...,u_{i} +0.5\Delta u_{i} ,...,u_{n} \right)-Z\left(u_{1} ,...,u_{i} -0.5\Delta u_{i} ,...,u_{n} \right)}{\Delta u_{i} }  \\
 ;i=1...n \label{eq:2-51}
\end{equation}

A two-sided method is generally more robust (because of often non-smooth limit state functions), but requires approximately twice as much computation time. The linearised $Z$-function is described by: 
\begin{equation}
Z_{{\rm L}} =B+A_{{\rm 1}} U_{{\rm 1}} +{\rm  }\ldots +A_{{\rm n}} U_{{\rm n}}  \label{eq:2-52} 
\end{equation}

In which the $A$-values are the partial derivatives as derived in equation \eqref{eq:2-50} and $B$ is derived by substituting the known $Z$-value in the current location $(u_1,\dots ,u_n)$: 
\begin{equation}
B={\rm Z}\left(u_{1} ,...,u_{n} \right)-A_{{\rm 1}} u_{{\rm 1}} -...-A_{{\rm n}} u_{{\rm n}}  \label{eq:2-53} 
\end{equation}

The linearised function is (temporarily) assumed to be valid for the entire $U$-space. This results in linear contour lines as shown in \Fref{fig:2.19}. 

\Note{In the \probLib, the following methods are available to determine the starting location of the $u$-vector in the first iteration step:}
\begin{itemize}
\item All values of the $u$-vector are equal to $0.0$.
\item All values of the $u$-vector are equal to $1.0$.
\item The vector is defined by the user.
\item Method ray search: the start vector $u$ is found iteratively on a ray specified by the $u_{Ray}$-vector. In the $u_{Ray}$-
vector, the load variables take value $1.0$ and the strength variables take value $0.0$.
\item Method sphere search: the start vector $u$ is found iteratively and the search takes place within a sphere with the
radius of $10$. The start vector is then defined as the $u$-vector (within the sphere) for which the smallest value of the $Z$-function is found.
\end{itemize}

\textbf{[2]} Subsequently, the linearised $Z$-function is normalized by dividing equation \eqref{eq:2-52} by \textbar \textbar $A$\textbar \textbar , i.e.\ the norm of the $A$-vector (as earlier described in \Aref{Section_2.2.5}). The normalized linear $Z$-function is described as: 
\begin{equation}
Z_{L} =\beta +\alpha _{1} U_{1} +...+\alpha _{n} U_{n}  \label{eq:2-54} 
\end{equation}

In which:
\begin{equation}
\beta =\frac{B}{\left\| A\right\| } ;\quad \alpha _{i} =\frac{A_{i} }{\left\| A\right\| } ,i=1...n;\quad \left\| A\right\| =\sqrt{\sum _{i=1}^{n}A_{i} ^{2}  }  \label{eq:2-55}  
\end{equation}

The normalization changes the contour lines of the linearised $Z$-function (compare \Fref{fig:2.19} with \Fref{fig:2.20}). The orientation of the lines is still the same, but the distances between the contour lines have changed. The location of the contour line $Z=0$, however, remains the same. 

\textbf{[3]} From the linear contour lines it is easy to estimate the location of the design point. This is done by drawing the line through the origin that is perpendicular to the contour line $Z_L$=0 (see \Fref{fig:2.21}). In formula this means the estimated location of the design point is as described in equation \eqref{eq:2-48}. The values of $\alpha$ and $\beta$ in equation \eqref{eq:2-48} are set equal to the ones derived from equation \eqref{eq:2-55}.

\textbf{[4]} The estimated location of the design point can be chosen as the next location in the iteration procedure.
Note, however, that this is most likely not the actual location of the design point, since it was derived from the linearised $Z$-function and not from the real $Z$-function.
This is the reason why the design point will not be located straight away, i.e.\ a number of iteration steps are required.
This is also the reason why in practical applications generally a relaxation parameter, $R$, is used in each iteration step: 
\begin{align}
u^{t+1} =R u_{d}^{t} +\left(1-R \right)u^{t}  \label{eq:2-56}
\end{align}

In which:\begin{align*}
t &= \text{iteration step}\\
R &= \text{the relaxation parameter ($0\leq R \leq1$)}\\
u^t &= \text{the selected location at the beginning iteration step $t$}\\
u^{t+1} &= \text{the selected location at the beginning iteration step $t+1$}\\
u_d^t &= \text{the estimated location of the design point in iteration step $t$}
\end{align*}

The functionality of the relaxation parameter can be explained as follows: in each iteration step, the $Z$-function is linearised in location $u^t$. The linearised function, $Z_L$ is the tangent of the actual $Z$-function at location $u^t$. In the vicinity of $u^t$, $Z_L$ is generally a good approximation of $Z$. However, with increasing distance from $u^t$, differences between $Z$ and $Z_L$ may increase, as can be seen from e.g.\ \Fref{fig:2.8}. Since the estimated location of the design point, $u_d^t$, is based on $Z_L$, this estimate may be unreliable if the distance between $u^t$ and $u_d^t$ is large. This might even lead to non-convergence of the iteration procedure. It is therefore better to prevent that the distance between two subsequent iteration steps becomes too large, and for this reason the relaxation parameter is used. The relaxation parameter helps making the iterative procedure more robust.

\Fref{fig:2.22} demonstrates the application of the relaxation parameter. It shows the location at the beginning of the iteration, $u^t$, (red dot), the estimated location, $u_d^t$, of the design point (yellow dot) and the location at the beginning of the next iteration, $u^{t+1}$ (green dot). Location $u^{t+1}$ is chosen somewhere on the line between the current location and the estimated location of the design point. For values of $R<0.5$, $u^{t+1}$ will be closer to $u^t$ for values of $R>0.5$, $u^{t+1}$ will be closer to $u_d^t$. 

\textbf{[5]} \Fref{fig:2.23} shows the resulting iteration steps of the example problem. The iteration procedure continues until location $u_d^t$ satisfies the following 2 criteria: 
\begin{align}
\frac{\left|Z\left(u_{d}^{t} \right)\right|}{\left\| A\right\| } =\left|Z_{L} \left(u_{d}^{t} \right)\right|<\varepsilon _{1} \label{eq:2-57} \\
\beta ^{t} -\varepsilon _{2} <\left\| u_{d}^{t} \right\| <\beta ^{t} +\varepsilon _{2} \label{eq:2-58}  
\end{align}
where:

\begin{tabular}{p{0.2in}p{0.2in}p{4in}} 
$\varepsilon_{1,2}$ & = & Small numbers, quantifying convergence criteria \\  
$\beta^t$ & = & Estimate of reliability index $\beta$ in iteration step 2 \\  
\end{tabular}

Criterion \eqref{eq:2-57} guarantees that the $Z$-function is sufficiently close to $0$, i.e.\ that $u_d^t$ is on (or in the neighbourhood of) the limit state $Z=0$. Note that for this purpose the value of $Z$ is normalized by dividing it by the norm of the vector of $A$-values. The second criterion guarantees that the distance from $u_d^t$ to the origin is (approximately) equal to the estimated reliability index $\beta$, which makes $u_d^t$ the point on $Z=0$ with the highest probability density. 

\Note{the FORM procedure has the advantage that it requires relatively little computation time, i.e.\ a relatively small number of $Z$-function evaluations. The disadvantage of this method is that the iterative algorithm sometimes does not converge and results may become less accurate. This is especially the case if the $Z$-function is highly non-linear.}

\Note{when FORM does not converge, then the results are corrected by averaging the results from the last ten iterations. The procedure is described below assuming $n$ iterations and $m$ random variables:
\begin{itemize}
\item For each variable determine the mean value of the last ten $u$-values: $u_{k}=\frac{1}{10}\sum_{i=n-9}^{n}u_{i,k}$ and $k=1...m$.
\item Determine the mean value of the last ten reliability indices and determine the sign of the resulting value (the sign is $-1.0$ when the mean value is less than zero and the sign is $1.0$ otherwise).
\item Calculate the final reliability index as: $\beta=sign\cdot \sqrt{\sum_{k=1}^{m}u_{k}^2}$.
\item For each variable calculate the final $\alpha$-value as: $\alpha_k=-\frac{u_k}{\beta}$.
\end{itemize} It has been shown that the above correction leads to more stable results.}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image26}
\caption{Contour lines of the example $Z$-function and the starting location (red dot) of the FORM procedure}\label{fig:2.17}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image27}
\caption{Sampling the $Z$-function in all directions to estimate the derivative of $Z$ to all $u$-variables.}\label{fig:2.18}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image28}
\caption{Contour lines of the linearised $Z$-function}\label{fig:2.19}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image29}
\caption{Contour lines of the normalised linearised $Z$-function}\label{fig:2.20}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image30}
\caption{Estimated location of the design point based on the normalised linearised $Z$-function }\label{fig:2.21}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image31}
\caption{Proces of relaxation. The red dot is the location of the current iteration step, the yellow dot is the estimated location of the design point based on the normalised linearised $Z$-function. The green dot shows the selected location of the next iteration, which is somewhere on the line between the red dot and the yellow dot.}\label{fig:2.22}
\end{figure}

\begin{figure}[H]\centering
\includegraphics[width=0.8\columnwidth]{funcdesign_chapters/figsystemreliability/image32}
\caption{Resulting steps in the iteration procedure.}\label{fig:2.23}
\end{figure}

\subsection{Computing $\alpha $ values for other methods than FORM}\label{Section_2.3.7}

In the previous section it was demonstrated that the FORM procedure not only provides an estimate of the probability of failure, but also a design point with a set of associated $\alpha$-variables that provide information on the relative influence of the random variables on the reliability index $\beta$ (see also \Aref{Section_2.2.5} on the meaning of $\alpha$-variables). In \Aref{sec:combinationprocedure} it will be demonstrated that these $\alpha$-variables are very practical for estimating failure probabilities of systems that consist of a set of components. 

The other probabilistic techniques that were described in the previous sections, i.e.\ numerical integration and the various Monte Carlo techniques, do not provide $\alpha$-variables as output. Nevertheless, there are methods available to estimate $\alpha$-variables for Monte Carlo methods and numerical integration (see, e.g., Van Gelder [2002]). For example, for Monte Carlo, the following methods can be applied:
\begin{enumerate}
\item Centre of gravity,
\item Method of angles and
\item Nearest to the mean.
\end{enumerate}
The third method can also be used for numerical integration, as will be explained below. These methods all take into account the fact that the design point is the location in the failure domain that is closest to the origin (in the $U$-space). For all methods, the quality of the estimates increases with increasing number of samples. The methods are explained below:

\paragraph*{Centre of gravity}

Suppose a crude Monte Carlo run is done with $n$ samples of which $M$ lead to failure. This means there are $M$ sampled combinations of $u_1, \dots, u_n$, for which $Z(u_1, \dots, u_n)<0$. For each of the $n$ random variables the mean value over the $M$ failure-events is derived as follows:
\begin{align} 
\bar{u}_{j} =\frac{1}{M} \sum _{i=1}^{M}u_{ij}  \quad ;j=1...n \label{eq:2-59}
\end{align}
where $u_{ij}$ is the $i^{th}$ failure sample of the $j^{th}$ random variable. The resulting point $\bar{u} = (\bar{u}_1,\dots , \bar{u}_n)$ is the centre of gravity in the failure domain in the $U$-space. This is an estimate of the probability weighted mean of the locations in the failure domain. Note that the actual probability weighted mean is equal to:
\begin{align}
\bar{u}_{j} =\frac{1}{P\left[Z<0\right]} \int _{Z<0}^{}f_{U} (u) {\kern 1pt} u_{j} du \label{eq:2-60}
\end{align}

From the estimated centre of gravity this location a line is drawn towards the origin in the $U$-space (see \Fref{fig:2.24}). The location where this line crosses the limit state  $(Z~=~0)$, is the estimated location of the design point. This guarantees that the first characteristic of the design point, i.e. that it is located on the limit state $Z=0$, is taken care of. The second characteristic, that it is the location on the limit state with the highest density is not guaranteed. However, the use of the centre of gravity makes that the estimated location of the design point is likely to be close to the real design point. The likelihood increases with increasing number of samples.

\begin{figure}[H]\centering
\includegraphics*[width=3.27in, height=2.62in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image33}
\caption{Schematic view of the method centre of gravity.}\label{fig:2.24}
\end{figure}

If importance sampling has been applied in the sampling procedure, the method needs to be corrected for: 
\begin{align}
\bar{u}_{j} =\frac{1}{M} \sum _{i=1}^{M}u_{ij} \frac{f_{U} \left(u_{i1} ,...,u_{in} \right)}{h_{U} \left(u_{i1} ,...,u_{in} \right)}  \quad ;i=1...n \label{eq:2-61} 
\end{align}
where $f_U$ is the probability density function of vector $U$ and $h_U$ is the applied density function of the importance sampling method.

\paragraph*{Method of angles}

The method of angles is similar to the method of centre of gravity. For each of the M samples that lead to failure the angle with the origin in the $U$-space is derived. After completion of the MC-procedure the mean angle of all $M$ samples is derived. Note that the mean is derived with respect to the sine and cosine of the angles of all samples. 

\paragraph*{Method ''nearest to the mean''}

In the method of ''nearest to the mean'' the distance to the origin of all samples in the failure domain is derived: 
\begin{align}
\left|u_{i} \right|=\sqrt{\sum _{j=1}^{n}u_{ij} ^{2}  } \quad ;i=1...M \label{eq:2-62} 
\end{align}

The sample with the smallest distance to the origin is taken to be the design point. This method can also be applied for other Monte Carlo techniques (directional sampling, importance sampling) and even for numerical integration. In the latter case, the design point is taken equal to the grid point in the failure domain that is closest to the origin in $U$-space. 

\subsection{Rationale}\label{Section_2.3.8}

In the \probLib, a variety of probabilistic techniques has been implemented, including the first order reliability method (FORM), various Monte-Carlo techniques (crude, directional sampling, importance sampling) and numerical integration. Each of these techniques requires a considerable number of evaluations of the $Z$-function at (randomly) selected $x$-values. The choice of the most suitable probabilistic computation technique depends on the problem under consideration. 

If the computation time of one $Z$-function evaluation is significant, crude Monte Carlo and numerical integration are generally not the preferred candidates because both methods generally require a large number of $Z$-function evaluations. For crude Monte Carlo, the required number of $Z$-function evaluations is inversely proportional to the failure probability. This is because a small probability of failure means it takes a large number of samples to obtain even a single failure event and it takes more than one failure event to obtain a reliable estimate of the failure probability. For numerical integration, the number of $Z$-function evaluations is defined by the number of random variables and the number of grids for each random variable. Generally, numerical integration is too time-consuming if more than just a few random variables are involved. In theory, Monte Carlo and numerical integration are exact methods but in practice some error can be expected because the number of $Z$-function evaluations is limited. 

Directional Sampling is a more advanced Monte Carlo method in comparison with crude Monte Carlo. For most practical problems it reduces the amount of $Z$-function evaluations in comparison with crude Monte Carlo. For a large number of random variables, the efficiency of directional sampling decreases (see e.g. \cite{Waarts2000}, pp 73). Importance sampling is another efficient Monte Carlo variant. The efficiency of importance sampling is accommodated by prescience of the location of the limit state function. Without prescience, the performance of importance sampling techniques is volatile.

FORM has the advantage that it requires relatively little computation time. The disadvantage of this method is that the iterative algorithm to find the design point sometimes does not converge or converges to a local design point. Furthermore, the $Z$-function is linearised in the method, which means errors are introduced if the actual $Z$-function is highly non-linear.

Combining two different probabilistic methods may result in the combined advantage of the underlying methods. For instance, a relatively fast method like FORM can be applied first to locate the design point and subsequently a more precise method like importance sampling can be applied to derive the probability of failure by sampling in the vicinity of the design point. FORM is usually more accurate in estimating design points, whereas a more robust Monte Carlo method will sometimes provide a more accurate estimate of the failure probability. Vice versa, Monte Carlo sampling can be applied to provide a starting point for FORM in the neighbourhood of the design point, to increase the chance that FORM converges to the correct design point. This increases the robustness of the FORM procedure.



\section{Combining failure probabilities for components - generic methods\label{sec:combinationprocedure}}%\label{Section_2.4}
In this section, generic methods for the combination of the results for individual components are described.


%\subsection{Introduction}\label{Section_2.4.1}
In \Aref{Section_2.2.1}, the concept of system analysis was explained, with special emphasis on parallel systems (the system fails only if all components of the system fail) and series systems (the system fails if one or more components of the system fail). The general formulations of failure probabilities for parallel and series systems of $k$ components are as follows:
\begin{align}
&\text{Series:}  P_{f} =P\left[Z_{1} <0\cup ...\cup Z_{k} <0\right]=P\left[\bigcup _{i=1}^{k}Z_{i} <0 \right]=1-P\left[\bigcap _{i=1}^{k}Z_{i} \ge 0 \right] \label{eq:2-63} \\
&\text{Parallel:} P_{f} =P\left[Z_{1} <0\cap ...\cap Z_{k} <0\right]=P\left[\bigcap _{i=1}^{k}Z_{i} <0 \right]=1-P\left[\bigcup _{i=1}^{k}Z_{i} \ge 0 \right] \label{eq2-64} 
\end{align}

If the events $[Z_i<0]$, $i=1...k$ are mutually independent, this can be simplified to:
\begin{align}
&\text{Series:}  P_{f} =1-\prod _{i=1}^{k}\left\{1-P\left[Z_{i} <0\right]\right\}  \label{eq:2-65} \\
&\text{Parallel:} P_{f} =\prod _{i=1}^{k}P\left[Z_{i} <0\right]  \label{eq:2-66} 
\end{align}

The failure probabilities, $P[Z_i<0]$, for the individual components are determined by the probabilistic computation techniques as described in \Aref{Section_2.3}. System analysis for mutually independent components is therefore a relatively straightforward procedure. If the components are mutually correlated, the complexity of the system analysis increases. The correlations need to be taken into account as it increases the probability of failure of parallel systems and decreases the probability of failure of series systems. The following sections describe various general techniques that can be applied to carry out system analysis for systems with mutually correlated components.

\subsection{Combining $n$ system components: the Hohenbichler method}\label{Section_2.4.2}
In this section, the Hohenbichler method for combining multiple components is highlighted.

\subsubsection{Introduction}\label{Section_2.4.2.1}
The Hohenbichler method initially is a method for computing conditional probabilities of two $Z$-functions: $P(Z_2<0$\textbar $Z_1<0)$, taking into account the mutual correlation between these two $Z$-functions. The application of this method can be extended to compute failure probabilities of:
\begin{enumerate}
\item A parallel system of two components;
\item A series system of two components;
\item Parallel and series systems of multiple components.
\end{enumerate}
This is explained as follows:

\textbf{[1]}
$A$ parallel system with two components refers to a system in which both components must fail in order for failure to occur (keyword: AND). That is, the probability of failure is given as follows:
\begin{align}
P(F)=P\left(Z_{1} <0 \cap  Z_{2} <0\right)  \label{eq:2-67} 
\end{align}

A parallel system and the schematization of the associated failure probability $P(Z_1<0~\cap~Z_2<0)$ is schematically depicted in \Fref{fig:2.25} below. 

\begin{figure}[H]\centering
\includegraphics*[width=4.08in, height=3.44in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image34}
\caption{ Failure domain for a parallel system of two components -- the shaded area indicates the combinations of $Z$-values thatwill lead to failure of the system.}\label{fig:2.25}
\end{figure}

The failure probability of this system can be written equivalently as the product of a probability and a conditional probability:
\begin{align}
P(F)=P\left(Z_{1} <0\right)\cdot P\left(Z_{2} <0|Z_{1} <0\right)  \label{eq:2-68}  
\end{align}

The first term, $P(Z_1<1)$ can be computed with the methods as described in \Aref{Section_2.3}. The second term, $P(Z_2<0$\textbar $Z_1<0)$, can be determined with the Hohenbichler method, as will be demonstated in subsequent sections. This shows that the Hohenbichler method can also be applied to compute the failure probability of a parallel system of two components.

\textbf{[2]}
$A$ series system with two components refers to a system in which at least one component must fail in order for failure to occur (keyword: OR):
\begin{align}
P(F)=P\left(Z_{1} <0  \cup   Z_{2} <0\right) \label{eq:2-69}  
\end{align}

This probability can be rewritten as follows:
\begin{align}
P(F)=P\left(Z_{1} <0  \cup   Z_{2} <0\right)=P\left(Z_{1} <0\right)+P\left(Z_{2} <0\right)-P\left(Z_{1} <0  \cap  Z_{2} <0\right)  \label{eq:2-70} 
\end{align}

The first two terms on the right hand side of equation \eqref{eq:2-70} describe failure probabilities of single components, which can be derived with the techniques that were described in \Aref{Section_2.3}. The last term describes a parallel system of two components, for which the computational method was described in [1]. This shows that the Hohenbichler method can alos be applied to compute the failure probability of a series system of two components.

\textbf{[3]}
Consider a series system of $n$ components. The failure probability for the system is given by: 
\begin{align}
P(F)=P(Z_{1} <0  \cup   Z_{2} <0\cup   Z_{3} <0  \cup ...\cup Z_{n} <0) \label{eq:2-71}
\end{align}

If we define $Z_{12}=Z_1\cup Z_2$, this equation can be rewritten as an arbitrary system of $n-1$ components:
\begin{align}
P(F)=P(Z_{12} <0  \cup   Z_{3} <0  \cup ...\cup Z_{n} <0) \label{eq:2-72}
\end{align}

Repeating this procedure $n-1$ times will result in a system of one component. So, the probability of failure for a system of $n$ components can be derived by the successive combining of combinations of two components. The method of Hohenbichler can be used to combine two components, as demonstrated above, and therefore it can also be used to combine $n$ components of a series system. A similar approach can be applied for a parallel system of $n$ components. In other words: successive application of the method can than be used to compute the probability of failure of a system of $n$ components.

The basic principles of the Hohenbichler method for computing the conditional probability of failure of two components is described in \Aref{Section_2.4.2.2}. The follow-up sections elaborate on some of the finer details.

\Note{the Hohenbichler method makes use of linearisation of the $Z$-functions, as described in \Aref{Section_2.2.5}. The probability of failure for a system as derived by the method of Hohenbichler is therefore an approximation of the real probability of failure. Errors made in the approximation will depend on the system under consideration.}

\subsubsection{Hohenbichler method for a system with fully correlated variables}\label{Section_2.4.2.2}

As stated in the previous section, the Hohenbichler method initially is a method for computing conditional probabilities of two $Z$-functions: $P(Z_2<0$\textbar $Z_1<0)$, taking into account the mutual correlation, $\rho$, between these two $Z$-functions. In this section, first the computational procedure will be described, followed by a detailed explanation.

The two $Z$-functions are described by a set of $U$-variables. In this section, the set of $U$-variables are assumed to be the same for the two $Z$-functions. In \Aref{Hohenbichler_method_for_the_general_case_of_partial_correlation} the slightly more complex case will be dealt with in which the set of $U$-variables are different for the two $Z$-functions.

\paragraph*{Computational procedure}

The following information is available from the single component probabilistic analysis as described in \Aref{Section_2.3}:
\begin{itemize}
\item The reliability index $\beta_1$ of $Z_1$

\item The reliability index $\beta_2$ of $Z_2$

\item The influence variable, $\alpha $, for each random variable involved. 
\end{itemize}
First, the correlation between the $Z$-functions of the two components needs to be quantified:
\begin{align}
\rho \left(Z_{1} ,Z_{2} \right)=\sum _{j=1}^{n}\alpha _{1j} \cdot \alpha _{2j}  \label{eq:2-73} 
\end{align}

Subsequently, the conditional probability is computed by solving the following integral:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{\int _{\beta _{1} }^{\infty }\Phi \left(-\frac{\beta _{2} -\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right)\phi \left(u_{1} \right)du_{1}  }{\Phi \left(-\beta _{1} \right)} \label{eq:2-74} 
\end{align}
where $\phi$ is the standard normal density function. The outcome of this integral can be approximated very accurately through numerical integration at relatively low computational costs.

\paragraph*{Detailed explanation}

Since the $\alpha$- and $\beta$-values of the two $Z$-functions are known, the $Z$-functions can be described in the standard linearised form (see \Aref{Section_2.2.5}):
\begin{align}
\begin{array}{l} {Z_{1} =\beta _{1} +\alpha _{11} U_{11} +...+\alpha _{1n} U_{1n} } \\ {Z_{2} =\beta _{2} +\alpha _{21} U_{21} +...+\alpha _{2n} U_{2n} } \end{array} \label{0.72)} 
\end{align}
where $U_{ij}$ refers to the $j^{th}$ random variable of the $i^{th}$ $Z$-function. The $U$-variables can be different for the different $Z$-functions. However, for the sake of simplicity, we assume for the moment that they are the same: 
\begin{align}
U_{1k} =U_{2k} \quad ;k=1...n \label{0.73)} 
\end{align}

In \Aref{Hohenbichler_method_for_the_general_case_of_partial_correlation} the slightly more complex case will be dealt with in which $U_{1k}$$\neq$$U_{2k}$. Since $U_{1j}$ and $U_{1k}$, $j\neq k$, are mutually independent, it can easily be verified that the correlation between $Z_1$ and $Z_2$ is equal to:
\begin{align}
\rho \left(Z_{1} ,Z_{2} \right)=\sum _{j=1}^{n}\alpha _{1j} \cdot \alpha _{2j}  \label{eq:2-77} 
\end{align}

The linearised $Z$-functions can be re-written (see \Aref{Section_2.2.5}) as follows:
\begin{align}
\begin{array}{l} {Z_{1} =\beta _{1} -U_{1} \quad ; U_{1} =-\left(\alpha _{11} U_{11} +...+\alpha _{1n} U_{1n} \right)} \\ {Z_{2} =\beta _{2} -U_{2} \quad ; U_{2} =-\left(\alpha _{21} U_{21} +...+\alpha _{2n} U_{2n} \right)} \end{array}\label{eq:2-78} 
\end{align}
where $U_1$ and $U_2$ are two newly defined standard normally distributed variables. Because the $\beta $-values in equation \eqref{eq:2-78} are constant, the correlation between the components $Z_1$ and $Z_2$ is equivalent to the correlation between the variables $U_1$ and $U_2$:
\begin{align}
\rho \left(Z_{1} ,Z_{2} \right)=\rho \left(U_{1} ,U_{2} \right)=\rho \label{eq:2-79} 
\end{align}

In other words, equation \eqref{eq:2-78} is only valid if $U_1$ and $U_2$ are mutually correlated with correlation coefficient $\rho $. To assure that this is the case, $U_2$ is written as a function of $U_1$: 
\begin{align}
U_{2} =\rho  U_{1} +U_{2}^{*} \sqrt{1-\rho ^{2} } \label{eq:2-80} 
\end{align}

In this equation, $U_2^{*}$ is also standard normally distributed and independent of $U_1$. The first term in this equation represents the dependent part of $U_2$ and the second term represents the independent part. Note in equation \eqref{eq:2-80} that if $\rho  = 1$, $U_2$ = $U_1$ (100\% correlated), and if $\rho = 0$, then $U_2$ = $U_2^{*}$ (100\% uncorrelated). To verify the applicability of equation \eqref{eq:2-80} it needs to be shown that [1] $U_2$ is standard normally distributed and [2] that $U_1$ and $U_2$ have a mutual correlation coefficient that is equal to $\rho $. To prove [1], we apply the following general rule (see, e.g. \cite{GrimmettStirzaker1983}): If $X$ and $Y$ are independent normally distributed random variables, then $aX+bY$ is also normally distributed with a mean, $\mu$, and standard deviation, $\sigma$, equal to:
\begin{align}
\begin{array}{l} {\mu =a\mu _{X} +b\mu _{Y} } \\ {\sigma =\sqrt{a^{2} \sigma _{X}^{2} +b^{2} \sigma _{Y}^{2} } } \end{array}\label{0.78)} 
\end{align}

Application of this rule on equation \eqref{eq:2-80}, where $U_1$ and $U_2^{*}$ are both normally distributed with mean $0$ and standard deviation $1$, gives:
\begin{align}
\begin{array}{l} {\mu(U_2) =\rho \cdot 0+\sqrt{1-\rho ^{2} } \cdot 0=0} \\ {\sigma (U_2) =\sqrt{\rho ^{2} \cdot 1+\left(1-\rho ^{2} \right)\cdot 1} =1} \end{array}\label{0.79)} 
\end{align}

Which proves that $U_2$ is standard normally distributed. To prove [2], the correlation coefficient between $U_1$ and $U_2$ is derived. The correlation coefficient of $U_1$ and $U_2$ is defined as:
\begin{align}
\rho \left(U_{1} ,U_{2} \right)=\frac{cov\left(U_{1} ,U_{2} \right)}{\left[\sigma \left(U_{1} \right)\sigma \left(U_{2} \right)\right]} =\frac{cov\left(U_{1} ,U_{2} \right)}{\left[1\cdot 1\right]} =cov\left(U_{1} ,U_{2} \right)\label{0.80)} 
\end{align}

The covariance of $U_1$ and $U_2$ is equal to:
\begin{align}
cov\left(U_{1} ,U_{2} \right) \begin{array}{l} {=E\left[U_{1} U_{2} -\mu \left(U_{1} \right)\mu \left(U_{2} \right)\right]=E\left[U_{1} U_{2} \right]} \\ {=E\left[U_{1} \left(\rho  U_{1} +U_{2}^{*} \sqrt{1-\rho ^{2} } \right)\right]} \\ {=E\left[\rho U_{1}^{2} +U_{1} U_{2}^{*} \sqrt{1-\rho ^{2} } \right]=E\left[\rho U_{1}^{2} \right]} \\ {=\rho E\left[U_{1}^{2} \right]=\rho Var\left[U_{1} \right]=\rho } \end{array}\label{0.81)} 
\end{align}

Which proves that the application of equation \eqref{eq:2-80} preserves the correlation between $U_1$ and $U_2$ and hence the correlation between $Z_1$ and $Z_2$. The combination of equations \eqref{eq:2-78} and \eqref{eq:2-80} provides the following alternative description for function $Z_2$:
\begin{align}
Z_{2} =\beta _{2} -\rho U_{1} -U_{2}^{*} \sqrt{1-\rho ^{2} } \label{ZEqnNum229315} 
\end{align}

This expression represents a line in the $Z_2=0$ plane. The hatched area in \Fref{fig:2.26} indicates the area in the $U$-space that contributes to the failure probability 

\begin{figure}[H]\centering
\includegraphics*[width=4.86in, height=3.66in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image35}
\caption{ The $Z_1 = 0$ and $Z_2=0$ contours in the $U$-space; hatched area indicates the area that contributes to the failure probability.}\label{fig:2.26}
\end{figure}

Consider the following general formulation for conditional failure probability:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{P\left[Z_{2} <0\cap Z_{1} <0\right]}{P\left[Z_{1} <0\right]} \label{0.83)} 
\end{align}

From equation \eqref{eq:2-78} it can be seen that $Z_1<0$ if and only if $U_1$$>$$\beta_1$. This means:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{P\left[Z_{2} <0\cap U_{1} >\beta _{1} \right]}{P\left[U_{1} >\beta _{1} \right]} =\frac{P\left[Z_{2} <0\cap U_{1} >\beta _{1} \right]}{\Phi \left(-\beta _{1} \right)} \label{ZEqnNum645275} 
\end{align}

Substitution of equation \eqref{ZEqnNum229315} in equation \eqref{ZEqnNum645275} gives:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{P\left[\beta _{2} -\rho U_{1} -\sqrt{1-\rho ^{2} } U_{2}^{*} <0\cap U_{1} >\beta _{1} \right]}{\Phi \left(-\beta _{1} \right)} \label{0.85)} 
\end{align}

The numerator can be computed by integration over all potential realisations of $U_1$$>$$\beta$, using the theorem of total probability:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{\int _{\beta _{1} }^{\infty }P\left[\beta _{2} -\rho u_{1} -\sqrt{1-\rho ^{2} } U_{2}^{*} <0\right]\phi \left(u_{1} \right)du_{1}  }{\Phi \left(-\beta _{1} \right)} \label{ZEqnNum437286} 
\end{align}

The probability in the numerator can be rewritten as follows:
\begin{align}
\begin{array}{l} {P\left[\beta _{2} -\rho u_{1} -\sqrt{1-\rho ^{2} } U_{2}^{*} <0\right]=P\left[U_{2}^{*} >\frac{\beta _{2} -\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right]} \\ {=P\left[U_{2}^{*} <-\frac{\beta _{2} -\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right]=\Phi \left(-\frac{\beta _{2} -\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right)} \end{array}\label{ZEqnNum775381} 
\end{align}

Substitution of equation \eqref{ZEqnNum775381} in equation \eqref{ZEqnNum437286} gives:
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{\int _{\beta _{1} }^{\infty }\Phi \left(-\frac{\beta _{2} -\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right)\phi \left(u_{1} \right)du_{1}  }{\Phi \left(-\beta _{1} \right)} \label{ZEqnNum284113} 
\end{align}

Note that due to the symmetry of the normal density function, $\phi$, this equation can be re-written as follows: 
\begin{align}
P\left[Z_{2} <0|Z_{1} <0\right]=\frac{\int _{-\infty }^{-\beta _{1} }\Phi \left(-\frac{\beta _{2} +\rho u_{1} }{\sqrt{1-\rho ^{2} } } \right)\phi \left(u_{1} \right)du_{1}  }{\Phi \left(-\beta _{1} \right)} \label{ZEqnNum584503} 
\end{align}

Both equations \eqref{ZEqnNum284113} and \eqref{ZEqnNum584503} can be solved by numerical integration at a relatievely low computational cost. Alternative methods using FORM or Monte Carlo can be applied as well, but for this specific application numerical integration is recommended as it takes little computation time and it is very robust and reliable.

\subsubsection{Derivation of equivalent $\alpha$-values}\label{Section_2.4.2.3}

The previous section describes the Hohenbichler method for combining the probability of failure of two single components. The result is the combined probability of failure. The goal of a systems approach to failure probability is to combine the failure probabilities of all the contributing components to determine the failure probability of the whole system. This combining of probabilities takes place in a sequential fashion. That is, first two components are combined into one component, and this new component is then combined with an additional component, and so on, until only one component (the entire system) remains. 

The combining of failure probabilities over components relies on $\alpha $-values, see equation \eqref{eq:2-77}. This means we require $\alpha$-values for $Z_1$$~\cup~$$Z_2$, in order to be able to quantify the correlation with a third component, $Z_3$. The required $\alpha$-values are referred to as `equivalent' $\alpha$-values. Basically, a $Z$-function description, $Z^e$, is required that represents the combined components $Z_1$ and $Z_2$. This $Z$-function needs to have the same probability of failure as $Z_1$ and $Z_2$:
\begin{align}
P(Z^{e} <0)=P\left(Z_{1} <0  \cup Z_{2} <0\right)\label{0.90)} 
\end{align}

The equivalent $\alpha$-values should be such that they describe this $Z$-function in the standard linearised way: 
\begin{align}
Z^{e} =\beta ^{e} +\alpha _{1}^{e} U_{1} +...+\alpha _{n}^{e} U_{n} \label{ZEqnNum596963} 
\end{align}

\paragraph*{Computational procedure}

To compute the equivalent $\alpha$-values, the following procedure is applied:

\textbf{[1]} Derive conditional failure probability $P(Z_2<0$\textbar $Z_1<0)$ with the Hohenbichler method as described in the previous section using the following values as input: reliability index $\beta_1$ of $Z_1$, reliability index $\beta_2$ of $Z_2$ and the influence variable, $\alpha $, for each random variable involved. Subsequently, compute $P[Z_1\cup Z_2]$ (or $P[Z_1\cap Z_2]$ if a parallel system is considered) with the techniques described in \Aref{sec:combinationprocedure}. Also compute the associated ''equivalent'' reliability index, $\beta^e$:
\begin{align}
\beta ^{e} =\Phi ^{-1} \left(P\left[Z_{1} \cup Z_{2} \right]\right)\label{ZEqnNum851771} 
\end{align}

\textbf{[2]} For each variable $U_k$ , $k=1...n$, carry out steps [2a] and [2b]

\textbf{[2a]} Repeat the procedure of step 1 with input reliability indices $\beta_1$' = $\beta_1$ + $\varepsilon$$\alpha_{1k}$ and $\beta_2$' = $\beta_2$ + $\varepsilon$$\alpha_{2k}$, where $\varepsilon$ is a small perturbation. Refer to the resulting reliability index as: $\beta_k^e$($\varepsilon$).

\textbf{[2b]} Compute the equivalent $\alpha$-value of variable $U_k$ as follows:
\begin{align}
\alpha _{k}^{e} =\frac{\beta _{k}^{e} \left(\varepsilon \right)-\beta ^{e} }{\varepsilon } \label{ZEqnNum303161} 
\end{align}

The result is set of $n$ equivalent $\alpha $-values, $\alpha_1^e$,..., $\alpha_n^e$ (step 2) and an equivalent reliability index $\beta^e$ (step 1). These values define the equivalent $Z$-function as described in equation \eqref{ZEqnNum596963}.

\paragraph*{Detailed explanation}

The equivalent value $\beta^e$ is the reliability index that is derived with the Hohenbichler method as described in \Aref{Section_2.4.2.2}. In order to derive the $\alpha$-values of function $Z^e$, recall from \Aref{Section_2.2.5} that the $\alpha$-values of a $Z$-function are related to the reliability index $\beta$ as follows: 
\begin{align}
\frac{\partial \beta ^{e} }{\partial \bar{u}_{k} } =\alpha _{k}^{e} \label{ZEqnNum799990} 
\end{align}
where $\bar{u}_k$ is the mean value of variable $U_k$. In order to estimate the sensitivity of the equivalent reliability index $\beta^e$ to small perturbations in the mean value of variable $U_k$, we require a set of definitions. First, consider the two following linear $Z$-functions of $n$ components:
\begin{align}
Z_{i} =\beta _{i} +\alpha _{i1} U_{1} +...+\alpha _{ik} U_{k} +...+\alpha _{in} U_{n} \quad     ;i=1,2\label{ZEqnNum883757} 
\end{align}

Furthermore, define $Z_{ik}$($\varepsilon$), $i=1,2$, as the $Z$-functions that result from increasing the mean value of variable $U_k$ in equation \eqref{ZEqnNum883757} with a small perturbation $\varepsilon$. To analyse $Z_{ik}$($\varepsilon$), define:
\begin{align}
U_{k}^{'} =U_{k} +\varepsilon \label{ZEqnNum189970} 
\end{align}

Since $U_k$ is standard normally distributed, $U_k'$ is normally distributed with mean $\varepsilon$ and standard deviation $1$. Function $Z_{ik}$($\varepsilon$) is obtained by replacing $U_k$ in equation \eqref{ZEqnNum883757} by $U_k'$:
\begin{align}
\begin{array}{l} 
{Z_{ik} \left(\varepsilon \right)=\beta _{i} +\alpha _{i1} U_{1} +...+\alpha _{ik} U_{k}^{'} +...+\alpha _{in} U_{n} \quad   \quad \quad    ;i=1,2} \\
 {\quad \quad    =\beta _{i} +\alpha _{i1} U_{1} +...+\alpha _{ik} \left(U_{k} +\varepsilon \right)+...+\alpha _{in} U_{n} \quad     ;i=1,2} \\
 {\quad \quad    =\left(\beta _{i} +\alpha _{ik} \varepsilon \right)+\alpha _{i1} U_{1} +...+\alpha _{ik} U_{k} +...+\alpha _{in} U_{n} \quad ;i=1,2} \end{array}\label{ZEqnNum876903} 
\end{align}

The equivalent reliability index, $\beta_k^e$, for this perturbed system of two components is computed as follows: 
\begin{align}
\beta _{k} ^{e} \left(\varepsilon \right)=\Phi ^{-1} \left(1-P\left[Z_{1k} \left(\varepsilon \right)<0\cup Z_{2k} \left(\varepsilon \right)<0\right]\right)\label{ZEqnNum789988} 
\end{align}

The equivalent $\alpha$-value, $\alpha_k^e$, can be derived from:
\begin{align}
\alpha _{k}^{e} =\frac{\partial \beta _{k} ^{e} }{\partial \varepsilon } =\frac{\partial }{\partial \varepsilon } \left[\Phi ^{-1} \left(1-P\left[Z_{1k} \left(\varepsilon \right)<0\cup Z_{2k} \left(\varepsilon \right)<0\right]\right)\right]\label{ZEqnNum109184} 
\end{align}

Generally, $Z$-functions are too complex to derive equation \eqref{ZEqnNum109184} analytically. Therefore, a numerical approach is required in which the mean of variable $U_i$ is perturbed by a small value $\varepsilon_i$ and subsequently the change in the value of $\beta^e$ is quantified.

From equations \eqref{ZEqnNum883757} and \eqref{ZEqnNum876903} it follows:
\begin{align}
Z_{ik} \left(\varepsilon \right)=Z_{i} +\alpha _{ik} \varepsilon     ;i=1,2\label{0.100)} 
\end{align}

So if $Z_i$ has a reliability index $\beta_i$, $Z_{ik}$($\varepsilon$) has a reliability index equal to $\beta_i$ + $\varepsilon$$\alpha_{ik}$. So $Z_i$ and $Z_{ik}$($\varepsilon$) have different reliability indices, but the $\alpha$-values of these two $Z$-functions are the same. This explains why in step 2 of the procedure as described in the previous section, $Z$-functions with reliability indices $\beta_i$+$\varepsilon$$\alpha_{ik}$ are evaluated with the Hohenbichler method to quantify the sensitivity of the equivalent $Z$-function to small perturbations $\varepsilon$ in the mean value of variable $U_k$. 

\paragraph*{Example}

\Fref{fig:2.27} shows an example of two $Z$-functions, $Z_1$ and $Z_2$, and the equivalent $Z$-function, $Z^{e}$. The variables of the $Z$-functions are displayed in \Tref{tab:2.4}. The failure probability $P(Z_1<0\cup Z_2<0)$ as derived with the Hohenbichler method in this example is equal to $6.120 \cdot 10^{-3}$, whereas the failure probability based on Monte Carlo sampling with $10^{8}$ samples (i.e.\ a very accurate method) is equal to $6.121 \cdot 10^{-3}$. This shows that the Hohenbichler method in principle is an exact method for combining 2 lineair $Z$-functions (linear as a function of the $u$-variables). However, errors will be introduced [a] if the real $Z$-functions are non-linear or [b] when combining more than 2 components.

\begin{longtable}{|p{\textwidth-84pt-90mm}|p{15mm}|p{15mm}|p{15mm}|p{15mm}|p{15mm}|p{15mm}|}
\caption{Description of the $Z$-functions of \Fref{fig:2.27}.}\label{tab:2.4} \\ \hline 
\textbf{Function} & \multicolumn{3}{|p{45mm}|}{\textbf{Before normalization}} & \multicolumn{3}{|p{45mm}|}{\textbf{After normalisation}} \\ \hline 
Variable: & $\alpha_1$ & $\alpha_2$ & $\beta$ & $\alpha_1$ & $\alpha_2$ & $\beta$ \\ \hline 
$Z_1$ & $-2$ & $-1$ & $6$ & $-0.89$ & $-0.45$ & $2.68$ \\ \hline 
$Z_2$ & $-1$ & $-2$ & $6$ & $-0.45$ & $-0.89$ & $2.68$ \\ \hline 
\end{longtable}

\begin{figure}[H]\centering
\includegraphics*[width=4.70in, height=3.53in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image36}
\caption{Replacement of ''$Z_1<0$ $U$ $Z_2$ $<$ 0'' with ''$Z^e$ $<$ 0''. Failure domains of the three $Z$-functions are on the upper right side of the respective lines.}\label{fig:2.27}
\end{figure}

\subsubsection{Hohenbichler method for the general case of partial correlation}\label{Hohenbichler_method_for_the_general_case_of_partial_correlation}%\label{Section_2.4.2.4}

In the previous sections, $Z_1$ and $Z_2$ were functions of the same variables $U_1$,\dots ,$U_n$. In the current section the more general case is considered: 
\begin{align}
\begin{array}{l} {Z_{1} =\beta _{1} +\alpha _{11} U_{11} +...+\alpha _{1n} U_{1n} } \\ {Z_{2} =\beta _{2} +\alpha _{21} U_{21} +...+\alpha _{2n} U_{2n} } \end{array} \label{ZEqnNum784001} 
\end{align}
where $U_{ik}$ refers to the $k^{th}$ random variable of the $i^{th}$ $Z$-function. In this case, variables $U_{1k}$ and $U_{2k}$ are partially correlated. All other correlations are equal to zero:
\begin{align}
\rho \left(U_{1j} ,U_{2k} \right)=\left\{\begin{array}{c} {\rho _{12k} \quad ;j=k} \\ {0\quad \quad  ;j\ne k} \end{array}\right.  \label{ZEqnNum773527} 
\end{align}

For components $1$ and $2$, the $k^{th}$ random variable (i.e.\ $U_{1k}$ and $U_{2k}$) in principle refers to the same load or strength variable, but the sampled values can be different because they refer to different components. For instance, the $k^{th}$ variable may refer to the thickness of a clay layer. This thickness will be different for different dike segments. 

Note that equation \eqref{ZEqnNum784001} also covers the case in which the two $Z$-functions depend on different sets of random variables, for instance because two dike segments are combined that are situated along different water systems, or if one component is a dike segment and the other one is a dune segment. In that case, the combined sets of random variables will be used in equation \eqref{ZEqnNum784001} and some of the $\alpha$-values will be equal to zero.

\paragraph*{Computational procedure}

To compute the equivalent $\alpha$-values, the following procedure is applied:

\textbf{[1]} Derive conditional failure probability $P(Z_2<0$\textbar $Z_1<0)$ with the Hohenbichler method as described \Aref{Section_2.4.2.2} using the following values as input: reliability index $\beta_1$ of $Z_1$, reliability index $\beta_2$ of $Z_2$ and the influence variable, $\alpha $, for each random variable involved. Subsequently, compute $P[Z_1\cup Z_2]$ (or $P[Z_1\cap Z_2]$ if a parallel system is considered) with the techniques described in \Aref{sec:combinationprocedure}. Also compute the associated ''equivalent'' reliability index, $\beta^e$:
\begin{align}
\beta ^{e} =\Phi ^{-1} \left(P\left[Z_{1} \cup Z_{2} \right]\right)\label{eq:smXX1} 
\end{align}

\textbf{[2]} For each variable $U_k$, $k=1...n$, carry out steps [2a] -- [2e]: 

\textbf{[2a]} Repeat the procedure of step 1 with input reliability indices $\beta_1$' = $\beta_1$ + $\varepsilon$$\alpha_{1k}$ and $\beta_2$' = $\beta_2$ + $\varepsilon\rho_{12k}\alpha_{2k}$. The resulting equivalent reliability index is referred to as: $\beta_c$.

\textbf{[2b]} Compute the following equivalent $\alpha$-value:
\begin{align}
\alpha _{k}^{I} =\frac{\beta ^{c} -\beta ^{e} }{\varepsilon } \label{0.104)} 
\end{align}

\textbf{[2c]} Repeat the procedure of step 1 with input reliability indices $\beta_1$' = $\beta_1$ and $\beta_2$' = $\beta_2$ + $c$, with $c$ equal to 
\begin{align}
c=\varepsilon \alpha _{2,k} \sqrt{1-\left(\rho _{12k} \right)^{2} } \label{0.105)} 
\end{align}

The resulting equivalent reliability index is referred to as: $\beta^u$.

\textbf{[2d]} Compute the following equivalent $\alpha$-value:
\begin{align}
\alpha _{k}^{II} =\frac{\beta ^{u} -\beta ^{e} }{\varepsilon } \label{0.106)} 
\end{align}

\textbf{[2e]} Compute:
\begin{align}
\alpha _{k}^{e} =\sqrt{\left(\alpha _{k}^{I} \right)^{2} +\left(\alpha _{k}^{II} \right)^{2} } \label{0.107)} 
\end{align}

The result of steps 2a -- 2e is set of $n$ equivalent $\alpha $-values, $\alpha_1^e$...$\alpha_n^e$.

\textbf{[3]} Carry out a normalization step
\begin{align}
\alpha _{k;final}^{e} =\frac{\alpha _{k}^{e} }{\sqrt{\sum _{j=1}^{n}\left(\alpha _{j}^{e} \right)^{2}  } } \quad ;k=1...n\label{ZEqnNum410058} 
\end{align}

\paragraph*{Detailed explanation}

From equation \eqref{ZEqnNum773527} it follows that the correlation between $Z_1$ and $Z_2$ is equal to: 
\begin{align}
\rho \left(Z_{1} ,Z_{2} \right)=\sum _{k=1}^{n}\alpha _{1k} \alpha _{2k} \rho _{12k}  \label{ZEqnNum590491} 
\end{align}

In order to determine the probability of failure $P$($Z_1<0$ $~\cup~$ $Z_2$$<$0), the exact same method as described in \Aref{Section_2.4.2.2} is used. In order to derive the equivalent $\alpha$-values, the procedure becomes somewhat more complicated than the procedure with full correlation that was described in \Aref{Section_2.4.2.3}. The approach is to describe the $U$-variables of $Z_2$ as a function of the $U$-variables of $Z_1$:
\begin{align}
u_{2k} =U_{1k} \rho _{12k} +U_{2k}^{*} \sqrt{(1-\rho _{12k}^{2} )} \quad ;k=1...n\label{ZEqnNum645187} 
\end{align}

The first term in this function represents the part of $U_{2k}$ that is fully correlated to $U_{1k}$, the second term describes the part that is fully uncorrelated to $U_{1k}$. Variable $U_{2k}^{*}$ is standard normally distributed and independent of variable $U_{1k}$. To verify the applicability of equation \eqref{ZEqnNum645187} it needs to be shown that [1] $U_{2k}$ is standard normally distributed and [2] that $U_{1k}$ and $U_{2k}$ have a mutual correlation coefficient that is equal to $\rho_{12k}$. Note that this was proven earlier in the follow-up of equation \eqref{eq:2-80}. Inserting the expression for $U_{2k}$, given by equation \eqref{ZEqnNum645187}, into the formula for $Z_2$ (equation \eqref{ZEqnNum784001}) gives:
\begin{equation}
Z_{2} =\beta _{2} +\alpha _{21} \left(U_{11} \rho _{121} +U_{21}^{*} \sqrt{(1-\rho _{121}^{2} )} \right)+\dots+\alpha _{2n} \left(U_{1n} \rho _{12n} +U_{2n}^{*} \sqrt{(1-\rho _{12n}^{2} )} \right)\label{0.111)} 
\end{equation}

which can be written more compactly as follows:
\begin{align}
Z_{2} =\beta _{2} +\sum _{k=1}^{n}\alpha _{2k} \left(U_{1k} \rho _{12k} +U_{2k}^{*} \sqrt{(1-\rho _{12k}^{2} )} \right) \label{ZEqnNum452558} 
\end{align}

Note that the expression for $Z_1$ in equation \eqref{ZEqnNum784001} can also be written more compactly as follows:
\begin{align}
Z_{1} =\beta _{1} +\sum _{k=1}^{n}\alpha _{1k} U_{1k}  \label{ZEqnNum898859} 
\end{align}

The procedure for determining the equivalent $\alpha$-values is similar to that introduced for the case of full correlation (\Aref{Section_2.4.2.3}). So again, an equivalent $Z$-function of the following form is derived:
\begin{align}
Z^{e} =\beta ^{e} +\alpha _{1}^{e} U_{1} +...+\alpha _{n}^{e} U_{n} =\beta ^{e} +\sum _{k=1}^{n}\alpha _{k}^{e} U_{k}  \label{ZEqnNum630877} 
\end{align}

The main difference with \Aref{Section_2.4.2.3} is that the random variable $U_{k}$ in equation \eqref{ZEqnNum630877} will represent the two variables $U_{1k}$ and $U_{2k}$ of the functions $Z_1$ and $Z_2$., whereas in \Aref{Section_2.4.2.3}, $U_{1k}$ and $U_{2k}$ were fully correlated and therefore described by a single variable $U_k$. The approach in the current section is that first a separate equivalent $\alpha$-value is computed for variables $U_{1k}$ and $U_{2k}^{*}$. Subsequently, these two equivalent $\alpha$-values are combined into a single equivalent $\alpha$-value. 

The first step is to derive the partial derivative of $\beta^e$ to variable $U_{1k}$. Similar to \Aref{Section_2.4.2.3} this is done by quantifying the change in $\beta^e$ as a result of a small perturbation in the value of $u_{1k}$. $\beta^e$ is related to the two $Z$-functions as follows:
\begin{align}
\beta ^{e} =\Phi ^{-1} \left[1-P\left\{Z_{1} <0\cup Z_{2} <0\right\}\right]\label{ZEqnNum481314} 
\end{align}

Substituting equations \eqref{ZEqnNum452558} and \eqref{ZEqnNum898859} into equation \eqref{ZEqnNum481314} gives:
{\begin{align}
\beta ^{e} =&\Phi ^{-1} [1-P\{\beta _{1} +\sum _{k=1}^{n}\alpha _{1k} U_{1k}  <0\cup \beta _{2} \label{ZEqnNum308662}\\\nonumber&+\sum _{k=1}^{n}\alpha _{2k} \left(U_{1k} \rho _{12k} +U_{2k}^{*} \sqrt{(1-\rho _{12k}^{2} )} \right) <0\}]  
\end{align}

A small perturbation, $\varepsilon$, on the mean value of $u_{1k}$ will have the following effect on $\beta^e$: 
\begin{align}
\beta _{k;c} ^{e} (\varepsilon )=\Phi ^{-1} [1-P\{ Z_{1} <-\alpha _{1k} \varepsilon \cup Z_{2} <-\alpha _{2k} \varepsilon \rho _{12k} \} ] \label{ZEqnNum346117} 
\end{align}

This value can be computed once again with the Hohenbichler method (or alternative methods for combining two components). The equivalent $\alpha$-value for variable $U_{1k}$ can than be obtained from:
\begin{align}
\alpha _{k}^{I} =\frac{\beta _{k;c} ^{e} (\varepsilon )-\beta ^{e} }{\varepsilon } \label{0.118)} 
\end{align}

Subsequently, a similar sensitivity analysis is done for $U_{2k}^{*}$. A small perturbation, $\varepsilon$, in the mean value of $U_{2k}^{*}$ has the following effect on $\beta_k^e$ (see equations \eqref{ZEqnNum481314} and \eqref{ZEqnNum308662}):
\begin{align}
\beta _{k;u} ^{e} (\varepsilon )=\Phi ^{-1} [1-P\{ Z_{1} <0   \cup    Z_{2} <-\alpha _{2k} \varepsilon \sqrt{1-\rho _{12k}^{2} } \} ]\label{ZEqnNum705847} 
\end{align}

This value can be computed once again with the Hohenbichler method (or alternative methods for combining two components). The equivalent $\alpha$-value for variable $U_{2k}^{*}$ can than be obtained from:
\begin{align}
\alpha _{k}^{II} =\frac{\beta _{k;u} ^{e} (\varepsilon )-\beta ^{e} }{\varepsilon } \label{0.120)} 
\end{align}

The two derived $\alpha$-values can then be combined as follows:
\begin{align}
\alpha _{k}^{e} =\sqrt{\left(\alpha _{k}^{I} \right)^{2} +\left(\alpha _{k}^{II} \right)^{2} } \label{ZEqnNum745223} 
\end{align}

This is the required equivalent $\alpha$-value for the $k^{th}$ random variable in the combined $Z$-function $Z^e$. Equation \eqref{ZEqnNum745223} can be explained as follows. The $Z$-functions of the two components, $Z_1$ and $Z_2$ are a function of mutually independent standard normally distributed variables $U_{11}$, $U_{21}^{*}$, \dots  , $U_{1n}$, $U_{2n}^{*}$ (see equations \eqref{ZEqnNum784001} and \eqref{ZEqnNum452558}). For each of these variables an equivalent $\alpha$-value was derived: $\alpha_1^I$, $\alpha_1^{II}$,\dots  $\alpha_n^I$, $\alpha_n^{II}$. This means the combined $Z$-function of the two components can be written as follows: 
\begin{align}
\begin{array}{l} {Z^{e} =\beta ^{e} +\alpha _{1}^{I} U_{11} +\alpha _{1}^{II} U_{21}^{*} +...+\alpha _{n}^{I} U_{1n} +\alpha _{n}^{II} U_{2n}^{*} } \\ {\quad  =\beta ^{e} +\sum _{k=1}^{n}\left(\alpha _{k}^{I} U_{1k} +\alpha _{k}^{II} U_{2k}^{*} \right) } \end{array}
\end{align}

If we compare this equation with equation \eqref{ZEqnNum630877} it is clear that the ''new'' random variable $U_k$ replaces the pair of random variables $U_{1k}$ and $U_{2k}^{*}$ and also that the equivalent $\alpha$-value, $\alpha_k^e$, replaces $\alpha_k^{I}$ and $\alpha_k^{II}$.This can only be done if the standard deviation of $\alpha_k^e U_k$ is equal to the standard deviation of: $\alpha_k^{I} U_{1k}+\alpha_k^{II} U_{2k}^{*}$. This is the case if we chose $\alpha_k^e$ according to equation \eqref{ZEqnNum745223}.

\Note{application of equation \eqref{ZEqnNum745223} will result in a value of $\alpha_k^e$ that is non-negative. For load variables this is incorrect, as they have negative $\alpha$-values. Therefore, for load variables, $\alpha_k^e$ should be taken equal to:}
\begin{align}
\alpha _{k}^{e} =-\sqrt{\left(\alpha _{k}^{I} \right)^{2} +\left(\alpha _{k}^{II} \right)^{2} }
\end{align}

If it is not known whether the $k^{th}$ variable is a load variable, this information can be obtained by reading the sign of $\alpha _{k}^{I}$ and $\alpha _{k}^{II}$.

\Note{since the equivalent $\alpha$-values are derived numerically, the sum of the squares of the equivalent $\alpha$-values may differ from $1$. In that case an additional normalization step is required:}
\begin{align}
\alpha _{k;final}^{e} =\frac{\alpha _{k}^{e} }{\sqrt{\sum _{j=1}^{n}\left(\alpha _{j}^{e} \right)^{2}  } } \quad ;k=1...n\label{0.122)} 
\end{align}

\subsubsection{Systems with an arbitrary number of components}\label{Section_2.4.2.5}

We have just considered the case of probability of failure of a parallel system of two components. In this section we extend the concept to an arbitrary number of components. Suppose we have an arbitrary system of $n$ components. The failure probability for the system is given by: 
\begin{align}
P(F)=P(Z_{1} <0  \cup   Z_{2} <0  \cup ...\cup Z_{n} <0)\label{0.123)}  
\end{align}

An example is the computation of the failure probability due to the mechanism overtopping over $n$ defense segments. The function $Z_i$ is the limit state function for component $i$ and the occurrence $Z_i$ $<$ $0$ indicates failure of component $i$. 

The procedure of combining is to first combine two components, so that the problem with $n$ components reduces to a problem with $n-1$ components. The next step combines two components again so that the problem reduces to one with $n-2$ components, and continues in this fashion until only one component remains, where this last component represents the entire system. 

The order of the combination is important. The determination of equivalent $\alpha$-values, discussed in the previous section, is an approximating method, which makes the entire combination procedure an approximating method. The accuracy of the resulting failure probability is influenced by the sequence in which the components are combined. The most accurate results are obtained by combining the most correlated components first. This is clarified by the example below with three $Z$-functions in \Tref{tab:2.5} and \Fref{fig:2.28}. 

\begin{longtable}{|p{0.7in}|p{0.5in}|p{0.5in}|p{0.5in}|}
\caption{Description of the $Z$-functions of \Fref{fig:2.28}}\label{tab:2.5} \\ \hline 
 & \multicolumn{3}{|p{1.5in}|}{\textbf{Variable}} \\ \hline 
\textbf{Function} & $\mathbf{\alpha_1}$ & $\mathbf{\alpha_2}$ & $\mathbf{\beta}$ \\ \hline 
$Z_1$ & $-1$ & $0$ & $2$ \\ \hline 
$Z_2$ & $0$ & $-1$ & $2$ \\ \hline 
$Z_3$ & $-1$ & $0$ & $2.5$ \\ \hline 
\end{longtable}

\begin{figure}[H]\centering
\includegraphics*[width=4.41in, height=3.31in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image37}
\caption{Functions $Z_1$, $Z_2$ and $Z_3$ of the current example}\label{fig:2.28}
\end{figure}

In this example, functions $Z_1$ and $Z_3$ are mutually fully correlated, whereas they are fully uncorrelated with function $Z_2$. We will demonstrate that the best strategy is to first combine the two correlated $Z$-functions ($Z_1$ and $Z_3$). First of all the exact solution for this relatively easy example is derived. It is clear from \Fref{fig:2.28} that if $Z_3$$<$0 $\Rightarrow$ $Z_1<0$. This means:
\begin{align}
P(Z_{1} <0    \cup   Z_{3} <0)=P(Z_{1} <0  )\label{ZEqnNum715861} 
\end{align}

And therefore:
\begin{align}
P(F)=P(Z_{1} <0  \cup   Z_{2} <0  \cup   Z_{3} <0)=P(Z_{1} <0  \cup   Z_{2} <0  )\label{0.125)} 
\end{align}

Since $Z_1$ and $Z_2$ are independent, the failure probability is equal to:
\begin{equation}
P(Z_{1} <0  \cup   Z_{2} <0)=1-P(Z_{1} \ge 0 )P(Z_{2} \ge 0 )=1-\Phi \left(\beta _{1} \right)\Phi \left(\beta _{2} \right)=1-\Phi \left(2\right)^{2} \approx 0.045\label{0.126)} 
\end{equation}

If we combine $Z_1$ and $Z_3$ first with the Hohenbichler method, the exact same result is obtained. If we combine $Z_1$ and $Z_2$ first, the estimated failure probability is equal to $0.0482$, whereas if we combine $Z_2$ and $Z_3$ first, the probability of failure is equal to $0.0492$. This demonstrates that in this example indeed the best strategy is to first combine the two components with the largest mutual correlation. To understand why this is the case, \Fref{fig:2.29} shows the equivalent function, $Z^e$ of $Z_1$$~\cup~$$Z_3$. This function turns out to be exactly the same as $Z_1$. Recall from equation \eqref{ZEqnNum715861} that this means that $Z^e$ is an exact representation of $Z_1$$~\cup~$$Z_3$. 

\Fref{fig:2.30} shows the equivalent function, $Z^e$ of $Z_1$$~\cup~$$Z_2$. Clearly, this function is a compromise between $Z_1<0$$~\cup~$$Z_2$$<0$ and it is clear why this introduces some errors after combining with function $Z_3$$<$0 (for instance because $Z_3$ now defines part of the failure domain, whereas in the original problem statement it was redundant). 

\begin{figure}[H]\centering
\includegraphics*[width=4.63in, height=3.47in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image38}
\caption{Function $Z_2$ and the equivalent $Z$-function of $Z_1$$~\cup~$$Z_3$.}\label{fig:2.29}
\end{figure}

\begin{figure}[H]\centering
\includegraphics*[width=4.15in, height=3.11in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image39}
\caption{Functions $Z_1$=0, $Z_2$ =0 and the equivalent $Z$-function of $Z_1<0$$~\cup~$$Z_2$$<$0.}\label{fig:2.30}
\end{figure}

\Fref{fig:2.31} helps illustrate the concept of combining components with the largest mutual correlation, with an example. Shown are four components with reliability functions $Z_1$, $Z_2$, $Z_3$, and $Z_4$. Let functions $Z_1$ and $Z_2$ be the most strongly correlated. These two components are then first combined and replaced by the equivalent reliability function $Z_2^e$. For the three remaining components, the correlations between them are again computed. Consider the case where $Z_3$ and $Z_4$ are now the most correlated; the following step will be the combination of $Z_3$ and $Z_4$, resulting in the equivalent reliability function $Z_4^e$. The final step is the combination of $Z_2^e$ and $Z_4^e$. 

\begin{figure}[H]\centering
\includegraphics*[width=3.41in, height=1.83in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image40}
\caption{Example of combining failure probabilities over four components}\label{fig:2.31}
\end{figure}

\subsection{Upscaling for systems with identical components: numerical integration with constant correlation}\label{Section_2.4.3}

Upscaling refers to combining failure probabilities over ''identical components''. Upscaling is distinguished from the more generic Hohenbichler combination techniques because the components being identical allows for a more convenient and efficient combination procedure. Identical in this case refers to the fact that the components have the same failure probability (i.e.\ the same reliability index $\beta $) and they are mutually correlated with the same correlation coefficient $\rho$: 
\begin{align}
\begin{array}{l} {\beta \left(Z_{i} \right)\quad    =\beta \quad ;i=1...n_{e} } \\ {\rho \left(Z_{i} ,Z_{j} \right)=\rho \quad ;i\ne j} \end{array}\label{ZEqnNum897176} 
\end{align}
where $n_e$ is the number of components, $\rho$ is the correlation coefficient ($\rho \geqslant 0$) and $Z_i$ is the $Z$-function of component $i$. Note that in general, the components also have in common the underlying set of random variables and the associated $\alpha$-values, but that is not a necessary condition for applying the method as described in this section.

Examples of when upscaling may be applied are the combining of failure probabilities at one time scale to a larger time scale  and upscaling failure probabilities from a cross section of a defense segment to the longitudinal extent of the segment. Such applications are described in \Aref{Section_2.5.5}. The value of $\rho$ first needs to be determined from system knowledge. For now, $\rho$ is assumed to be known.

\subsubsection{Computational procedure}\label{Section_2.4.3.1}

The failure probability of this system can be computed by solving the following integral numerically:
\begin{align}
P(F)=\int _{v}^{}\left[1-\left\{1-\Phi \left(-\beta^{*}\right)\right\}^{n} \right]\phi (v)dv  \label{0.145)} 
\end{align}

In which $\phi$ is the standard normal density function and $\beta^{*}$ is equal to:
\begin{align}
\beta ^{*} =\frac{\beta -v\sqrt{\rho } }{\sqrt{1-\rho } } \label{0.144)} 
\end{align}

\subsubsection{Detailed explanation}\label{Section_2.4.3.2}

The upscaling method makes use of linearized approximations of the $Z$-functions, as described in \Aref{Section_2.2.5}. The estimated probability of failure of the system is therefore an approximation of the true probability of failure. Errors made in the approximation will depend on the system under condideration. In \Aref{Section_2.2.5} it was shown that linearised $Z$-functions can be described as follows:
\begin{align}
Z_{i} =\beta -\alpha _{1} U_{i1} -...-\alpha _{n} U_{in} \quad ;i=1...n_{e} \label{ZEqnNum573383} 
\end{align}

Furthermore, it was shown that the sum of the product of $\alpha$-values and standard normally distributed $U$-values can be replaced by a single standard normally distributed $U$-variable:
\begin{align}
Z_{i} =\beta -U_{i} \quad ;i=1...n_{e} \label{ZEqnNum310338} 
\end{align}
where $U_i$ is a standard normally distributed variable and $\beta$ is the reliability index of each individual $Z$-function. The value of $\beta$ is considered to be known, i.e.\ it is determined by the probabilistic computation techniques as described in \Aref{Section_2.3}. This means $\beta$ is a constant in equation \eqref{ZEqnNum310338} and the mutual correlation of the $Z$-functions is therefore entirely determined by the mutual correlation of the $U$-variables:
\begin{align}
\rho \left(Z_{i} ,Z_{j} \right)=\rho \left(U_{i} ,U_{j} \right)=\rho \quad ;i\ne j\label{ZEqnNum614884} 
\end{align}

To describe a system that satisfies the relation of equation \eqref{ZEqnNum614884}, variable $U_i$ is written as a function of two independent standard normal random variables $U_i^{*}$ and $V$:
\begin{align}
U_{i} =U_{i}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \label{ZEqnNum725764} 
\end{align}

The variables $U_i^{*}$, $i=1...n$ are taken to be mutually independent:
\begin{align}
\rho \left(U_{_{i} }^{*} ,U_{_{j} }^{*} \right)=0\quad ;i\ne j\label{ZEqnNum257919} 
\end{align}

\Note{there is a difference between equation \eqref{ZEqnNum725764} and equation \eqref{eq:2-80}, where $\rho$ is used instead of $\sqrt{\rho}$, even though in both cases the correlation between $U_1$ and $U_2$ is equal to $\rho$. The reason for this difference is the fact that in equation \eqref{eq:2-80}, variable $U_2$ is written as a function of $U_1$, whereas in this case, $U_1$ and $U_2$ are both written as a function of a separate variable $V$.}

\Note{in the equations above, $\rho$ $>$ $0$. For $\rho$ = $0$, the Hohenbichler method in combination with the outcrossing approach should be used. The outcrossing method is discussed further on in this chapter.}

To verify the applicability of equation \eqref{ZEqnNum725764} it needs to be shown that [1] $U_i$ is standard normally distributed and [2] that the relation of equation \eqref{ZEqnNum614884} holds. To prove [1], we apply the following general rule (see, e.g. \cite{GrimmettStirzaker1983}): If $X$ and $Y$ are independent normally distributed random variables, then $aX+bY$ is also normally distributed with a mean, $\mu$, and standard deviation, $\sigma$, equal to:
\begin{align}
\begin{array}{l} {\mu =a\mu _{X} +b\mu _{Y} } \\ {\sigma =\sqrt{a^{2} \sigma _{X}^{2} +b^{2} \sigma _{Y}^{2} } } \end{array} 
\end{align}

Application of this rule on equation \eqref{ZEqnNum725764}, where $U_i^{*}$ and $V$ are both normally distributed with mean $0$ and standard deviation $1$, gives:
\begin{align}
\begin{array}{l} {\mu =\sqrt{1-\rho } \cdot 0-\sqrt{\rho } \cdot 0=0} \\ {\sigma =\sqrt{\left(1-\rho \right)\cdot 1+\rho \cdot 1} =1} \end{array} 
\end{align}

Which proves that $U_i$ is standard normally distributed. To prove [2], i.e.\ that the relation of equation \eqref{ZEqnNum614884} holds, equations \eqref{ZEqnNum310338} and \eqref{ZEqnNum725764} are combined: 
\begin{align}
Z_{i} =\beta -U_{i}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \quad ;i=1...n_{e} \label{ZEqnNum883030} 
\end{align}

The variable $V$ in equation \eqref{ZEqnNum883030} is part of each $Z$-function. This creates the desired mutual correlation between the functions $Z_i$, $i=1...n_e$. To prove this, it needs to be shown that variables $U_i$ and $U_j$, $i\neq j$, have a mutual correlation equal to $\rho$. The correlation between $U_i$ and $U_j$ is derived as follows:
\begin{align}
\rho \left(U_{i} ,U_{j} \right)=\frac{cov\left(U_{i} ,U_{j} \right)}{\left[\sigma \left(U_{i} \right)\sigma \left(U_{j} \right)\right]} =\frac{cov\left(U_{i} ,U_{j} \right)}{\left[1\cdot 1\right]} =cov\left(U_{i} ,U_{j} \right)\label{0.134)} 
\end{align}

The covariance of $U_i$ and $U_j$ is equal to:
\begin{align}
cov\left(U_{i} ,U_{j} \right) \begin{array}{l} {=E\left[U_{i} U_{j} -\mu \left(U_{i} \right)\mu \left(U_{j} \right)\right]=E\left[U_{i} U_{j} \right]} \\ {=E\left[\left(U_{i}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \right)\left(U_{j}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \right)\right]} \\ {=E\left[U_{i}^{*} U_{j}^{*} \left(1-\rho \right)-U_{i}^{*} V\sqrt{\rho \left(1-\rho \right)} -U_{j}^{*} V\sqrt{\rho \left(1-\rho \right)} +\rho V^{2} \right]} \\ {=E\left[0-0-0+\rho V^{2} \right]=\rho E\left[V^{2} \right]=\rho Var\left(V\right)=\rho } \end{array}\label{0.135)} 
\end{align}

This proves that equation \eqref{ZEqnNum883030} describes a system of $n_{e}$ components that are mutually correlated with a correlation coefficient $\rho$. The theorem of total probability is used to derive the probability of failure of this system: 
\begin{align}
P(F)=P\left(Z_{1} <0  \cup   \dots  \cup Z_{n} <0\right)=\int _{v}^{}P\left(Z_{1} <0  \cup   \dots  \cup Z_{n} <0|v\right)\phi \left(v\right)dv \label{ZEqnNum846568} 
\end{align}
where $\phi(v)$ is the probability density function of the standard normal distribution. The probability that at least $1$ component fails is equal to $1$ minus the probability that none of the components fails. Equation \eqref{ZEqnNum846568} can therefore be rewritten as:
\begin{align}
\begin{array}{l} {P(F)=\int _{v}^{}\left[1-P\left(Z_{1} \ge 0  \cap     \dots  \cap Z_{n} \ge 0|v\right)\right]\phi \left(v\right)dv } \\ {\quad \quad   =\int _{v}^{}\left[1-P\left\{\left(Z_{1} \ge 0|v\right)\cap  \dots  \cap  \left(Z_{n} \ge 0|v\right)\right\}\right]\phi \left(v\right)dv } \end{array}\label{ZEqnNum753473} 
\end{align}

For a given value of $V$, the individual failure probabilities of the $Z$-functions are mutually independent:
\begin{align}
P\left[\left(Z_{i} <0|v\right)\cap \left(Z_{j} <0|v\right)\right]=P\left(Z_{i} <0|v\right)P\left(Z_{j} <0|v\right)\quad ;i\ne j\label{ZEqnNum938061} 
\end{align}

This can be easily verified from equation \eqref{ZEqnNum725764}. If the value of $V$ is given, equation \eqref{ZEqnNum725764} only contains one random variable: $U_i^{*}$. Since the $U_i^{*}$-values are mutually independent (see equation \eqref{ZEqnNum257919}), the $Z$-functions of equation \eqref{ZEqnNum725764} are mutually independent as well, which leads to the equality in equation \eqref{ZEqnNum938061}. Substitution of equation \eqref{ZEqnNum938061} in equation \eqref{ZEqnNum753473} gives:
\begin{align}
P\left(F\right)=\int _{v}\left[1-\prod _{i=1}^{n_{e} }P\left(Z_{i} \ge 0  |v\right) \right]  \phi \left(v\right) dv\label{ZEqnNum169397} 
\end{align}

Because all components are identical, the following is true:
\begin{align}
P\left(Z_{1} \ge 0|v\right)=P\left(Z_{2} \ge 0|v\right)=...=P\left(Z_{n} \ge 0|v\right)\begin{array}{c} {\text{def}} \\ {=} \\ {} \end{array}P\left(Z\ge 0|v\right)\label{0.140)} 
\end{align}

This changes equation \eqref{ZEqnNum169397} into:
\begin{align}
P\left(F\right)=\int _{v}\left[1-P\left(Z\ge 0  |v\right)^{n_{e} } \right]  \phi \left(v\right)dv\label{ZEqnNum717936} 
\end{align}

The conditional probability of the $Z$-function in the integral is equal to: 
\begin{align}
P\left(Z\ge 0|v\right)=P\left(\beta -U^{*} \sqrt{1-\rho } -v\sqrt{\rho } \ge 0\right)=P\left(U^{*} \le \frac{\beta -v\sqrt{\rho } }{\sqrt{1-\rho } } \right)\label{ZEqnNum225981} 
\end{align}

Since $\nu$ is a given constant, $U^{*}$ is the only random variable in equation \eqref{ZEqnNum225981}, and since $U^{*}$ is standard normally distributed, the conditional probability is defined as:
\begin{align}
P\left(Z\ge 0|v\right)=\Phi \left(\frac{\beta -v\sqrt{\rho } }{\sqrt{1-\rho } } \right)\begin{array}{c} {\text{def}} \\ {=} \\ {} \end{array}\Phi \left(\beta ^{*} \right)\label{0.143)} 
\end{align}
where $\Phi$ is the standard normal distribution function and $\beta$ is equal to:
\begin{align}
\beta ^{*} =\frac{\beta -v\sqrt{\rho } }{\sqrt{1-\rho } } \label{0.144)} 
\end{align}

Equation \eqref{ZEqnNum717936} then changes into: 
\begin{align}
P(F)=\int _{v}^{}\left[1-\left\{1-\Phi \left(-\beta^{*}\right)\right\}^{n} \right]\phi (v)dv \label{ZEqnNum147042} 
\end{align}

The right hand side of equation \eqref{ZEqnNum147042} can be computed by numerical integration over the standard normally distributed variable $V$. Since $V$ is the only variable, the grid size of $V$ can be chosen small without requiring significant computation time. The error from the numerical integration of equation \eqref{ZEqnNum147042} can therefore be made as small as desired. This means the only potentially significant error that is introduced in this method is related to the linearisation of the $Z$-function, which was necessary to derive equation \eqref{ZEqnNum147042}.

\subsubsection{Equivalent alpha-values}\label{Section_2.4.3.3}

As with the Hohenbichler method, equivalent $\alpha$-values can be computed for the component that represents the combination of $n_{e}$ identical components. This is necessary in case the resulting component is used in subsequent combining procedures where $\alpha$-values are required. A similar approach with perturbed $u$-values is used as in the Hohenbichler method. However, because in this special case the components are identical, this allows for some convenient simplifications that require less computation time.

\paragraph*{Computational procedure}

The method for deriving equivalent $\alpha$-values for the systems with identical components is as follows:

\textbf{[1]} Apply the upscaling method of \Aref{Section_2.4.3.1} to $n_{e}$ components with reliability index $\beta$ and mutual correlation $\rho$ to derive the reliability index $\beta^e$ of the combined (upscaled) component.

\textbf{[2]} Apply the upscaling method of \Aref{Section_2.4.3.1} on $n_{e}$ components with reliability index   $\beta$-$\varepsilon$$\surd$$\rho$ and mutual correlation $\rho$ to derive the reliability index $\beta^e$($\varepsilon$) of the combined (upscaled) component.

\textbf{[3]} Determine $\alpha_v$: 
\begin{align}
\alpha _{v} =\frac{\beta ^{e} \left(\varepsilon \right)-\beta ^{e} }{\varepsilon } \label{0.159)} 
\end{align}

\textbf{[4]} For all random variables $k=1...n$, determine the equivalent $\alpha$-value, $\alpha_k^e$:
\begin{align}
\alpha _{k}^{e} =\sqrt{1-\alpha _{v}^{2} } \frac{\alpha _{k} \sqrt{1-\rho _{k} } }{\sqrt{1-\rho } } +\alpha _{v} \frac{\alpha _{k} \sqrt{\rho _{k} } }{\sqrt{\rho } } \label{0.163)} 
\end{align}

in which $\rho _{k}$ is the correlation between the $k^{th}$ random variable of an element with the corresponding ($k^{th}$) random variable in another element

\textbf{[5]} Normalise the equivalent $\alpha$-values:
\begin{align}
\alpha _{k;final}^{e} =\frac{\alpha _{k}^{e} }{\sqrt{\sum _{j=1}^{n}\left(\alpha _{j}^{e} \right)^{2}  } } \quad ;k=1...n\label{0.122)} 
\end{align}

The equivalent $\alpha$-values of the $n$ variables for the combined $n_{e}$ components are obtained with only two applications of the upscaling method (steps 1 and 2 above). This is very efficient, when compared to the Hohenbichler method, which needs to be repeated $2n+1$ times to derive the equivalent $\alpha$-values for combining only two components. Another difference with the Hohenbichler method is that the method for identical components can be applied for non-integer values of $n_{e}$; this is not the case for the Hohenbichler method. The application with non-integer values of $n_{e}$ is useful for example for upscaling the failure probability of a cross section to the failure probability of a dike section, as will be explained later on in this document.

\paragraph*{Detailed explanation}

Consider the system of $n_e$ identical components as described in equation \eqref{ZEqnNum573383}:
\begin{align}
Z_{i} =\beta -\alpha _{1} U_{i1} -...-\alpha _{n} U_{in} \quad ;i=1...n_{e} \label{ZEqnNum188185} 
\end{align}

These components are combined according to the method as described in \Aref{Section_2.4.3.1}, resulting in a failure probability and associated reliability index $\beta^e$. The combined component can be described by (similar to equation \eqref{ZEqnNum645187}): 
\begin{align}
Z^{e} =\beta ^{e} -\alpha _{1}^{e} U_{1} -...-\alpha _{n}^{e} U_{n} \label{ZEqnNum266010} 
\end{align}

The superscript ''e'' in this equation refers to the fact that these are equivalent values and functions. In order to derive the $\alpha$-values of function $Z^e$, recall from \Aref{Section_2.2.5} that the $\alpha$-values of a $Z$-function are related to the reliability index $\beta^e$ as follows: 
\begin{align}
\frac{\partial \beta ^{e} }{\partial \bar{u}_{k} } =\alpha _{k}^{e} \quad ;k=1...n\label{0.148)} 
\end{align}

In which $\overline{u}_i$ is the mean of variable $U_i$. Note the value $\alpha_k^e$ represents the combined effect of variables $U_{1k}$...$U_{n_{e}k}$, and that these variables are mutually correlated. In order to determine $\alpha_k^e$, these variables need to be split in an independent and mutually dependent part, similar to the description in \Aref{Section_2.4.3.1}. Consider for this purpose equation \eqref{ZEqnNum188185}. The different $U$-variables within a single component are mutually uncorrelated, whereas corresponding $U$-values in different components are correlated. In formula:
\begin{align}
\begin{array}{l} {\rho \left(U_{ij} ,U_{ik} \right)=0\quad   ;j\ne k} \\ {\rho \left(U_{ik} ,U_{\ell k} \right)=\rho _{k} \quad ;i\ne \ell } \end{array}\label{ZEqnNum568983} 
\end{align}

Each $U$-variable can therefore be split in a correlated and uncorrelated part: 
\begin{align}
U_{ik} =U_{ik}^{*} \sqrt{1-\rho _{k} } +V_{k} \sqrt{\rho _{k} } \label{ZEqnNum383869} 
\end{align}

%In which $U_{ik}^{*}$ and  $V_k$ are mutually independent standard normally distributed variables. 
Furthermore, the variables $U_{ik}^{*}$, $i=1...n_e$, $k=1...n$ and  $V_k$, $k=1...n$ are all taken to be mutually independent:
\begin{align}
\begin{array}{l} {\rho \left(U_{ij}^{*} ,U_{k\ell }^{*} \right)=0\quad ;i\ne j\cup k\ne \ell } \\ {\rho \left(U_{ij}^{*} ,V_{k} \right)=0} \\ {\rho \left(V_{j} ,V_{k} \right)=0\quad ;j\ne k} \end{array}\label{ZEqnNum753050} 
\end{align}

With this formulation, variables $U_{ik}$, $i=1...n_e$, $k=1...n$, automatically fulfill requirement \eqref{ZEqnNum568983}, as can be shown in the same manner as was done below equation \eqref{ZEqnNum257919} in the previous section. Substituting equation \eqref{ZEqnNum383869} in equation \eqref{ZEqnNum188185} gives:
\begin{align}
\begin{array}{l} {Z_{i} =\beta -\alpha _{1} \left(U_{i1}^{*} \sqrt{1-\rho _{1} } +V_{1} \sqrt{\rho _{1} } \right)-...-\alpha _{n} \left(U_{in}^{*} \sqrt{1-\rho _{n} } +V_{n} \sqrt{\rho _{n} } \right)\quad ;i=1...n_{e} } \\ {\quad =\beta -\sum _{k=1}^{n}\alpha _{k}  U_{ik}^{*} \sqrt{1-\rho _{k} } -\sum _{k=1}^{n}\alpha _{k} V_{k}  \sqrt{\rho _{k} } \quad ;i=1...n_{e} } \end{array}\label{0.152)} 
\end{align}

This equation can be replaced by:
\begin{align}
Z_{i} =\beta -U_{i}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \quad ;i=1...n_{e} \label{ZEqnNum395894} 
\end{align}

In which:
\begin{align}
\begin{array}{l} {\rho =\sum _{k=1}^{n}\left(\alpha _{k} \right)^{2}  \rho _{k} } \\ {U_{i}^{*} =\frac{1}{\sqrt{1-\rho } } \sum _{k=1}^{n}\alpha _{k} U_{ik}^{*} \sqrt{1-\rho _{k} }  \quad ;i=1...n_{e} } \\ {V=\frac{1}{\sqrt{\rho } } \sum _{k=1}^{n}\alpha _{k} V_{k}  \sqrt{\rho _{k} } } \end{array}\label{ZEqnNum809022} 
\end{align}

The validity of this replacement can be easily verified by substituting the formulations of $U_i$ and $V$ of equation \eqref{ZEqnNum809022} into equation \eqref{ZEqnNum395894}. Equation \eqref{ZEqnNum395894} is equivalent to equation \eqref{ZEqnNum883030} if and only if $U_i^{*}$ and $V$ are mutually independent standard normally distributed variables. The mutual independency can easily be shown since all components $U_{ik}^{*}$, $i=1...n_e$, $k=1...n$, and  $V_k$, $k=1...n$ are mutually independent (see \eqref{ZEqnNum753050}. 

To verify if $U_i^{*}$ and $V$ are standard normally distributed we apply the following general rule (see, e.g. \cite{GrimmettStirzaker1983}): If $X$ and $Y$ are independent normally distributed random variables, then $aX+bY$ is also normally distributed with a mean, $\mu$, and standard deviation, $\sigma$, equal to:
\begin{align}
\begin{array}{l} {\mu =a\mu _{X} +b\mu _{Y} } \\ {\sigma =\sqrt{a^{2} \sigma _{X}^{2} +b^{2} \sigma _{Y}^{2} } } \end{array}\label{0.155)} 
\end{align}

Application of this rule on equation \eqref{ZEqnNum809022}, where all components $U_{ik}^{*}$ and  $V_k$ are normally distributed with mean $0$ and standard deviation $1$, gives:
\begin{align}
\begin{array}{l} {\mu \left(U_{i} \right)=\frac{1}{\sqrt{\rho } } \sum _{k=1}^{n}\alpha _{k} \cdot 0\cdot \sqrt{1-\rho _{k} }  =0} \\ {\sigma \left(U_{i} \right)=\sqrt{\left(\frac{1}{\sqrt{1-\rho } } \right)^{2} \sum _{k=1}^{n}\left(\alpha _{k} \right)^{2} \cdot \left(1\right)^{2} \cdot \left(1-\rho _{k} \right) } =\sqrt{\frac{1}{1-\rho } \left[\sum _{k=1}^{n}\alpha _{k}^{2}  -\sum _{k=1}^{n}\alpha _{k}^{2} \rho _{k}  \right]} } \\ {\quad \quad   =\sqrt{\frac{1}{1-\rho } \left[1-\rho \right]} =\sqrt{1} =1} \end{array}\label{0.156)} 
\end{align}

This shows that $U_i^{*}$ and $V$ in equation \eqref{ZEqnNum395894} are mutually independent standard normally distributed variables. Taking into account the formulation of the $Z$-function in equation \eqref{ZEqnNum395894}, The equivalent coefficient $\alpha_k^e$ can now be derived as follows
\begin{align}
\alpha _{k}^{e} =\frac{\partial \beta ^{e} }{\partial \bar{u}_{k} } =\frac{\partial \beta ^{e} }{\partial \bar{u}} \frac{\partial \bar{u}}{\partial \bar{u}_{k} } +\frac{\partial \beta ^{e} }{\partial \bar{v}} \frac{\partial \bar{v}}{\partial \bar{u}_{k} } \label{ZEqnNum438636} 
\end{align}
where $\bar{u}$, $\bar{v}$ and $\bar{u}_k$ are the mean values of variables $U^{*}$, $V$ and $U_k$. So, the derivation of coefficients $\alpha_k$ $i=1...n_e$ it comes down now to determining the four partial derivatives of equation \eqref{ZEqnNum438636}. The first two can be determined directly from equation \eqref{ZEqnNum809022}:
\begin{align}
\begin{array}{l} {\frac{\partial \bar{u}}{\partial \bar{u}_{k} } =\frac{\alpha _{k} \sqrt{1-\rho _{k} } }{\sqrt{1-\rho } } } \\ {\frac{\partial \bar{v}}{\partial \bar{u}_{k} } =\frac{\alpha _{k} \sqrt{\rho _{k} } }{\sqrt{\rho } } } \end{array}\label{ZEqnNum945170} 
\end{align}

The partial derivative of $\beta^e$ to $\bar{v}$ is determined numerically:
\begin{align}
\alpha _{v} =\frac{\partial \beta ^{e} }{\partial \bar{v}} \approx \frac{\beta ^{e} \left(\varepsilon \right)-\beta ^{e} }{\varepsilon } \label{ZEqnNum432286} 
\end{align}

In which $\beta^e$($\varepsilon$) is the reliability index of the upscaled system of $n_e$ components, after perturbation of $\bar{v}$ with a small value $\varepsilon$. 
\begin{align}
\beta ^{e} \left(\varepsilon \right)=\Phi ^{-1} \left(1-P\left(Z^{e} \left(\varepsilon \right)<0\right)\right)=\Phi ^{-1} \left(1-\bigcup _{i=1}^{n_{e} }P\left(Z_{i} \left(\varepsilon \right)\right) \right)\label{ZEqnNum242869} 
\end{align}

In which function $Z_i$($\varepsilon$) is as follows: 
\begin{align}
\begin{array}{l} {Z_{i} \left(\varepsilon \right)=\beta -U_{i}^{*} \sqrt{1-\rho } -\left(V+\varepsilon \right)\sqrt{\rho } \quad ;i=1...n_{e} } \\ {\quad \quad    =\left(\beta -\sqrt{\rho } \varepsilon \right)-U_{i}^{*} \sqrt{1-\rho } -V\sqrt{\rho } \quad ;i=1...n_{e} } \end{array}\label{ZEqnNum791362} 
\end{align}

In other words: $Z_i$($\varepsilon$) is a $Z$-function with a reliability index $\beta_{Z_{\varepsilon}}$ following:
\begin{equation}
%\beta_Z = \frac{\mu_Z}{\sigma_Z} = \beta - \frac{\epsilon v}{\sqrt{\rho}}
\beta_{Z_{\varepsilon}} = \frac{\mu_Z}{\sigma_Z} = \frac{\beta  - \sqrt \rho  \varepsilon }{1} = \beta  - \sqrt \rho  \varepsilon 
%\{\beta _{{Z_\varepsilon }}} = \frac{{{\mu _{{Z_\varepsilon }}}}}{{{\sigma _{{Z_\varepsilon }}}}} = \frac{{\beta  - \sqrt \rho  \varepsilon }}{1} = \beta  - \sqrt \rho  \varepsilon \
\end{equation}
So $\beta^e$($\varepsilon$) is quantified by substituting equation \eqref{ZEqnNum791362} into equation \eqref{ZEqnNum242869} and subsequent application of the upscaling procedure of \Aref{Section_2.4.3.1}. Subsequently, $\beta^e$($\varepsilon$) is substituted in equation \eqref{ZEqnNum432286} in order to derive $\alpha_v$ the partial derivative of $\beta^e$ to $\bar{v}$. The next step is to derive the partial derivative of $\beta^e$ to $\bar{u}$. This can be derived as follows:
\begin{align}
\frac{\partial \beta ^{e} }{\partial \bar{u}} =\sqrt{1-\left(\frac{\partial \beta ^{e} }{\partial \bar{v}} \right)^{2} } =\sqrt{1-\alpha _{v}^{2} } \label{ZEqnNum165859} 
\end{align}

This can be explained as follows: the partial derivative of $\beta^e$ to $\bar{v}$ is the resulting $\alpha$-value for the dependent part of the $n_e$ components, represented by variable $V$. The partial derivative of $\beta^e$ to $\bar{u}$ is the resulting $\alpha$-value for the independent part of the $n_e$ components, represented by variable $U^{*}$. The sum of the squares of these $\alpha$-values should be equal to $1$. 

Substitution of equations \eqref{ZEqnNum945170}, \eqref{ZEqnNum432286} and \eqref{ZEqnNum165859} into equation \eqref{ZEqnNum438636} provides the requested equivalent $\alpha$-values:
\begin{align}
\alpha _{k}^{e} =\frac{\partial \beta ^{e} }{\partial \bar{u}_{k} } =\sqrt{1-\alpha _{v}^{2} } \frac{\alpha _{k} \sqrt{1-\rho _{k} } }{\sqrt{1-\rho } } +\alpha _{v} \frac{\alpha _{k} \sqrt{\rho _{k} } }{\sqrt{\rho } } \label{ZEqnNum998801} 
\end{align}

The $Z$-function of the resulting component from the upscaling procedure (equation \eqref{ZEqnNum266010}) needs to have a standard deviation equal to $1$. This means the sum of the squares of the equivalent $\alpha$-values shoud be equal to $1$. Equation \eqref{ZEqnNum998801} guarantees that this is the case if all values of $\rho_k$ are equal to either $0$ or $1$, which is generally the case for upscaling in time (i.e.\ slow varying random load variables and strength variables have an autocorrelation equal to $1$, while fast varying random variables have an autocorrelation equal to 0). This can be deducted as follows: 
\begin{align}
\begin{array}{l} {\sum _{k=1}^{n}\left(\alpha _{k}^{e} \right)^{2}  =\sum _{k=1}^{n}\left[\left(1-\alpha _{v}^{2} \right)\frac{\alpha _{k}^{2} \left(1-\rho _{k} \right)}{1-\rho } +2\alpha _{v} \left(\sqrt{1-\alpha _{v}^{2} } \right)\frac{\alpha _{k} \sqrt{1-\rho _{k} } }{\sqrt{1-\rho } } \frac{\alpha _{k} \sqrt{\rho _{k} } }{\sqrt{\rho } } +\alpha _{v}^{2} \frac{\alpha _{k}^{2} \rho _{k} }{\rho } \right] } \\ {\quad \quad \quad    =\sum _{k=1}^{n}\left[\left(1-\alpha _{v}^{2} \right)\frac{\alpha _{k}^{2} \left(1-\rho _{k} \right)}{1-\rho } +\alpha _{v}^{2} \frac{\alpha _{k}^{2} \rho _{k} }{\rho } \right] } \\ {\quad \quad \quad    =\frac{\left(1-\alpha _{v}^{2} \right)}{1-\rho } \sum _{k=1}^{n}\alpha _{k}^{2} \left(1-\rho _{k} \right)+ \frac{\alpha _{v}^{2} }{\rho } \sum _{k=1}^{n}\alpha _{k}^{2} \rho _{k}  } \\ {\quad \quad \quad    =\frac{\left(1-\alpha _{v}^{2} \right)}{1-\rho } \left(1-\rho \right)+\frac{\alpha _{v}^{2} }{\rho } \rho =\left(1-\alpha _{v}^{2} \right)+\alpha _{v}^{2} =1} \end{array}\label{0.164)} 
\end{align}

\Note{in the second step of this equation, the middle term is removed because it is equal to zero (since either $\rho_k=0$ or $1-\rho_k = 0$). In the fourth step, equation \eqref{ZEqnNum809022} is used. If not all values of $\rho_k$ are equal to either $0$ or $1$, the sum of the squares of the equivalent $\alpha$-values is not necessarily equal to $1$. In that case, they have to be normalized as follows:}
\begin{align}
\alpha _{k}^{e} =\frac{\alpha _{k}^{e} }{\sqrt{\sum _{j=1}^{n}\left(\alpha _{j}^{e} \right)^{2}  } } ;k=1...n\label{0.165)} 
\end{align}

\subsection{Techniques for time and space dependent processes}\label{Section_2.4.5}
In this section, aspects of upscaling in time and space are addressed.

%\subsubsection{Introduction}\label{Section_2.4.5.1}
The techniques described in the previous sections all deal with system analysis of a discrete number of components which may represent dike sections, wind directions, etc. In some applications, however, $Z$ is a function of space and time, which means in principle the number of components is infinite. This is schematically depicted in \Fref{fig:2.32}. In the left panel, $Z$ is a time-dependent function and failure potentially can occur at any time. On the right, $Z$ is a function of space, and failure can occur at any location.

\begin{figure}[H]\centering
\includegraphics*[width=5.93in, height=1.58in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image41}
\caption{Stochastic variation of the $Z$-function in time (left) and space (right)}\label{fig:2.32}
\end{figure}

This section describes some approaches to deal with these type of continuos descriptions of $Z$-functions.

\subsubsection{Poisson counting process} \label{Section_2.4.5.2}
The Poisson counting process describes the probability of occurrence of $n$ events, where a single event generally refers to an upcrossing or downcrossing of a threshold value. With respect to failure, the downcrossing of the threshold $Z=0$ is most relevant. In a Poisson process it is assumed that for small values of $\Delta t$ [a] the occurrence of an event in an interval $[t, t+\Delta t]$ is proportional to $\Delta t$ and [b] the probability of occurrence of two events occurring in $[t, t+\Delta t]$ is negligible. This means for small values of $\Delta t$, the probability of an event occurring in $[t, t+\Delta t]$ is approximately equal to:
\begin{align}
P\left(\mbox{1 event during } \left[t,t+\Delta t\right]\right)\approx \upsilon \Delta t\label{ZEqnNum344717} 
\end{align}

In this equation, $\nu$ is the `intensity' of the Poisson process. This is the single parameter that describes the Poisson process. Define $N(t)$ as the number of events occurring in the time interval  $[0,T]$. For a Poisson process the probability distribution of $N(t)$ is:
\begin{align}
P\left(N(t)=n\right)=\frac{\left(\nu t\right)^{n} e^{-\nu t} }{n!} \label{ZEqnNum870173} 
\end{align}

The time interval between two subsequent events is also a random variable and it is exponentially distributed. So if $t_{1}$ is the time interval between two events, then:
\begin{align}
P\left(T_{1} \le t_{1} \right)=1-e^{-\nu t_{1} } \label{ZEqnNum865643} 
\end{align}

The assumption of a Poisson process is often used to translate exceedance frequencies into exceedance probabilities. Suppose $\nu$ is expressed as ''number of events per year''. In that case $\nu$ is the annual frequency of occurrence. Then, according to equation \eqref{ZEqnNum865643}, the annual probability of occurrence is equal to: 
\begin{align}
P\left(T_{1} \le 1\right)=1-e^{-\nu } \label{ZEqnNum774553} 
\end{align}

This shows the relation between probability and frequency in case of a Poisson process. An event can be for instance the exceedance of a threshold level $x$, for load variable $X$. In that case, equation \eqref{ZEqnNum774553} can be applied to translate the annual frequency of exceedance of threshold $x$ into the annual probability of exceedance of threshold $x$, or vice versa.

In the description above, $\nu$ was assumed to be time-independent. If this is not the case, equation \eqref{ZEqnNum870173} changes into:
\begin{align}
P\left(N(t)=n\right)=\frac{\left(\int _{0}^{t}\nu \left(\tau \right)d\tau  \right)^{n} e^{-\int _{0}^{t}\nu \left(\tau \right)d\tau  } }{n!} \label{0.171)} 
\end{align}

\subsubsection{Outcrossing}\label{Section_2.4.5.3} 

If an event refers to failure in a continuous process, i.e.\ the downcrossing of threshold $Z=0$ in \Fref{fig:2.32}, then the outcrossing rate is defined as:
\begin{align}
v=\begin{array}{c} {\lim } \\ {\Delta t\downarrow 0} \end{array}  \frac{P\left[Z\left(t\right)\ge 0\cap Z\left(t+\Delta t\right)<0\right]}{\Delta t} \label{ZEqnNum351501} 
\end{align}

Note that the numerator in this equation is the probability that failure occurs in time interval $[t,t+\Delta t]$. The rate $\nu$ is similar to the one defined in the previous section and can also be time-dependent: $\nu$=$\nu(t)$. Assume for the moment that $\nu$ is a constant, i.e.\ independent of time. The probability that failure occurs in an interval  $(0,T]$, given the fact that no failure occurs at  $t=0$, is then equal to:
\begin{align}
P\left[\begin{array}{c} {\min } \\ {t\in \left(0,T\right]} \end{array} \left\{Z(t)\right\}<0\left| Z\left(0\right)\ge 0\right. \right]=1-e^{-vT} \label{0.173)} 
\end{align}

Note that this probability of failure is described by an exponential distribution function. The exponential distribution is by definition the distribution which describes failure probabilities for processes with a constant failure rate (see e.g. \cite{GrimmettStirzaker1983}). \Fref{fig:2.33} shows an example of an exponential distribution function. In this figure the failure rate, $\nu$, is taken equal to $1$, which makes this a standard exponential distribution function. 

\begin{figure}[H]\centering
\includegraphics*[width=4.45in, height=3.34in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image42}
\caption{Standard exponential distribution function:  $F(T) = 1-exp(-T)$.}\label{fig:2.33}
\end{figure}

The probability that no failure occurs in an interval  $(0,T]$, i.e.\  $t=0$ included, given the fact that no failure occurs at  $t=0$, is equal to:
\begin{align}
P\left[\begin{array}{c} {\min } \\ {t\in \left(0,T\right]} \end{array} \left\{Z(t)\right\}\ge 0\left| Z\left(0\right)\ge 0\right. \right]=e^{-vT} \label{0.174)} 
\end{align}

The probability that no failure occurs in an interval  $[0,T]$, i.e.\  $t=0$ included, is then equal to:
\begin{align}
P\left[\begin{array}{c} {\min } \\ {t\in \left[0,T\right]} \end{array} \left\{Z(t)\right\}\ge 0\right]=\left[1-P_{{\rm F}} \left(0\right)\right]e^{-vT} \label{0.175)} 
\end{align}

In which  $P_F(0)$ is the initial probability of failure (see below for more information on this probability), $i$.e the probability that $Z<0$ at  $t=0$. The probability, $P_f$, that failure occurs in an interval  $[0,T]$ is equal to:
\begin{align}
P_{F} \left(T\right)=P\left[\begin{array}{c} {\min } \\ {t\in \left[0,T\right]} \end{array} \left\{Z(t)\right\}<0\right]=1-{\rm  }\left[1-P_{{\rm F}} \left(0\right)\right]e^{-vT} \label{ZEqnNum473284} 
\end{align}

If the outcrossing rate, $\nu$, and the initial failure probability,  $P_F(0)$, are small, the probability of failure can be approximated by:
\begin{align}
P_{F} \left(T\right)\approx P_{{\rm F}} \left(0\right)+\nu T\label{ZEqnNum950209} 
\end{align}

This is an upper bound of the failure probability. In essence, this approximation ''double counts'' the probability of events in which two or more failures occur in the interval  $[0,T]$. If $\nu$ and  $P_F(0)$ are small, the probability of two or more failures occurring in the interval  $[0,T]$ is negligible and therefore equation \eqref{ZEqnNum950209} is a good approximation in that case.

In the equations above, failure rate $\nu$ was assumed to be constant. If this is not the case, equation \eqref{ZEqnNum473284} changes into the following, more generic, equation: 
\begin{align}
P_{F} \left(T\right)=1-{\rm  }\left[1-P_{{\rm F}} \left(0\right)\right]\exp \left(-\int _{0}^{T}v\left(t\right)dt \right)\label{ZEqnNum131880} 
\end{align}

The equations above can also be used if $Z$ is a function of space. In that case, $t$ and $T$ need to be replaced by $x$ and $X$, where $x$ represents distance, e.g. the longitudinal distance along a dike section. 

\Note{in the \probLib, the outcrossing method is applied in time as well as in space. First, the probabilities of failure of the smallest ''components'' are computed with the probabilistic techniques for single components as described in \Aref{Section_2.3}. The smallest component is e.g. a cross section of a flood defense (space) during a tidal period (time) for a single failure mechanism. So, the initial result of the probabilistic procedure is the probability that failure occurs at a certain cross section within the time-span of a tidal period for a single mechanism. This result will be used as  $P_F(0)$ in the equations above, i.e. the initial failure probability. Subsequently the outcrossing approach is applied for upscaling the probability of failure (for the mechanism under consideration) from a cross-section to a dike section and from a tidal period to a year.

The failure rate $\nu(t)$ or $\nu($x$)$ needs to be derived from spatial and temporal autocorrelations of the strength and load variables. This is described in more detail in \cite{TechRef}. In general, functions $\nu(t)$ and $\nu(x)$ are too complex to solve equation \eqref{ZEqnNum131880} analytically, which means approximating techniques are required. The \probLib uses different outcrossing approaches for space and time because of mutual differences in autocorrelation structures.

Note that the component for which  $P_F(0)$ is computed in the \probLib has a ''width'' equal to the assumed breach width. This means a (slight) reduction in the length of the remainder of the dike section and hence a (slight) reduction in the computed failure probability. The assumed breach width depends on the mechanism under consideration. The ''width'' in time is taken equal to a tidal period. This has to do with the fact that the input statistics of random load variables like sea water level, river discharge or wind speed represent probabilities of the maximum value in a tidal period (see also \cite{TechRef}). These values are therefore suitable to represent the whole tidal period.}

\section{Spatial upscaling - from cross section to flood defence segment}\label{Section_2.5.5}
In this section, upscaling in space is discussed. This type of upscaling is applicable to flood defence systems.

\subsection{Computing the failure probability}\label{Section_2.5.5.1}
The spatial upscaling technique as described in the current section is done over homogeneous reaches of the flood defense. Homogenous in this case means the statistical characteristics remain constant. It is therefore relevant that the flood defence system is divided into segments for which the assumption of homogeneity is valid. So, if a dike segment is inhomogeneous, it needs to be split up into smaller, homogenous, segments.

Spatial upscaling is subject to a concept known as the length effect. The length effect essentially has to do with the increase in failure probability when going from a cross-section to a longitudinal segment and from a single segment to a flood defense system (interconnected segments). That is, the length effect refers to the effect that an increase in length has on the probability of failure. Note that this effect is also present when upscaling over time; the failure probability will increase as the considered time period increases. 

The mathematical description of the length effect is the ratio of the failure probability of the larger length to that of the shorter. For the upscaling from cross-section to longitudinal segment (assuming statistical homogeneity!) this would be as follows: 
\begin{align}
\text{Length effect} = \frac{P_{f, segment} }{P_{f, cross-section} } \label{ZEqnNum163660} 
\end{align}
where $P_{f,segment}$ refers to the failure probability of the longitudinal segment and $P_{f,cross-section}$ refers to the failure probability of the cross section within that longitudinal segment. To derive the ratio of equation \eqref{ZEqnNum163660}, a notion of the spatial correlation within the segment is required, for each random variable, $X$, involved. In the \probLib this correlation is described with the following model:
\begin{align}
\rho \left(\Delta y\right)=\rho _{x} +(1-\rho _{x} )\exp \left[-\left(\frac{\Delta y^{2} }{d_{x}^{2} } \right)\right]\label{ZEqnNum807806} 
\end{align}
where $\rho$ is the correlation between two locations within the segment ($\rho \geqslant 0$), $\Delta_y$ is the distance between these two locations, $\rho_x$ is the residual correlation length of variable $X$ and $d_x$ is the spatial correlation length of variable $X$. Parameter $d_x$ determines how quickly the correlation of variable $X$ decreases over distance and $\rho_x$ is the minimum correlation of variable $X$ between two locations of the same (homogeneous) segment. The parameters $d_x$ and $\rho_x$ need to be determined for each variable $X$, based on a combination of measurements and expert judgement.

\begin{figure}[H]\centering
\includegraphics*[width=4.09in, height=3.06in, keepaspectratio=false]{funcdesign_chapters/figsystemreliability/image52}
\caption{Autocorrelation function, correlation within a dike section; in this picture, the correlation $\rho$ is visualized against $\Delta x$, made non-dimensional by the correlation distance $d_x$.}\label{fig:2.40}
\end{figure}

The correlation model of equation \eqref{ZEqnNum807806} and \Fref{fig:2.40} in principle is applied for each strength variable (load variables can generally be assumed to have correlation $1$ within a single segment). This results in a similar model for the $Z$-function, i.e.\ in values $d_Z$ and $\rho_Z$:
\begin{align}
\rho \left(\Delta y\right)\approx \rho _{Z} +(1-\rho _{Z} )\exp \left[-\left(\frac{\Delta y^{2} }{d_{Z}^{2} } \right)\right]  \label{eq:rhodely} 
\end{align}

The parameters $d_Z$ and $\rho_Z$ can be derived as follows: 
\begin{align}
\rho _{Z} =\sum _{i=1}^{n}\alpha _{i}^{2} \rho _{i}  \label{eq:sm2} 
\end{align}
\begin{align}
\frac{1}{d_{Z}^{2} } =\frac{1}{1-\rho _{Z} } \sum _{i=1}^{n}\alpha _{i}^{2} (1-\rho _{i} )\frac{1}{d_{i}^{2} }  \label{ZEqnNum304048} 
\end{align}

In which: 

\begin{tabular}{p{\textwidth-36pt-125mm}p{5mm}p{120mm}}
$d_i$ & = &correlation length of random variable $i$ \\
$\rho_i$& = &residual correlation length of random variable $i$ \\
$\alpha_i$& = &influence coefficient of random variable $i$ \\
\end{tabular}

Note that coefficients $\alpha_1$, \dots ,$\alpha_n$ are determined in the probabilistic computation for a ''representative'' cross-section within the flood defence segment. For this purpose the probabilistic techniques for a single component are used (see \Aref{Section_2.3}).

To derive the probability of failure of a dike segment, the segment is divided into components of equal length $\Delta L$. The number of components is equal to:
\begin{align}
n_{e} =\frac{L}{\Delta L} 
\end{align}
where $L$ is the length of the dike segment. The probability of failure for the entire dike segment is then equal to:
\begin{align}
P_{f,segment} \approx \left(1+n_{e} \right)P_{f,cross-section} =\left(1+\frac{L}{\Delta L} \right)P_{f,cross-section} \label{ZEqnNum665818} 
\end{align}

This means the continuous process, in which failure can occur at any location along the dike is now replaced by a discrete process in which the dike segment is composed of a finite number of components, each of which has a failure probability that is equal to the probability of failure of a cross-section. This simplification/approximation is only valid for a well selected value of $\Delta L$. If we assume that the spatial variation of $Z$ is a Gaussian ergodic process (i.e. $\rho_Z=0$), the length $\Delta L$ should be taken equal to:
\begin{align}
\Delta L=d_{Z} \sqrt{\pi } /\beta \quad ;{\rm if} \rho _{Z} =0\label{ZEqnNum979631} 
\end{align}
where $\beta$ is the reliability index as derived in the probabilistic computation for a cross-section (see \Aref{Section_2.3}). The value of $\Delta L$ is a result of the outcrossing approach (see \Aref{Section_2.4.5.3}) in which the spatial variation of $Z$ is assumed to be a Gaussian ergodic process. The derivation of $\Delta L$, as described in equation \eqref{ZEqnNum979631}, is described in \cite{Jongejan2012}.

With the assumption of a Gaussian ergodic process, the failure probability of a dike segment of length $L$ is approximately equal to (combine equations \eqref{ZEqnNum665818} and \eqref{ZEqnNum979631}):
\begin{align}
P_{f,segment} \approx \left(1+\frac{L\beta }{d_{Z} \sqrt{\pi } } \right) \Phi \left(-\beta \right)\quad ;{\rm if} \rho _{Z} =0\label{eq:sm3} 
\end{align}

If $\rho_Z$$>$0, the assumption of a Gausian ergodic process does not hold and an alternative solution is required. In that case, $\rho_Z>0$ represents the part of the correlation function that does not contribute to the length effect, because it is the correlation that persists over the entire dike segment. In that case the $Z$-function is split in an ergodic part (with $\rho$ approaching zero over long distances) and a non ergodic part (with $\rho$ constant):
\begin{align}
Z=\beta -v\sqrt{\rho } -u\sqrt{1-\rho } \label{ZEqnNum628933} 
\end{align}
where $v$ is the non-ergodic constant and $u$ is the ergodic stochastic process with:
\begin{align}
\rho \left(\Delta y\right)=\exp \left[-\left(\frac{\Delta y^{2} }{d_{Z}^{2} } \right)\right] \label{eq:sm4} 
\end{align}
where $\rho$ is the correlation between two locations within the segment and $\Delta_y$ is the distance between two locations. Using the theorem of total probability, the failure probability of the flood defence segment can be described as follows:
\begin{align}
P\left[Z<0\right]=\int _{}^{}P\left[Z<0|v\right]f_{V} \left(v\right)dv \label{ZEqnNum551150} 
\end{align}
where $f_V(v)$ is the standard normal density function. The conditional failure probability, $P[Z<0|v]$, in equation \eqref{ZEqnNum551150} can be written as (see Jongejan, 2012):
\begin{align}
\begin{array}{l} {P\left[Z<0|v\right]=1-\left(1-P\left(Z_{cross} <0\right)\right)e^{-N_{f} } } \\\\ {N_{f} =\frac{L}{2\pi } e^{-\frac{\beta^{*2} }{2} } \frac{\sqrt{2} }{d_{z} } } \\\\ {\beta^{*}=\frac{\beta _{cross} -v\sqrt{\rho _{z} } }{\sqrt{1-\rho _{z} } } } \end{array}\label{ZEqnNum591781} 
\end{align}
where $Z_{cross}$ and $\beta_{cross}$ are the $Z$-function and reliability index of the cross section and $\Phi$ is the standard normal distribution function. The combination of equations \eqref{ZEqnNum551150} and \eqref{ZEqnNum591781} provide the probability of failure for a flood defence segment. More details on the derivation of equations \eqref{ZEqnNum551150} and \eqref{ZEqnNum591781} can be found in \cite{Jongejan2012}. Equation \eqref{ZEqnNum591781} can be evauated with high accuracy using for example numerical integration.

\Note{in formula \ref{eq:sm3}  the width of the mechanism is not taken into account. In the \probLib the width of the mechanism is taken equal to $\Delta L$. In the formula for $N_F$ the length $L$ is replaced by $L$ - $\Delta L$. The idea behind this correction is that for stretches smaller than $\Delta L$ it is not be possible to have an increase in failure probability as a result of the length effect.}

In earlier versions of PC-Ring, the following approximation for equations \eqref{ZEqnNum551150} and \eqref{ZEqnNum591781} was implemented to save computation time:
\begin{align}
P\left[Z<0\right]=\left(1+\frac{L\beta \sqrt{1-\rho _{z} } }{d_{Z} \sqrt{\pi } } \right)\Phi \left(-\beta \right)\label{ZEqnNum246851}
\end{align}

This approximation is only valid for small values of $\rho_z$. With the current day computation power, equation \eqref{ZEqnNum591781} can be evaluated in a split second, so it is recommended not to use the approximation as described with equation \eqref{ZEqnNum246851}.

\Note{formula \eqref{ZEqnNum591781} is only valid for value of $\rho_z$ $>$ $0$. For values of $\rho_z = 0$ the Hohenbichler method is used in the \probLib in combination with the outcrossing approach.}

\subsection{Computing equivalent $\alpha$-values}\label{Section_2.5.5.2}

As stated in the previous section, the flood defence segment can be thought of to consist of identical components of $n$ identical components of length $\Delta L$. Upscaling to a dike section in essence is therefore the same as upscaling over $n$ identical components. The last step in such an upscaling process, is the derivation of new equivalent $\alpha$-values for the individual random variables, see \Aref{Section_2.4.3.2}. The first step in this method is to determine the $\alpha$-value of the correlated part of the $Z$-function of equation \eqref{ZEqnNum628933}, i.e.\ variable $V$. This is done in the standard way by perturbing the mean value of $V$ with a small value $\varepsilon$ and quantifying the effect on the computed $\beta$-value of the dike section of a small perturbation ($\varepsilon$) in the mean value of $V$. The $\alpha$-value of $V$ is thus equal to
\begin{align}
\alpha _{v} =\frac{\partial \beta _{section} }{\partial \bar{v}} 
\end{align}

Equation \eqref{ZEqnNum998801} states that the equivalent value, $\alpha_k^e$, of variable $k$ can then be derived as follows: 
\begin{align}
\alpha _{k}^{e} =\sqrt{1-\alpha _{v}^{2} } \frac{\alpha _{k} \sqrt{1-\rho _{k} } }{\sqrt{1-\rho_Z} }+\alpha _{v} \frac{\alpha _{k} \sqrt{\rho _{k} } }{\sqrt{\rho_Z} } \label{ZEqnNum642181} 
\end{align}

In which $\alpha_k$ is the $\alpha$-value of variable $k$ before upscaling and $\rho_k$ is the correlation of variable $k$ between two components. Since components in this case have length $\Delta L$, this correlation is equal to (see equation \eqref{ZEqnNum807806}:
\begin{align}
\rho _{k} =\rho _{kr} +\left(1-\rho _{kr} \right)\cdot \exp \left[-\left(\frac{\Delta L}{d_{k} } \right)^{2} \right]\label{eq:sm5} 
\end{align}
where, $\rho_{kr}$ is the residual correlation length of variable $k$ and $d_x$ is the spatial correlation length of variable $k$. 
